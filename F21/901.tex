\documentclass{amsart}[12pt]
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amscd,mathabx}
\usepackage{amssymb,setspace}
\usepackage{latexsym,amsfonts,amssymb,amsthm,amsmath,amscd,stmaryrd,mathrsfs,xcolor}
\usepackage[all, knot]{xy}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\xyoption{all}
\xyoption{arc}
\usepackage{hyperref}


%\usepackage[notcite,notref]{showkeys}
 
%\CompileMatricesx
\newcommand{\edit}[1]{\marginpar{\footnotesize{#1}}}
%\newcommand{\edit}[1]{}
\newcommand{\rperf}[2]{\operatorname{RPerf}(#1 \into #2)}



\newcommand{\vectwo}[2]{\begin{bmatrix} #1 \\ #2 \end{bmatrix}}

\newcommand{\vecfour}[4]{\begin{bmatrix} #1 \\ #2 \\ #3 \\ #4 \end{bmatrix}}

\newcommand{\Cat}[1]{\left<\left< \text{#1} \right>\right>}


\def\htpy{\simeq_{\mathrm{htpc}}}
\def\tor{\text{ or }}
\def\fg{finitely generated~}

\def\Ass{\operatorname{Ass}}
\def\ann{\operatorname{ann}}
\def\sign{\operatorname{sign}}

\def\ob{{\mathfrak{ob}} }
\def\BiAdd{\operatorname{BiAdd}}
\def\BiLin{\operatorname{BiLin}}

\def\Syl{\operatorname{Syl}}
\def\span{\operatorname{span}}

\def\sdp{\rtimes}
\def\cL{\mathcal L}
\def\cR{\mathcal R}



\def\ay{??}
\def\Aut{\operatorname{Aut}}
\def\End{\operatorname{End}}
\def\Mat{\operatorname{Mat}}


\def\a{\alpha}



\def\etale{\'etale~}
\def\tW{\tilde{W}}
\def\tH{\tilde{H}}
\def\tC{\tilde{C}}
\def\tS{\tilde{S}}
\def\tX{\tilde{X}}
\def\tZ{\tilde{Z}}
\def\HBM{H^{\text{BM}}}
\def\tHBM{\tilde{H}^{\text{BM}}}
\def\Hc{H_{\text{c}}}
\def\Hs{H_{\text{sing}}}
\def\cHs{{\mathcal H}_{\text{sing}}}
\def\sing{{\text{sing}}}
\def\Hms{H^{\text{sing}}}
\def\Hm{\Hms}
\def\tHms{\tilde{H}^{\text{sing}}}
\def\Grass{\operatorname{Grass}}
\def\image{\operatorname{im}}
\def\im{\image}
\def\ker{\operatorname{ker}}
\def\coker{\operatorname{coker}}
\def\cone{\operatorname{cone}}
\newcommand{\Hom}{\mathrm{Hom}}

\newcommand{\onto}{\twoheadrightarrow}


\def\ku{ku}
\def\bbu{\bf bu}
\def\KR{K{\mathbb R}}

\def\CW{\underline{CW}}
\def\cP{\mathcal P}
\def\cE{\mathcal E}
\def\cL{\mathcal L}
\def\cJ{\mathcal J}
\def\cJmor{\cJ^\mor}
\def\ctJ{\tilde{\mathcal J}}
\def\tPhi{\tilde{\Phi}}
\def\cA{\mathcal A}
\def\cB{\mathcal B}
\def\cC{\mathcal C}
\def\cZ{\mathcal Z}
\def\cD{\mathcal D}
\def\cF{\mathcal F}
\def\cG{\mathcal G}
\def\cO{\mathcal O}
%\def\cI{\mathcal I}
\def\cS{\mathcal S}
\def\cT{\mathcal T}
\def\cM{\mathcal M}
\def\cN{\mathcal N}
\def\cMpc{{\mathcal M}_{pc}}
\def\cMpctf{{\mathcal M}_{pctf}}
\def\L{\Lambda}

\def\sA{\mathscr A}
\def\sB{\mathscr B}
\def\sC{\mathscr C}
\def\sZ{\mathscr  Z}
\def\sD{\mathscr  D}
\def\sF{\mathscr  F}
\def\sG{\mathscr G}
\def\sO{\mathscr  O}
\def\sI{\mathscr I}
\def\sS{\mathscr S}
\def\sT{\mathscr  T}
\def\sM{\mathscr M}
\def\sN{\mathscr N}

%\newcommand{\inc}{\subseteq}
\newcommand{\id}{\mathrm{id}}


\newcommand{\Aug}[1]{\textcolor{violet}{Lecture of August #1, 2021}}
\newcommand{\Sept}[1]{\textcolor{violet}{Lecture of September #1, 2021}}
\newcommand{\Oct}[1]{\textcolor{violet}{Lecture of October #1, 2021}}
\newcommand{\Nov}[1]{\textcolor{violet}{Lecture of November #1, 2021}}
\newcommand{\Dec}[1]{\textcolor{violet}{Lecture of December #1, 2021}}

\def\Ext{\operatorname{Ext}}
 \def\ext{\operatorname{ext}}



\def\ov#1{{\overline{#1}}}

\def\vecthree#1#2#3{\begin{bmatrix} #1 \\ #2 \\ #3 \end{bmatrix}}

\def\tOmega{\tilde{\Omega}}
\def\tDelta{\tilde{\Delta}}
\def\tSigma{\tilde{\Sigma}}
\def\tsigma{\tilde{\sigma}}


\def\d{\delta}
\def\td{\tilde{\delta}}

\def\e{\epsilon}
\def\nsg{\unlhd}
\def\pnsg{\lhd}

\newcommand{\tensor}{\otimes}
\newcommand{\homotopic}{\simeq}
\newcommand{\homeq}{\cong}
\newcommand{\iso}{\approx}

\DeclareMathOperator{\ho}{Ho}
\DeclareMathOperator*{\colim}{colim}


\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\H}{\mathbb{H}}

\newcommand{\bP}{\mathbb{P}}
\newcommand{\bM}{\mathbb{M}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\bH}{{\mathbb{H}}}
\newcommand{\G}{\mathbb{G}}
\newcommand{\bR}{{\mathbb{R}}}
\newcommand{\bL}{{\mathbb{L}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bK}{\mathbb{K}}


\newcommand{\bD}{\mathbb{D}}
\newcommand{\bS}{\mathbb{S}}

\newcommand{\bN}{\mathbb{N}}


\newcommand{\bG}{\mathbb{G}}

\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\M}{\mathcal{M}}
\newcommand{\W}{\mathcal{W}}



\newcommand{\itilde}{\tilde{\imath}}
\newcommand{\jtilde}{\tilde{\jmath}}
\newcommand{\ihat}{\hat{\imath}}
\newcommand{\jhat}{\hat{\jmath}}

\newcommand{\fc}{{\mathfrak c}}
\newcommand{\fp}{{\mathfrak p}}
\newcommand{\fm}{{\mathfrak m}}
\newcommand{\fn}{{\mathfrak n}}
\newcommand{\fq}{{\mathfrak q}}

\newcommand{\op}{\mathrm{op}}
\newcommand{\dual}{\vee}

\newcommand{\DEF}[1]{\emph{#1}\index{#1}}
\newcommand{\Def}[1]{#1 \index{#1}}


% The following causes equations to be numbered within sections
\numberwithin{equation}{section}


\theoremstyle{plain} %% This is the default, anyway
\newtheorem{thm}[equation]{Theorem}
\newtheorem{thmdef}[equation]{TheoremDefinition}
\newtheorem{introthm}{Theorem}
\newtheorem{introcor}[introthm]{Corollary}
\newtheorem*{introthm*}{Theorem}
\newtheorem{question}{Question}
\newtheorem{cor}[equation]{Corollary}
\newtheorem{por}[equation]{Porism}
\newtheorem{lem}[equation]{Lemma}
\newtheorem{lemminition}[equation]{Lemminition}
\newtheorem{prop}[equation]{Proposition}
\newtheorem{porism}[equation]{Porism}
\newtheorem{fact}[equation]{Fact}


\newtheorem{conj}[equation]{Conjecture}
\newtheorem{quest}[equation]{Question}

\theoremstyle{definition}
\newtheorem{defn}[equation]{Definition}
\newtheorem{chunk}[equation]{}
\newtheorem{ex}[equation]{Example}

\newtheorem{exer}[equation]{Optional Exercise}

\theoremstyle{remark}
\newtheorem{rem}[equation]{Remark}

\newtheorem{notation}[equation]{Notation}
\newtheorem{terminology}[equation]{Terminology}



\renewcommand{\sec}[1]{\section{#1}}
\newcommand{\ssec}[1]{\subsection{#1}}
\newcommand{\sssec}[1]{\subsubsection{#1}}

\newcommand{\br}[1]{\lbrace \, #1 \, \rbrace}
\newcommand{\li}{ < \infty}
\newcommand{\quis}{\simeq}
\newcommand{\xra}[1]{\xrightarrow{#1}}
\newcommand{\xla}[1]{\xleftarrow{#1}}
\newcommand{\xlra}[1]{\overset{#1}{\longleftrightarrow}}

\newcommand{\xroa}[1]{\overset{#1}{\twoheadrightarrow}}
\newcommand{\xria}[1]{\overset{#1}{\hookrightarrow}}
\newcommand{\ps}[1]{\mathbb{P}_{#1}^{\text{c}-1}}




\def\and{{ \text{ and } }}
\def\oor{{ \text{ or } }}

\def\Perm{\operatorname{Perm}}
\newcommand{\Ss}{\mathbb{S}}

\def\Op{\operatorname{Op}}
\def\res{\operatorname{res}}
\def\ind{\operatorname{ind}}

\def\sign{{\mathrm{sign}}}
\def\naive{{\mathrm{naive}}}
\def\l{\lambda}


\def\ov#1{\overline{#1}}
\def\cV{{\mathcal V}}
%%%-------------------------------------------------------------------
%%%-------------------------------------------------------------------

\newcommand{\chara}{\operatorname{char}}
\newcommand{\Kos}{\operatorname{Kos}}
\newcommand{\opp}{\operatorname{opp}}
\newcommand{\perf}{\operatorname{perf}}

\newcommand{\Fun}{\operatorname{Fun}}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\SL}{\operatorname{SL}}
\def\o{\omega}
\def\oo{\overline{\omega}}

\def\cont{\operatorname{cont}}
\def\te{\tilde{e}}
\def\gcd{\operatorname{gcd}}

\def\stab{\operatorname{stab}}

\def\va{\underline{a}}

\def\ua{\underline{a}}
\def\ub{\underline{b}}


\newcommand{\Ob}{\mathrm{Ob}}
\newcommand{\Set}{\mathbf{Set}}
\newcommand{\Grp}{\mathbf{Grp}}
\newcommand{\Ab}{\mathbf{Ab}}
\newcommand{\Sgrp}{\mathbf{Sgrp}}
\newcommand{\Ring}{\mathbf{Ring}}
\newcommand{\Fld}{\mathbf{Fld}}
\newcommand{\cRing}{\mathbf{cRing}}
\newcommand{\Mod}[1]{#1-\mathbf{Mod}}
\newcommand{\vs}[1]{#1-\mathbf{vect}}
\newcommand{\Vs}[1]{#1-\mathbf{Vect}}
\newcommand{\vsp}[1]{#1-\mathbf{vect}^+}
\newcommand{\Top}{\mathbf{Top}}
\newcommand{\Setp}{\mathbf{Set}_*}
\newcommand{\Alg}[1]{#1-\mathbf{Alg}}
\newcommand{\cAlg}[1]{#1-\mathbf{cAlg}}
\newcommand{\PO}{\mathbf{PO}}
\newcommand{\Cont}{\mathrm{Cont}}
\newcommand{\MaT}[1]{\mathbf{Mat}_{#1}}
\newcommand{\Rep}[2]{\mathbf{Rep}_{#1}(#2)}

%%%-------------------------------------------------------------------
%%%-------------------------------------------------------------------
%%%-------------------------------------------------------------------
%%%-------------------------------------------------------------------
%%%-------------------------------------------------------------------

\makeindex
\title{Math 901 Lecture Notes, Fall 2021}


\begin{document}
\onehalfspacing

\maketitle

\tableofcontents

\sec{Category Theory}

\ssec{Categories}

\

\Aug{23}


\sssec{Definition of category}



\begin{defn} A \DEF{category} $\sC$ consists of the following data:
\begin{enumerate}
\item a collection of \DEF{objects}, denoted $\Ob(\sC)$\index{$\Ob$},
\item for each pair of objects $A,B\in \Ob(\sC)$, a set \Def{$\Hom_{\sC}(A,B)$} of \emph{morphisms}\index{morphisms} (also known as \emph{arrows}\index{arrows}) from $A$ to $B$,
\item for each triple of objects $A,B,C\in \Ob(\sC)$, a function
\[\Hom_{\sC}(A,B) \times \Hom_{\sC}(B,C)\longrightarrow \Hom_{\sC}(A,C) \]
written as $(\alpha,\beta)\mapsto \beta \circ \alpha$ that we call the \emph{composition rule}\index{composition}.
\end{enumerate}
These data are required to satisfy the following axioms:
\begin{enumerate}
\item (Disjointness) the Hom sets are disjoint: if $A\neq A'$ or $B\neq B'$, then \[\Hom_{\sC}(A,B)\cap \Hom_{\sC}(A',B')=\varnothing.\]
\item (Identities) for every object $A$, there is an \DEF{identity morphism} $1_A\in \Hom_{\sC}(A,A)$\index{$1_A$} such that $1_A \circ f = f$ and $g \circ 1_A = g$ for all $f\in \Hom_{\sC}(B,A)$ and all $g\in \Hom_{\sC}(A,B)$.
\item (Associativity) composition is associative: $f \circ (g \circ h) = (f\circ g) \circ h$.
\end{enumerate}
\end{defn}

\begin{rem}
\begin{enumerate}
\item The word ``collection'' as opposed to ``set'' is important here. The point is that there is no set of all sets, but by utilizing bigger collecting objects in set theory, we can sensibly talk about the collection of all sets. We'll sweep all of the set theory under the rug there, but it's worth keeping in mind that the objects of a category don't necessarily form a set. We did assume that the collections of morphisms between a pair of objects  form a set, though not everyone does. 
\item The first axiom above guarantees that every morphism $\alpha$ in a category $\sC$ has a well-defined \DEF{source} and \DEF{target} in $\Ob(\sC)$, namely, the unique $A$ and $B$ (respectively) such that ${\alpha\in \Hom_{\sC}(A,B)}$.
\end{enumerate}
\end{rem}

The name arrow dovetails with the common practice of depicting a morphism $\alpha\in \Hom_{\sC}(A,B)$ as
\[ A \stackrel{\alpha}{\longrightarrow} B.\]
The composition of $A \xra{\alpha} B$ and $B\xra{\beta} C$ is $A\xra{\beta \circ \alpha} C$.

\begin{exer} Prove that every element in a category has a unique identity morphism (i.e., a unique morphism that satisfies the hypothesis of axiom (2)).
\end{exer}

\sssec{Examples of categories}

Many of our favorite objects from algebra naturally congregate in categories!

\begin{ex}
\begin{enumerate}
\item There is a category \Def{$\Set$} where
\begin{itemize}
\item $\Ob(\Set)$ is the collection of all sets
\item for two sets $X$, $Y$, $\Hom_{\Set}(X,Y)$ is the the set of functions from $X$ to $Y$
\item the composition rule is composition of functions
\end{itemize}
We observe that every set has an identity function, which behaves as an identity for composition, and that composition of functions is associative.
\item There is a category \Def{$\Grp$} where
\begin{itemize}
\item $\Ob(\Grp)$ is the collection of all groups
\item for two sets $X$, $Y$, $\Hom_{\Grp}(X,Y)$ is the the set of group homomorphisms from $X$ to $Y$
\item the composition rule is composition of functions
\end{itemize}
Note that the identity function on a group is a group homomorphism, and that a composition of two group homomorphisms is a group homomorphism.
\item There is a category \Def{$\Ab$} where
\begin{itemize}
\item $\Ob(\Ab)$ is the collection of all abelian groups
\item for two sets $X$, $Y$, $\Hom_{\Ab}(X,Y)$ is the the set of group homomorphisms from $X$ to $Y$
\item the composition rule is composition of functions
\end{itemize}
\item In this class,
\begin{itemize}
\item A \DEF{semigroup} is a set $S$ with an associative operation $\cdot$ that has an identity element; some may prefer the term \DEF{monoid}, but I don't. 
\item A \DEF{semigroup homomorphism} from semigroups $S\to T$ is a function that preserves the operation and maps the identity element to the identity element.
\end{itemize}
There is a category $\Sgrp$ where the objects are all semigroups and the morphisms are semigroup homomorphisms. (The composition rule is composition again.)

\item In this class, 
\begin{itemize}
\item A \DEF{ring} is a set $R$ with two operations $+$ and $\cdot$ such that
$(R,+)$ is abelian group, with identity $0$, and $(R, \cdot)$ is a semigroup with identity $1$, and such that
the left and right distributive laws hold: $(r+s)t = rt + st$ and $t(r+s) = tr + ts$. 
\item A \DEF{ring homomorphism} is a function that preserves $+$ and $\cdot$ and sends $1$ to $1$.
\end{itemize}
There is a category \Def{$\Ring$} where the objects are all rings and the morphisms are ring homomorphisms. 

\item Let $R$ be a ring. In this class, 
\begin{itemize}
\item A \DEF{left $R$-module} is an abelian group $(M, +)$ equipped with a pairing $R \times M \to M$, written $(r,m) \mapsto rm$ or $(r,m)\mapsto r \cdot m$
such that 
\begin{enumerate}
\item $r_1(r_2m)  = (r_1r_2)m$,
\item  $(r_1+r_2)m  = r_1m + r_2m$,
\item $r(m_1 + m_2) = rm_1 + rm_2$, and 
\item $1m = m$.
\end{enumerate}
\item A \DEF{left module homomorphism} or \DEF{$R$-linear map} between left $R$-modules $\phi:M\to N$ is a homomorphism of abelian groups from $(M,+)\to (N,+)$ such that $\phi(r m) = r \phi(m)$.
\end{itemize}
There is a category \Def{$\Mod{R}$} where the objects are all left $R$-modules and the morphisms are $R$-linear maps.

\item There is a category \Def{$\Fld$} where the objects are all fields and the morphisms are all field homomorphisms.

\item There is a category \Def{$\Top$} where the objects are all topological spaces and the morphisms are all continuous functions.
\end{enumerate}
\end{ex}


\begin{rem}
There are two special cases of the category of $R$-modules that are worth singling out:
\begin{itemize}
\item Every abelian group $M$ is a $\Z$-module in a unique way, by setting
\[n\cdot m = \underbrace{m + \cdots + m}_{n-\text{times}} \quad \text{and} \quad  -n\cdot  m = -(\underbrace{m + \cdots + m}_{n-\text{times}}) \qquad\text{for $n\geq0$}. \]
Thus, $\Ab$ is basically just $\Mod{\Z}$.
\item When $R=K$ happens to be a field, we are accustomed to calling $K$-modules \DEF{vector spaces}. Thus, we might write \Def{$\Vs{K}$} for $\Mod{K}$.
\end{itemize}
\end{rem}



\Aug{25}

\begin{ex}
Here are some variations on the category $\Vs{K}$.
\begin{enumerate}
\item The collection of finite dimensional $K$-vector spaces with all linear transformations is a category; call it \Def{$\vs{K}$}.
\item The collection of all $n$-dimensional $K$-vector spaces with all linear transformations is a category.
\item The collection of all $K$-vector spaces (or $n$-dimensional vector spaces) with linear isomorphisms is a category.
\item The collection of all $K$-vector spaces (or $n$-dimensional vector spaces) with nonzero linear transformations  is not a category, since it's not closed under composition.
\item The collection of all $n$-dimensional vector spaces with singular linear transformations is not a category, since it doesn't have identity maps.
\end{enumerate}
\end{ex}

\begin{ex}\label{eg:1.4}
\begin{enumerate}
\item There is a category \Def{$\Setp$} of \DEF{pointed sets} where
\begin{itemize}
\item the objects are pairs $(X,x)$ where $X$ is a set and $x\in X$,
\item for two pointed sets, the morphisms from $(X,x)$ to $(Y,y)$ are functions $f:X\to Y$ such that $f(x)=y$,
\item usual composition.
\end{itemize}
\item For a commutative ring $A$, 
\begin{itemize}
\item A \DEF{commutative $A$-algebra} is a commutative ring $R$ plus a homomorphism $\phi:A\to R$.
\item Slightly more generally, an \DEF{$A$-algebra} is a ring $R$ plus a homomorphism $\phi:A\to R$ such that $\phi(A)$ lies in the center of $R$: $r\cdot \phi(a) = \phi(a) \cdot r$ for any $a\in A$ and $r\in R$. (In the more general situation, $A$ is still commutative but $R$ may not be.) 
\item An \DEF{$A$-algebra homomorphism} between two $A$-algebras $(R,\phi)$ and $(S,\psi)$ is a ring homomorphism $\alpha:R\to S$ such that $\alpha\circ \phi = \psi$.
\end{itemize}
The category of $A$-algebras is denoted \Def{$\Alg{A}$}, and the category of commutative $A$-algebras is \Def{$\cAlg{A}$}.


\item Fix a field $K$, and define a category $\MaT{K}$ as follows: the objects are the positive natural numbers $n\in \N_{>0}$, and $\Hom_{\sC}(a,b)$ is the set of $b\times a$ matrices with entries in $K$. To see this as a category, we need a composition rule. Given $B\in \Hom_{\sC}(b,c)$ and $A\in \Hom_{\sC}(a,b)$, take the composition $A\circ B\in \Hom_{\sC}(a,c)$ to be the product $AB$. Since matrix multiplication is associative, axiom (3) holds, and the $n\times n$ identity matrix serves as an identity morphism in the sense of axiom (2). Finally, if $A\in \Hom_{\sC}(a,b)\cap \Hom_{\sC}(a',b')$, then $A$ is a $b\times a$ matrix and a $b'\times a'$ matrix, so $a=a'$ and $b=b'$. Notably, the morphisms in this category are not functions.
\end{enumerate}
\end{ex}

We can also make a bunch of categories in a hands-on way as follows:

\begin{ex} Let $(P,\leq)$ be a poset.
We define a category \Def{$\PO(P)$} from $P$ as follows. The objects of $\PO(P)$ are just the elements of $P$.
For each pair $a,b\in P$ with $a\leq b$, form a symbol $f_a^b$. Then we set
\[ \Hom_{\PO(P)}(a,b) = \begin{cases} \{f_a^b\} &\text{if}\  a\leq b \\ \varnothing & \text{otherwise}. \end{cases}\]
There is only one possible composition rule:
\[ \Hom_{\PO(P)}(a,b) \times \Hom_{\PO(P)}(b,c) \longrightarrow \Hom_{\PO(P)}(a,c)\]
when $a\leq b$ and $b\leq c$ we also have $a\leq c$, and the unique pair on the left must map to the unique element on the right, so $f_b^c \circ f_a^b = f_a^c$; when either $a\not\leq b$ or $b\not\leq c$, there is nothing to compose!

Each morphism $f_a^b$ is in only one Hom set (with source $a$ and target $b$). Composition is associative since there is at most one function between one element sets. For any $a$, $f_a^a\in \Hom_{\PO(P)}(a,a)$ is the identity morphism.

For a specific example, we can think of $\N_{> 0}$ as a category this way. Drawing all of the morphisms would be a mess, but any morphism is a composition of the ones depicted:

\[  1 \longrightarrow 2 \longrightarrow 3 \longrightarrow 4 \longrightarrow 5 \longrightarrow \cdots .\] 
Note that the objects of this category are exactly the same as in Example~\ref{eg:1.4}(3), but with much fewer morphisms!
\end{ex}

\begin{ex} A category with one object is nothing but a semigroup.\end{ex}

\sssec{Constructions of categories}

Here are a few more basic constructions of categories:

\begin{defn} Given a category $\sC$, the \DEF{opposite category} \Def{$\sC^{\mathrm{op}}$} is the category with $\Ob(\sC^{\mathrm{op}})=\Ob(\sC)$, and $\Hom_{\sC}(A,B) = \Hom_{\sC}(B,A)$ for all $A,B\in \Ob(\sC)$.
\end{defn}

That is, the opposite category is the ``same category with the arrows reversed.'' To avoid confusion, we might write $\alpha^{\mathrm{op}}$ for the morphism $B\stackrel{\alpha^{\mathrm{op}}}{\longrightarrow} A$ in $\sC^{\mathrm{op}}$ corresponding to $A \stackrel{\alpha}{\longrightarrow} B$ in $\sC$.

\begin{defn} Given two categories $\sC$ and $\sD$, the \DEF{product category} \Def{$\sC \times \sD$} is the category with $\Ob(\sC \times \sD)$ given by the collection of pairs $(C,D)$ with $C\in \Ob(\sC)$ and $D\in \Ob(\sD)$, and $\Hom_{\sC \times \sD}((A,B),(C,D))=\Hom_{\sC}(A,C) \times \Hom_{\sD}(B,D)$. We leave it to you to pin down the composition rule.
\end{defn}

\begin{defn} A category $\sD$ is a \DEF{subcategory} of another category $\sC$ provided 
\begin{enumerate}
\item every object of $\sD$ is an object of $\sC$
\item for every $A,B\in \Ob(\sD)$, $\Hom_{\sD}(A,B)\subseteq \Hom_{\sC}(A,B)$, and
\item for every $A \xra{\alpha} B$ and $B \xra{\beta} C$ in $\sD$, the composition of $\alpha$ and $\beta$ in $\sD$ equals the composition of $\alpha$ and $\beta$ in $\sC$.
\end{enumerate}
If equality hold in (2) (for all $A,B$), we say that $\sD$ is a \DEF{full subcategory} of $\sC$.
\end{defn}

\begin{ex} Since every group is a set, and every homomorphism is a function, $\Grp$ is a subcategory of $\Set$. However, since not every function between groups is a homomorphism, $\Grp$ is not a full subcategory of $\Set$. Similarly, $\Ab$, $\Ring$, $\Mod{R}$, and $\Top$ are all subcategories of $\Set$.

On the other hand, $\Ab$ is a full subcategory of $\Grp$, and $\Grp$ is a full subcategory of $\Sgrp$: a morphism of abelian groups is a morphism of groups that happens to be between abelian groups (and likewise for groups and semigroups)!
\end{ex}


\Aug{27}


%\begin{defn} Given a category $\sC$ and an object $A\in \Ob(\sC)$, the \DEF{coslice category} \Def{$\sC_A$} is the category with objects given by all arrows from $A$: namely morphisms of the form 
%\[ A \stackrel{\alpha}{\longrightarrow} X \ \quad\text{with}\quad \ X\in \Ob(\sC)\]
%and morphisms 
%\[ \Hom_{\sC_A}(A \stackrel{\alpha}{\longrightarrow} X,A \stackrel{\beta}{\longrightarrow} Y) = \{ X \stackrel{\gamma}{\longrightarrow} Y \ | \ \gamma \circ \alpha = \beta\}.\]
%Equivalently, we can think of such a morphism as a triangle:
% \[\xymatrix{
%&A \ar[dl]_{\alpha} \ar[dr]^{\beta}& \\
%X \ar[rr]^{\gamma}&&Y
%}   \]
%The composition rule comes from the composition rule on $\sC$: if $X \xrightarrow{\gamma}Y$ and $Y \xrightarrow{\delta\circ \gamma} Z$.
%\end{defn}
%
%This construction may be less new to you than you think!
%
%\begin{exer} Explain how $\cAlg{A}$ and $\Setp$ are special cases of the coslice category construction. Can you describe the category $\Alg{A}$ in terms of the terms above?
%\end{exer}



\subsection{Basic notions with morphisms}

\begin{defn} A \DEF{diagram} in a category $\sC$ is a directed multigraph whose vertices are objects in $\sC$ and whose arrows/edges are morphisms in $\sC$. A \DEF{commutative diagram} in $\sC$ is a diagram in which for each pair of vertices $A,B$, any two paths from $A$ to $B$ compose to the same morphism.
\end{defn}

\begin{ex} To say that the diagram
\[\xymatrix{
A \ar[r]^{\alpha} \ar[d]_{\gamma}& B \ar[d]^{\beta}\\
C \ar[r]^{\delta} &D 
}\]
commutes is to say that $\beta\circ \alpha = \delta \circ \gamma$ in $\Hom_{\sC}(A,D)$.
\end{ex}



\begin{defn}
Let $\mathscr{C}$ be any category and $A\xra{\alpha} B$ a morphism.

\begin{itemize}
	\item $\alpha$ is an \DEF{isomorphism} if there exists $B \xra{\beta} A$ such that $\beta \circ \alpha = 1_A$ and $\alpha\circ \beta = 1_B$. Such an  $\beta$ is called the \DEF{inverse} of $f$.
	\item $\alpha$ has $\beta$ as a \DEF{left inverse} if $\beta \circ \alpha = 1_A$. Similarly define \DEF{right inverse}.
	\item $\alpha$ is a \DEF{monomorphism} or is \DEF{monic} if for all arrows  
$$\xymatrix{C \ar@<0.5ex>[r]^-{\beta_1} \ar@<-0.5ex>[r]_-{\beta_2} & A \ar[r]^-\alpha & B}$$
if $\alpha \beta_1 =\alpha \beta_2$ then $\beta_1 = \beta_2$. That is, $\alpha$ can be cancelled from the left.
	\item $\alpha$ is an \DEF{epimorphism} or is \DEF{epic} if for all arrows 
$$\xymatrix{A \ar[r]^-\alpha & B \ar@<0.5ex>[r]^-{\beta_1} \ar@<-0.5ex>[r]_-{\beta_2} & C}$$
if $\beta_1\alpha = \beta_2\alpha$ then $\beta_1 = \beta_2$. That is, $\alpha$ can be cancelled from the right.
\end{itemize}
\end{defn}

\begin{rem}
Note that $\alpha$ has a left inverse in $\sC$ if and only if $\alpha^{\op}$ has a right inverse in $\sC^{\op}$, and that $\alpha$ is monic in $\sC$ if and only if $\alpha^{\op}$ is epic in $\sC^{\op}$. We say that these are \DEF{dual} notions in category theory.
\end{rem}

\begin{lem} If $\alpha$ has a left inverse, then $\alpha$ is monic. Similarly for ``right inverse'' and ``epic''.
\end{lem}
\begin{proof}
If $\beta\circ \alpha = 1_A$ and $\gamma_1,\gamma_2$ are two morphisms from $C \to A$ such that $\alpha\circ \gamma_1 = \alpha\circ \gamma_2$, then 
\[ \gamma_1 = (\beta\circ \alpha) \circ \gamma_1 = \beta\circ (\alpha \circ \gamma_1)=\beta\circ (\alpha \circ \gamma_2) = (\beta\circ \alpha) \circ \gamma_2 = \gamma_2. \]
Similarly for ``right inverse'' and ``epic''.
\end{proof}

\begin{ex} In $\Set$, the monomorphisms and  left-invertible morphisms agree, and these are the injective functions. The epimorphisms and right-invertible morphisms agree, and there are the surjective functions.
\end{ex}

\begin{exer} For any poset $P$, in $\PO(P)$, every morphism is both monic and epic, but no nonidentity morphism has a left or right-inverse.
\end{exer}


\subsection{Category-theoretic constructions of objects}

A property or construction is \DEF{category theoretic} if can be described just in terms of the data of the category rather than aspects of a particular category.

\begin{ex}
Can we identify $\varnothing$ in $\Set$ without looking at the objects' and morphisms' names? We can:
for every set $S$, there is a unique function $f:\varnothing \to S$; $\varnothing$ is the only set with this property.
\end{ex}

\begin{defn}
\begin{enumerate} \item An object $X$ in a category $\sC$ is \DEF{initial} if there for every $Y\in \Ob(\sC)$, there is a unique morphism $X\to Y$.
\item An object $X$ in a category $\sC$ is \DEF{terminal} if there for every $Y\in \Ob(\sC)$, there is a unique morphism $Y\to X$.
\end{enumerate}
\end{defn}

\begin{ex}
\begin{enumerate} \item We just saw that $\varnothing$ is initial in $\Set$. Any singleton is terminal.
\item A group with only one element $\{e\}$ is both initial and terminal in $\Grp$.
\item $\Z$ is initial in $\Ring$.
\end{enumerate}
\end{ex}


\sssec{Definitions of product and coproduct}

\begin{defn} Let $\sC$ be a category, and $\{ X_\lambda\}_{\lambda\in \Lambda}$ be a family of objects. A \DEF{product} of $\{ X_\lambda\}_{\lambda\in \Lambda}$ is given by an object $P$ and a family of morphisms $\{p_\lambda : P \to X_\lambda\}_{\lambda\in \Lambda}$ that is universal in the following sense:

Given an object $Y$ and a family of morphisms $\{f_\lambda:Y\to X_\lambda\}_{\lambda\in \Lambda}$, there is a unique morphism $\phi: Y\to P$ such that $p_\lambda \circ \phi = f_\lambda$ for all $\lambda$.
\end{defn}

Here is a diagram for the (first few) maps involved when $\Lambda=\N$ is countable:
\[\xymatrix{  & & & P \ar[drr]^-{p_1} \ar[ddrr]^-{p_2} \ar[dddrr]^-{p_3} \ar[ddddrr]  & & \\
 & & & & & X_1\\
Y \ar@{-->}[uurrr]^-{\phi}\ar[urrrrr]^-{f_1}  \ar[rrrrr]^-{f_2} \ar[drrrrr]^-{f_3} \ar[ddrrrrr]&  & & & & X_2\\
& & & & & X_3\\
& & & & & \vdots }\]

We can also take a ``big picture'' view of this universal property:
\[\xymatrix{ & P\ar@{~>}[dr]^{\{p_\lambda\}} & \\
Y \ar@{-->}[ur]^{\phi} \ar@{~>}[rr]_{\{f_\lambda\}} & & \{X_{\lambda}\},}\]
where the squiggly arrows are again collections of maps instead of maps. The data of $Y$ with a family of maps to each $X_\lambda$ is the sort of thing a product might be, so we may think of it as a ``product candidate.'' In this way, we can think of a product as a ``terminal product candidate.''



\Aug{30}



\begin{rem}
Note that $(P,\{p_\lambda\}_{\lambda\in\Lambda})$ is a product of $\{ X_\lambda\}_{\lambda\in \Lambda}$ if and only if the function
\[ \xymatrix{ \Hom_{\sC}(Y,P) \ar[rr] & & \bigtimes_{\lambda\in\Lambda} \Hom_{\sC}(Y,X_\lambda)\\
\phi \ar@{|->}[rr] & &  ( p_\lambda \circ \phi )_{\lambda\in \Lambda}}\]
is a bijection for all objects $Y$: the universal property says that everything in the target comes from a unique thing in the source.
\end{rem}




\begin{defn} Let $\sC$ be a category, and $\{ X_\lambda\}_{\lambda\in \Lambda}$ be a family of objects. A \DEF{coproduct} of $\{ X_\lambda\}_{\lambda\in \Lambda}$ is given by an object $C$ and a family of morphisms $\{i_\lambda : X_\lambda \to C\}_{\lambda\in \Lambda}$ that is universal in the following sense:

Given an object $Y$ and a family of morphisms $\{f_\lambda:X_\lambda \to Y\}_{\lambda\in \Lambda}$, there is a unique morphism $\phi: C\to Y$ such that $\phi \circ i_\lambda = f_\lambda$ for all $\lambda$.
\end{defn}


Here is a diagram for the (first few) maps involved when $\Lambda=\N$ is countable:
\[\xymatrix{  & & & C \ar@{-->}[ddrr]^{\phi}& & \\
\vdots\ar[urrr] \ar[drrrrr]  & & & & & \\
X_3\ar[uurrr]^-{i_3} \ar[rrrrr]^-{f_3} & & & & & Y\\
X_2\ar[uuurrr]^-{i_2} \ar[urrrrr]^-{f_2}  & & & & & \\
X_1\ar[uuuurrr]^-{i_1} \ar[uurrrrr]^-{f_1} & & & & &}\]

We can also take a ``big picture'' view of the universal property:
\[\xymatrix{ & C\ar@{-->}[dr]^{\phi} & \\
\{ X_{\lambda}\} \ar@{~>}[ur]^{\{i_\lambda\}} \ar@{~>}[rr]_{\{f_\lambda\}} & & Y,}\]
where the squiggly arrows are now collections of maps instead of maps. We can again think of the coproduct as the ``initial coproduct candidate.''



\begin{rem}
Note that $(C,\{i_\lambda\}_{\lambda\in\Lambda})$ is a coproduct of $\{ X_\lambda\}_{\lambda\in \Lambda}$ if and only if the function
\[ \xymatrix{ \Hom_{\sC}(C,Y) \ar[rr] & & \bigtimes_{\lambda\in\Lambda} \Hom_{\sC}(X_\lambda,Y)\\
\phi \ar@{|->}[rr] & &  ( \phi \circ i_\lambda )_{\lambda\in \Lambda}}\]
is a bijection for all objects $Y$: the universal property says that everything in the target comes from a unique thing in the source.
\end{rem}

\begin{prop} If $(P,\{p_\lambda:P\to X_\lambda\}_{\lambda\in \Lambda})$ and $(P',\{p'_\lambda:P'\to X_\lambda\}_{\lambda\in \Lambda})$ are both products for the same family of objects $\{X_\lambda\}_{\lambda\in \Lambda}$ in a category $\sC$, then there is a unique isomorphism $\alpha: P \xrightarrow{\sim} P'$ such that $p'_\lambda \circ \alpha = p_\lambda$ for all $\lambda$. The analogous statement holds for coproducts.
\end{prop}
\begin{proof} We will just deal with products. The following picture is a rough guide:
\[\xymatrix{ P \ar@{-->}[rr]^{\alpha} \ar@{~>}[drrrrr]_-{\{p_\lambda\}}   & & P' \ar@{-->}[rr]^{\beta} \ar@{~>}[drrr]^-{\{p'_\lambda\}} & & P \ar@{~>}[dr]^-{\{p_\lambda\}}   & \\
& & & & & \{ X_\lambda\} }\]

Since $(P,\{p_\lambda\})$ is a product and $(P',\{p'_\lambda\})$ is an object with maps to each $X_\lambda$, there is a unique map $\beta:P'\to P$ such that $p_\lambda\circ \beta = p'_\lambda$. Switching roles, we obtain a unique map $\alpha:P\to P'$ such that $p'_\lambda\circ \alpha = p_\lambda$. 

Consider the composition $\beta\circ \alpha:P\to P$. We have $p_\lambda \circ \beta \circ \alpha = p'_\lambda \circ \alpha = p_\lambda$ for all $\lambda$. The identity map $1_P:P\to P$ also satisfies the condition $p_\lambda \circ 1_P = p_\lambda$ for all $\lambda$, so by the uniqueness property of products, $\beta\circ \alpha = 1_P$. We can again switch roles to see that $\alpha\circ\beta = 1_{P'}$. Thus $\alpha$ is an isomorphism. The uniqueness of $\alpha$ in the statement is part of the universal property.
\end{proof}
\begin{exer} Prove the analogous statement for coproducts.\end{exer}



We use the notation \Def{$\prod_{\lambda \in \Lambda} X_\lambda$} to denote the (object part of the) product of $\{X_{\lambda}\}$ and \Def{$\coprod_{\lambda \in \Lambda} X_\lambda$} to denote the (object part of the) coproduct of $\{X_{\lambda}\}$.


Observe that products and coproducts are dual notions in the same way as monic versus epic morphisms. The product of a family in $\sC$ is the coproduct of the same family in $\sC^{\op}$.

\sssec{Products in familiar categories}

The familiar notion of Cartesian product or direct product serves as a product in many of our favorite categories. Let's note first that given a family of objects $\{X_\lambda\}_{\lambda\in \Lambda}$ in any of the categories $\Set,\Sgrp,\Grp,\Ring,\Mod{R},\Top$, the direct product $\bigtimes_{\lambda\in\Lambda} X_\lambda$ is an object of the same category:
\begin{itemize}
\item for sets, this is clear;
\item for semigroups, groups, and rings, take the operation coordinate by coordinate: $(x_{\lambda})_{\lambda\in \Lambda} \cdot (y_\lambda)_{\lambda\in \Lambda} = (x_\lambda \cdot y_\lambda)_{\lambda\in \Lambda}$;
\item for modules, addition is coordinate by coordinate, and the action is the same on each coordinate: $r\cdot (x_\lambda)_{\lambda\in \Lambda}=(r\cdot x_\lambda)_{\lambda\in \Lambda}$;
\item for topological spaces, use the product topology.
\end{itemize}
Note that this is not true for fields!

\begin{prop} In each of the categories $\Set,\Sgrp,\Grp,\Ring,\Mod{R}, \Top$, given a family $\{X_\lambda\}_{\lambda\in\Lambda}$, the direct product $\bigtimes_{\lambda\in\Lambda} X_\lambda$ along with the projection maps $\pi_{\lambda}:\bigtimes_{\gamma\in\Lambda} X_\gamma \to X_\lambda$ forms a product in the category.
\end{prop}
\begin{proof} We observe that in each category, the direct product is an object, and the projection maps $\pi_{\lambda}$ are morphisms in the category. 

Let $\sC$ be one of these categories, and suppose that we have morphisms $g_{\lambda}:Y \to X_\lambda$ for all $\lambda$ in $\sC$. We need to show there is a unique morphism $\phi:Y \to \bigtimes_{\lambda\in\Lambda} X_\lambda$ such that $\pi_\lambda \circ \phi = g_\lambda$ for all $\lambda$. The last condition
 is equivalent to $(\phi(y))_\lambda=(\pi_\lambda\circ\phi)(y) =g_\lambda(y)$ for all $\lambda$, which is equivalent to $\phi(y)=(g_\lambda(y))_{\lambda\in \Lambda}$, so if this is a valid morphism, it is unique.  Thus, it suffices to show that the map $\phi(y)=(g_\lambda(y))_{\lambda\in \Lambda}$ is a morphism in~$\sC$, which is easy to see in each case.
\end{proof}

%\begin{rem} We already saw that direct products are not products in the category of fields. In fact, there are no products in the category of fields in general. For example, suppose that $P$ was a product of $\F_p$ and $\F_q$ in $\Fld$ for two primes $p\neq q$. Then $\Hom_{\Fld}(K,P)$ is bijective with $\Hom_{\Fld}(K,\F_p) \times \Hom_{\Fld}(K,\F_q)$ for all $K$, so in particular, $\Hom_{\Fld}(\F_p,P)\neq 0$ and $\Hom_{\Fld}(\F_q,P)\neq 0$, but no such field exists!
%\end{rem}



\sssec{Coproducts in familiar categories}



\begin{ex} Let $\{X_\lambda\}_{\lambda\in \Lambda}$ be a family of sets. The product of $\{X_\lambda\}_{\lambda\in \Lambda}$ is given by the cartesian product along with the projection maps. The coproduct of $\{X_\lambda\}_{\lambda\in \Lambda}$ is given by the ``disjoint union'' with the various inclusion maps. By disjoint union, we simply mean union if the sets are disjoint; in general do something like replace $X_\lambda$ with $X_\lambda \times \{\lambda\}$ to make them disjoint.
\end{ex}





\begin{prop} Let $R$ be a ring, and $\{M_\lambda\}_{\lambda\in \Lambda}$ be a family of left $R$-modules. A coproduct for the family $\{M_\lambda\}_{\lambda\in \Lambda}$ is  $\left(\bigoplus_{\lambda \in \Lambda} M_\lambda, \{\iota_\lambda\}_{\lambda\in \Lambda}\right)$, where \[\bigoplus_{\lambda \in \Lambda} M_\lambda=\{ (x_\lambda)_{\lambda\in \Lambda} \ | \ x_\lambda\neq 0 \ \text{for at most finitely many} \ \lambda\} \subseteq \ \prod_{\lambda \in \Lambda} M_\lambda\] is the direct sum of the modules $M_\lambda$, and $\iota_\lambda$ is the inclusion map to the $\lambda$ coordinate.
\end{prop}

\Sept{1}

\begin{rem} If the index set $\Lambda$ is finite, then the objects $\prod_{\lambda \in \Lambda} M_\lambda$ and $\bigoplus_{\lambda \in \Lambda} M_\lambda$ are identical, but the product and coproduct are not the same since one involves projection maps and the other involves inclusion maps.
\end{rem}
\begin{proof}
Given $R$-module homomorphisms $g_\lambda:M_\lambda \to N$ for each $\lambda$, we need to show that there is a unique $R$-module homomorphism $\alpha: \bigoplus_{\lambda \in \Lambda} M_\lambda \to N$ such that $\alpha\circ \iota_\lambda = g_\lambda$. We define
\[ \alpha((m_\lambda)_{\lambda\in\Lambda}) = \sum_{\lambda\in \Lambda} g_\lambda(m_\lambda).\]
Note that since $(m_\lambda)_{\lambda\in\Lambda}$ is in the direct sum, at most finitely many $m_\lambda$ are nonzero, so the sum on the right hand side is finite, and hence makes sense in $N$. We need to check that $\alpha$ is $R$-linear; indeed,
\[ \alpha((m_\lambda) + (n_\lambda)) = \alpha((m_\lambda+n_\lambda)) = \sum g_\lambda(m_\lambda + n_\lambda) = \sum g_\lambda(m_\lambda) +\sum g_\lambda(n_\lambda) = \alpha((m_\lambda))+\alpha((n_\lambda)),\]
and the check for scalar multiplication is similar. For uniqueness of $\alpha$, note that $\bigoplus_{\lambda \in \Lambda} M_\lambda$ is generated by the elements $\iota_\lambda(m_\lambda)$ for $m_\lambda\in M_\lambda$. Thus, if $\alpha'$ also satisfies $\alpha' \circ \iota_\lambda = g_\lambda$ for all $\lambda$, then $\alpha(\iota_\lambda(m_\lambda))= g_\lambda(m_\lambda) = \alpha'(\iota_\lambda(m_\lambda))$ so the maps must be equal.
\end{proof}

\begin{rem} For any indexing set $\Lambda$, $\coprod_{\lambda\in \Lambda} R$ is a free $R$-module. If $R=K$ happens to be a field, then $\prod_{\lambda\in \Lambda} K$ is free, since all vector spaces are free modules, but in general, $\prod_{\lambda\in \Lambda} R$ is not free for an infinite set~$\Lambda$.
\end{rem}

\begin{rem}
\begin{itemize}
\item In $\Top$, disjoint unions serve as coproducts.
\item In $\Sgrp$ and $\Grp$, coproducts exist, and are given as free products. You may see or have seen them in topology in the context of Van Kampen's theorem.
\item In $\Ring$, the story is more complicated. Let's note first that disjoint unions won't work, since they aren't rings. Direct sums of infinitely many rings don't have $1$, so aren't rings, but even finite direct sums/products won't work, since the inclusion maps don't send $1$ to $1$. We will later on construct coproducts in \Def{$\cRing$}, the full subcategory of $\Ring$ consisting of commutative rings.
\end{itemize}
\end{rem}

%\sssec{Definition of direct limit}
%
%
%\begin{defn} We say a poset $P$ is \DEF{filtered} if for any $p,q\in P$, there is some $r\in P$ with $r\geq p$ and $r\geq q$.
%\end{defn}
%
%\begin{ex}
%\begin{itemize}
%\item Any totally ordered poset is filtered. The positive integers $\N_{> 0}$ are a key example.
%\item The collection of finite subsets of a set under containment is a filtered poset.
%\end{itemize}
%\end{ex}
%
%\begin{defn}
%Given a filtered poset $(\Lambda,\leq)$ and a category $\sC$, a \DEF{direct limit system} in $\sC$ indexed by $\Lambda$ or a \DEF{$\Lambda$-diagram} \emph{in $\sC$} is a commutative diagram with vertices corresponding to elements of $P$ and arrows corresponding to inequalities in $P$. To unpackage this, we have
%\begin{itemize}
%\item an object $X_\lambda\in \Ob(\sC)$ for each $\lambda\in \Lambda$
%\item a morphism $X_\lambda \xrightarrow{t_{\lambda}^{\mu}} X_{\mu}$ for each pair $\lambda \leq \mu$ 
%\end{itemize}
% such that
%\begin{enumerate}
%\item each $t_{\lambda}^{\lambda}$ is the identity on $X_\lambda$ and
%\item if $\lambda \leq \mu \leq \nu$, then $t_{\lambda}^{\nu} = t_{\mu}^{\nu}\circ t_{\lambda}^{\mu}$.
%\end{enumerate}
%
%\end{defn}
%
%\begin{ex} A direct limit system in a category $\sC$ indexed by $\N_{>0}$ is determined by a diagram of the form 
%\[ X_1 \xrightarrow{a_1} X_2  \xrightarrow{a_2} X_3 \xrightarrow{a_3} X_4 \xrightarrow{a_4} X_5 \rightarrow \cdots.\]
%All the other maps $X_i \to X_j$ for $i<j$ are given by composition: $a_{j-1} \circ \cdots \circ a_i$.
%\end{ex} 
%
%
%
%
%
%\begin{defn} Let $\sC$ be a category, $\Lambda$ be a filtered poset, and $\{ X_\lambda\}_{\lambda\in \Lambda}$ be a direct limit system indexed by $\Lambda$. A \DEF{direct limit} of $\{ X_\lambda\}_{\lambda\in \Lambda}$ is given by an object $C$ and a family of morphisms $\{i_\lambda : X_\lambda \to C\}_{\lambda\in \Lambda}$ such that for all $\lambda\leq \mu$, $i_{\lambda}=i_\mu \circ t_{\lambda,\mu}$, that is universal in the following sense:
%
%Given an object $Y$ and a family of morphisms $g_\lambda:X_\lambda \to Y$ that satisfy the condition for all $\lambda\leq \mu$, $g_{\lambda}=g_\mu \circ t_{\lambda}^{\mu}$, there is a unique morphism $\alpha: C\to Y$ such that $\alpha \circ i_\lambda = g_\lambda$ for all $\lambda$.
%\end{defn}
%
%Here is a diagram for the (first few) maps involved when $\Lambda=\N$:
%\[\xymatrix{  & & & C \ar@{-->}[ddrr]^{\phi}& & \\
%\vdots\ar[urrr] \ar[drrrrr] & & & & & \\
%X_3\ar[uurrr]^-{i_3} \ar[u]^-{t_3^4} \ar[rrrrr]^-{g_3} & & & & & Y\\
%X_2\ar[uuurrr]^-{i_2} \ar[u]^-{t_2^3} \ar[urrrrr]^-{g_2}  & & & & & \\
%X_1\ar[uuuurrr]^-{i_1} \ar[u]^-{t_1^2} \ar[uurrrrr]^-{g_1} & & & & &}\]
%
%We can also take a ``big picture'' view of the universal property:
%\[\xymatrix{ & C\ar@{-->}[dr]^{\phi} & \\
%\stackrel{\{ X_{\lambda}\}}{\longrightarrow} \ar@{~>}[ur]^{\{i_\lambda\}} \ar@{~>}[rr]_{\{g_\lambda\}} & & Y,}\]
%where $\stackrel{\{ X_{\lambda}\}}{\longrightarrow}$ represents the collection of $X$'s and the maps between them, and
%where the squiggly arrows are now collections of maps that commute with the maps between the $X$'s.
%
%
%These are not guaranteed to exist, but are unique when they do, by an argument analogous to that for coproducts.
%
%\sssec{Constructions of direct limits}
%
%\begin{ex} Let $\sC$ be $\Set$, $\Grp$, $\Ab$, $\Ring$, $\Mod{R}$, or $\Top$.
%Let's start with the special case where all of the $X_\lambda$ are subobjects of some fixed object $Z$, and the transition maps are inclusion maps. In this case, the union is a subobject of $Z$ (for $\Grp$, $\Ab$, $\Ring$, $\Mod{R}$, the union is closed under the operations since any pair of elements lie in a common $X_\lambda$), and the inclusion maps $\iota_\lambda: X_\lambda \to \bigcup_{\gamma\in \Lambda} X_\gamma$ commute with the transition maps. In this case, a direct limit is $(\bigcup_{\lambda\in \Lambda} X_\lambda, \iota_\lambda)$. The point is that a bunch of consistent maps from the sets $X_\lambda$ uniquely determine a map from their union. In this case, we call the direct limit a \DEF{directed union}.\end{ex}
%
%\begin{prop} Let $\sC$ be $\Set$, $\Grp$, $\Ab$, $\Ring$, $\Mod{R}$, or $\Top$, and let $\{X_\lambda\}_{\lambda\in \Lambda}$ be a directed system on a poset $\Lambda$ in $\sC$. A direct limit for $\{X_\lambda\}$ is given by 
%\begin{itemize}
%\item the disjoint union of $\{X_\lambda\}$, where we will write $(x,\lambda)$ for an element $x \in X_\alpha$
%\item modulo the equivalence relation $\sim$ generated by
%\[ (x,\alpha) \sim (t_\alpha^\beta(x), \beta) \qquad \text{for all $\alpha\leq \beta$ in $\Lambda$ and all $x\in X_\alpha$},\]
%\item with the maps $i_\alpha$ that send each element in $X_\alpha$ to its equivalence class.
%\end{itemize}
%\end{prop}
%\begin{proof} First, we show it for sets. We have $g_\lambda(x) = \phi \circ i_\lambda (x) = \phi( [(x,\lambda)])$ for all $x\in X_\lambda$, so this is our map $\phi$ is unique and given by this formula as well as this is well-defined. We note that the equivalence relation can be realized as
%\[ (x,\alpha) \sim (y,\beta) \quad \Longleftrightarrow \quad \exists \gamma \geq \alpha,\beta : t_\alpha^\gamma(x) = t_\beta^\gamma(y).\]
%Thus, if $(x,\alpha)\sim (y,\beta)$, then 
%\[g_\alpha(x) = g_\gamma \circ t_\alpha^\gamma(x) = g_\gamma \circ t_\beta^\gamma(y) = g_\beta(y),\]
%so $\phi$ is well-defined by this formula. This concludes the proof for $\Set$.
%
%The cases of the other categories $\sC$ will follow as long as the object specified is an object in $\sC$ and the morphisms $i_\alpha$ and $\phi$ are morphisms in $\sC$. To see that our proposed direct limit object is an object of the same sort, we need to that it admits the same operations. Let's explain this for $\Grp$; the cases $\Ab$, $\Ring$, $\Mod{R}$ will be similar. We define $[(x,\alpha)] \cdot [(y,\beta)]$ to be $[(t_\alpha^\gamma(x) \cdot t_\beta^\gamma(y), \gamma)]$ for some $\gamma\geq\alpha,\beta$. If we choose some other $\delta\geq \alpha,\beta$, then for some $\varepsilon\geq \gamma,\delta$, we have
%\[ t_\gamma^\varepsilon(t_\alpha^\gamma(x) \cdot t_\beta^\gamma(y) ) = t_\gamma^\varepsilon(t_\alpha^\gamma(x))\cdot  t_\gamma^\varepsilon(t_\beta^\gamma(x)) = t_\delta^\varepsilon(t_\alpha^\delta(x))\cdot  t_\delta^\varepsilon(t_\beta^\delta(x))= t_\delta^\varepsilon(t_\alpha^\delta(x) \cdot t_\beta^\delta(y) ) ,\]
%(using that our morphisms in $\Grp$ preserve $\cdot$),
%which means that 
%\[ [(t_\alpha^\gamma(x) \cdot t_\beta^\gamma(y), \gamma)] = [(t_\alpha^\delta(x) \cdot t_\beta^\delta(y) ,\delta)].\] 
%To check that the operation is associative, for any three elements we can find an $X_\lambda$ where they all have a representative, and associativity in $X_\lambda$ implies those three associate. Similarly for checking the other axioms of the other structures. It's then easy to see that the maps $i_\lambda$ and $\phi$ are morphisms in the respective categories.
%\end{proof}

\ssec{Functors}

\begin{defn} Let $\sC$ and $\sD$ be categories. A \DEF{covariant functor} $F:\sC \to \sD$ is a mapping that assigns to each object $A\in \Ob(\sC)$ an object $F(A) \in \Ob(\sD)$ and to each morphism $\alpha\in \Hom_{\sC}(A,B)$ a morphism $F(\alpha)\in \Hom_{\sD}(F(A),F(B))$ such that
\begin{enumerate}
\item $F$ preserves compositions, meaning $F(\alpha \circ \beta) = F(\alpha) \circ F(\beta)$ for all morphisms $\alpha,\beta$ in $\sC$, and
\item $F$ preserves identity morphisms, meaning $F(1_A) = 1_{F(A)}$ for all objects $A$ in $\sC$.
\end{enumerate}

A \DEF{contravariant functor} $F:\sC \to \sD$ is a mapping that assigns to each object $A\in \Ob(\sC)$ an object $F(A) \in \Ob(\sD)$ and to each morphism $\alpha\in \Hom_{\sC}(A,B)$ a morphism $F(\alpha)\in \Hom_{\sD}(F(B),F(A))$ such that
\begin{enumerate}
\item $F$ preserves compositions, meaning $F(\alpha \circ \beta) =  F(\beta) \circ F(\alpha)$ for all morphisms $\alpha,\beta$ in $\sC$, and
\item $F$ preserves identity morphisms, meaning $F(1_A) = 1_{F(A)}$ for all objects $A$ in $\sC$.
\end{enumerate}
\end{defn}

\begin{rem} One can also interpret a contravariant functor as a covariant functor from $\sC^{\op} \to \sD$, or as a covariant functor from $\sC \to \sD^{\op}$.
\end{rem}


\begin{ex} Here are some examples of functors.
\begin{enumerate}
\item Many of the categories we considered before are sets with extra structure, and the morphisms are functions that preserve the extra structure. The \DEF{forgetful functor} from such a category $\sC$ to $\Set$, is the covariant functor that forgets that extra structure are returns the underlying set of the object. For example the forgetful functor $\Grp \to \Set$ sends each group to its set of elements, and each homomorphism to its corresponding function of sets. Along similar lines, a ring is a group under addition with the bonus structure of multiplication, and we can talk about the forgetful functor from $\Ring$ to $\Grp$, etc.
\item The \DEF{identity functor} \Def{$1_{\sC}$} on any category $\sC$ sends each object to itself and each morphism to itself. It is covariant.
\item There is a covariant functor $(-)^{\times 2}:\Set\to\Set$ that sends every set $S$ to its cartesian square $S\times S$, and every function $f:S\to T$ to the function $(f,f): S\times S \to T\times T$ that sends $(s_1,s_2)\mapsto (f(s_1),f(s_2))$. Let's check the axioms: given $g:S\to T$ and $f:T\to U$, we need to see that $(f, f)\circ (g, g) = (f\circ g,f\circ g)$, which is clear, and that $(1_S,1_S)$ is the identity map on $S\times S$, which is also clear.
\item Given a group $G$, the subgroup $G'\leq G$ generated by the set of commutators $\{ghg^{-1}h^{-1} \ | \ g,h\in G\}$ is a normal subgroup, and the quotient $G^{\mathrm{ab}}:=G/G'$\index{$G^{\mathrm{ab}}$} is called the \DEF{abelianization} of $G$. The group $G^{\mathrm{ab}}$ is abelian. Given a group homomorphism $\phi:G\to H$, $\phi$ automatically takes commutators to commutators, so it induces a homomorphism $G^{\mathrm{ab}} \to H^{\mathrm{ab}}$. Put together, abelianization gives a covariant functor from $\Grp$ to $\Ab$. 
%Alternatively, we can think of abelianization as a functor from $\Grp \to \Grp$.
\item Given any topological space $X$, the set of continuous functions from $X$ to $\R$, \Def{$\Cont(X,\R)$} is a ring with pointwise addition and multiplication. Given a continuous map $X\xrightarrow{\alpha}Y$, and a continuous map $Y \xrightarrow{f} \R$, the composition $X\xrightarrow{\alpha \circ f} \R$ is a continuous function. In this way, we get a map from $\Cont(Y,\R)$ to $\Cont(X,\R)$. In fact, this map is a ring homomorphism. Put together, we obtain a contravariant functor from $\Top$ to $\Ring$.
\item Fix a field $K$. Given a vector space $V$, the collection \Def{$V^*$} of linear transformations from $V$ to $K$ is again a $K$-vector space, the \DEF{dual vector space} of $V$. If $\phi:W\to V$ is a linear transformation and $\ell:V\to K$ is in $V^*$ then $\ell \circ \phi:W\to K$ is in $W^*$, so there is a map $V^* \to W^*$. You can check that this together forms a functor \Def{$(-)^*$} that is contravariant.
\item You may be familiar with the fundamental group of a pointed topological space; this is a group $\pi_1(X,x)$ assigned to a topological space and a point in it. The rule $\pi_1$ gives a functor from pointed topological spaces to groups.
\item The unit group functor $\Ring\to \Grp$ sends each ring to its group of units. A homomorphism of rings restricts to a group homomorphism on the units: if $x\in R$ is a unit, so $xy=1$, and $\phi:R\to S$ is a group homomorphism, then $1=\phi(xy)=\phi(x) \phi(y)$, so $\phi(x)$ is a unit; $\phi$ preserves multiplication as well. This is covariant.
\end{enumerate}
\end{ex}

\Sept{3}

It follows from the definition of covariant functor that if we apply a covariant functor $F$ to a commutative diagram, we get another commutative diagram of the same shape, e.g.:
\[ 
\xymatrix{
A \ar[r]^{\alpha} \ar[d]_{\gamma}& B \ar[d]^{\beta}\\
C \ar[r]^{\delta} &D } \qquad \stackrel{F}{\rightsquigarrow} \qquad 
\xymatrix{
F(A) \ar[r]^{F(\alpha)} \ar[d]_{F(\gamma)}& F(B) \ar[d]^{F(\beta)}\\
F(C) \ar[r]^{F(\delta)} &F(D).
}
\]



If we apply a contravariant functor $G$ to a commutative diagram, we get a commutative diagram of the same shape with the arrows reversed, e.g.:
\[ 
\xymatrix{
A \ar[r]^{\alpha} \ar[d]_{\gamma}& B \ar[d]^{\beta}\\
C \ar[r]^{\delta} &D } \qquad \stackrel{G}{\rightsquigarrow} \qquad 
\xymatrix{
G(A)  & G(B) \ar[l]_{G(\alpha)} \\
G(C) \ar[u]^{G(\gamma)} &G(D) \ar[u]_{G(\beta)} \ar[l]_{G(\delta)} .
}
\]


\begin{rem} A composition of two covariant functors, or of two contravariant functors, is a covariant functor. The composition of a covariant functor and a contravariant functor, or vice versa, is a contravariant functor.
\end{rem}

\begin{comment}
Here is another simple but essential source of functors.

\begin{ex} Let $\sC$ be a category, and $A$ be an object of $\sC$.
\index{Hom functors}
		\begin{itemize}
		\item The covariant functor \Def{$\Hom_{\sC}(A,-)$} $: {\sC} \longrightarrow  {\bf Set}$ sends each object $B$ to the set $\Hom_{\sC}(A,B)$, and each $\alpha \in \Hom_{\sC}(B,C)$ to the function \Def{$\Hom_{\sC}(A,\alpha)$}  $=:$ \Def{$\alpha_*$} of ``postcomposition by $\alpha$'':
		$$\xymatrix@R=1mm{\Hom_{\sC}(A,B) \ar[r]^{\alpha_*} & \Hom_{\sC}(A,C) \\f \ar@{|->}[r] & \alpha\circ f
		\\ A \xrightarrow{f}B  & A \xrightarrow{f} B \xrightarrow{\alpha} C.  }$$
		\item The contravariant functor \Def{$\Hom_{\sC}(-,A)$} $: {\sC}\longrightarrow  {\bf Set}$ sends each object $B$ to the set $\Hom_{\sC}(B,A)$, and each $\alpha \in \Hom_{\sC}(B,C)$ to the function \Def{$\Hom_\mathscr{C}(\alpha,A)$} $=:$ \Def{$ \alpha^*$} of ``precomposition by $\alpha$'':
		$$\xymatrix@R=1mm{\Hom_{\sC}(C,A) \ar[r]^{\alpha^*} & \Hom_{\sC}(B,A) \\f \ar@{|->}[r] & f\circ \alpha
		\\ C\xrightarrow{f} A & B\xrightarrow{\alpha} C \xrightarrow{f} A.}$$
		\end{itemize}
		We may say that $\Hom_{\sC}(A,-)$ is the \DEF{covariant functor represented by $A$} and $\Hom_{\sC}(-,A)$ is the \DEF{contravariant functor represented by $A$}.
		
		The contravariant Hom functor does preserve compositions in a contravariant way:
given maps
\[ \xymatrix{ 								& C \ar[dr]^{\beta} 	& 	\\
		B \ar[ur]^\alpha \ar[rr]_{\beta\circ \alpha} 	&				& D }\]
		we get
		
{\[\xymatrix@R-1.2pc@C-1pc{ 
					&				& \Hom_\sC(C,A)  \ar[lldddd]_{\alpha^*} 		&				&\\
					&             	    	 	&{\color{blue}f \circ \beta} \ar@{|->}@[blue][ddl]	& 				&\\
					&				&							&				&\\
					&{\color{blue}(f\circ \beta)\circ \alpha }	&							&  {\color{blue}f}\ar@{|->}@[blue][uul]  \ar@{|->}@[blue][ll] &&\\
\Hom_\sC(B,A)	&				&							&				&
\Hom_\sC(D,A).\ar[uuuull]_{\beta^*} \ar[llll]^-{(\beta\circ\alpha)^*}}\]}
		\end{ex}
		
	\begin{exer}
	Let $\alpha$ be a morphism in a category $\sC$.
	\begin{enumerate}
	\item Show that $\alpha$ is monic if and only if $\Hom_\sC(Y,\alpha)$ is injective for all objects $Y$.
	\item Show that $\alpha$ is epic if and only if $\Hom_\sC(\alpha,Y)$ is injective for all objects $Y$.
	\item Show that $\alpha$ has a right inverse if and only if $\Hom_\sC(Y,\alpha)$ is surjective for all objects $Y$.
	\item Show that $\alpha$ has a left inverse if and only if $\Hom_\sC(\alpha,Y)$ is surjective for all objects $Y$.
	\end{enumerate}
	\end{exer}
	\end{comment}

\ssec{Natural transformations}

\begin{defn}
	Let $F$ and $G$ be covariant functors $\mathscr{C} \longrightarrow \mathscr{D}$. A \DEF{natural transformation} $\eta$ between $F$ and $G$ is a mapping that to each object $A$ in $\mathscr{C}$ assigns a morphism $\eta_A \in \Hom_\mathscr{D}(F(A),G(A))$ such that for all $f \in \Hom_\mathscr{C}(A,B)$, the diagram
	$$\xymatrix{F(A) \ar[d]_-{F(f)} \ar[r]^-{\eta_A} & G(A) \ar[d]^-{G(f)} \\ F(B) \ar[r]_-{\eta_B} & G(B)}$$
	commutes. We sometimes write $\eta: F \implies G$\index{$F\implies G$}.
	
	A \DEF{natural isomorphism} is a natural transformation $\eta$ where each $\eta_A$ is an isomorphism. 
\end{defn}

In short, a natural transformation is a rule to turn $F$ of whatever into $G$ of whatever in a reasonable way.

\begin{exer} Let $F,G: \sC \to \sD$ be covariant functors. Show that a natural transformation $\eta: F\implies G$ is a natural isomorphism if and only if there is another natural transformation $\mu:G\implies F$ such that $\mu \circ \eta$ is the identity natural isomorphism on $F$ and $\eta\circ \mu$ is the identity natural transformation on $G$.
\end{exer}

We can make make a similar definition for contravariant functors.

\begin{defn}
	Let $F$ and $G$ be contravariant functors $\mathscr{C} \longrightarrow \mathscr{D}$. A \DEF{natural transformation} between $F$ and $G$ is a mapping that to each object $A$ in $\mathscr{C}$ assigns a morphism $\eta_A \in \Hom_\mathscr{D}(F(A),G(A))$ such that for all $f \in \Hom_\mathscr{C}(A,B)$, the diagram
	$$\xymatrix{F(A)  \ar[r]^-{\eta_A} & G(A)  \\ F(B) \ar[u]^-{F(f)} \ar[r]_-{\eta_B} & G(B) \ar[u]_-{G(f)}}$$
	commutes. \end{defn}


\begin{ex} 
Let's describe a natural transformation of functors $\eta:(-)^{\times 2} \implies 1_{\Set}$, namely we take 
\[\eta_S:S\times S \to S \qquad (s_1,s_2) \mapsto s_1. \]
We need to check for every map $f:S\to T$ the commutativity of a diagram:
$$\xymatrix{S\times S \ar[d]_-{(f,f)} \ar[r]^-{\eta_{S}} & S \ar[d]^-{f} \\ T\times T \ar[r]_-{\eta_{T}} & T.}$$
Going either down then left or right then down, $(s_1,s_2)$ maps to $f(s_1)$, so this does commute, and we indeed have a natural transformation. This is not a natural isomorphism, since the map $\eta_S$ is not always (almost never) an isomorphism of sets. 
\end{ex}


\begin{ex}
Let $\sC$ be the full subcategory of $\Set$ consisting of countable sets. For every $S\in \Ob(\sC)$, there is a $\sC$-isomorphism, i.e., a bijection, $\eta_S:S \to S\times S$. Namely, we can take $\eta_S$ as follows: enumerate $S$ as $S=\{s_1,s_2,s_3,\dots\}$, and do the usual zigzag trick\\
\begin{minipage}[c]{0.4 \textwidth}
\[\begin{split}
s_1&\mapsto (s_1,s_1)\\
s_2&\mapsto (s_2,s_1)\\
s_3&\mapsto (s_1,s_2)\\
s_4&\mapsto (s_1,s_3)\\
s_5&\mapsto (s_2,s_2)\\
s_6&\mapsto (s_3,s_1)\\
&\vdots
\end{split}\]
\end{minipage}
\begin{minipage}[c]{0.4 \textwidth}
\xymatrix@C=1em@R=1em{ 
\vdots & \vdots & \vdots & \vdots &  \reflectbox{$\ddots$} \\
(s_1,s_4) \ar@{.}@[blue][u] & (s_2,s_4) \ar@{->}@[blue][dr] \ar@{.}@[blue][ul]& (s_3,s_4)  \ar@{.}@[blue][ul] & (s_4,s_4) \ar@{.}@[blue][ul] \ar@{.}@[blue][dr] & \cdots \\
(s_1,s_3) \ar@{->}@[blue][dr] & (s_2,s_3) \ar@{->}@[blue][ul] & (s_3,s_3) \ar@{->}@[blue][dr] & (s_4,s_3) \ar@{->}@[blue][ul] \ar@{.}@[blue][dr]& \cdots \\
(s_1,s_2) \ar@{->}@[blue][u] & (s_2,s_2)\ar@{->}@[blue][dr]  & (s_3,s_2)\ar@{->}@[blue][ul]  & (s_4,s_2) \ar@{.}@[blue][dr] & \cdots \\
(s_1,s_1)\ar@{->}@[blue][r]  & (s_2,s_1)\ar@{->}@[blue][ul]   & (s_3,s_1)\ar@{->}@[blue][r]   & (s_4,s_1)\ar@{->}@[blue][ul]  & \cdots }
\end{minipage}

\


However, the bijections $\eta_S$ do not form a natural bijection (in fact, if we just choose $\eta_S$ like so for one set $S$, no matter what the other choices are, we can't get a natural transformation). Let $f:S\to S$ satisfy $f(s_1)=s_2$ and $f(s_2)=s_1$. Then in the diagram
$$\xymatrix{S \ar[d]_-{f} \ar[r]^-{\eta_S} & S \times S \ar[d]^-{(f,f)} \\ S \ar[r]_-{\eta_S} & S\times S,}$$
we have $\eta_S(f(s_1)) =(s_2,s_1)$ while $(f,f)(\eta_S(s_1)) = (s_2,s_2)$, so the diagram does not commute.

Intuitively, we can blame the fact that our map $\eta$ decided on a \emph{choice} of enumeration of the set.

\end{ex}


\begin{ex}
Recall the contravariant functor $(-)^*:\vs{K}\to \vs{K}$; here we are restricting to finite dimensional vector spaces. 

For every $V\in \vs{K}$, there is an isomorphism $V\cong V^*$: if we fix a basis $\cB$ for $V$, there is a dual basis for $V^*$ (the $\cB$-coordinate functions) of the same size, so they are isomorphic. However, there is no natural isomorphism $\eta: 1_{\vs{K}} \implies (-)^*$, since $1_{\vs{K}}$ is covariant and $(-)^*$ is contravariant. We will actually see a more compelling version of this nonnatruality statement in the homework.

Composing the dual functor with itself twice we get the covariant double-dual functor $(-)^{**}:\vs{K}\to\vs{K}$. We will show that there is a natural isomorphism $1_{\vs{K}}\implies (-)^{**}$.

For every $v\in V$, there is a map $\mathrm{ev}_v:V^{*}\to V$ given by evaluation at $v$: $\mathrm{ev}_v(\ell) = \ell(v)$. So, $\mathrm{ev}_v\in V^{**}$. Since we have one for each $v$, there is a function $\mathrm{ev}:V\to V^{**}$ given by $\mathrm{ev}(v)=\mathrm{ev}_v$.

The map $\mathrm{ev}$ is a linear transformation:
\[ \mathrm{ev}_{cv+w} (\ell) = \ell(cv+w) = c\ell(v) +\ell(w) = c \,\mathrm{ev}_v(\ell) + \mathrm{ev}_w(\ell).\]
It is injective, since any nonzero vector takes on a nonzero value for some linear functional. It is then a bijection since $\dim(V)=\dim(V^{*})=\dim(V^{**})$.

We just need to check commutativity of the square:
\[\xymatrix{ V \ar[r]^{\mathrm{ev}} \ar[d]^{\phi} & V^{**} \ar[d]^{\phi^{**}} \\ W \ar[r]^{\mathrm{ev}} & W^{**} }\]
This translates to \[ \mathrm{ev}_{\phi(v)} = (\mathrm{ev} \circ \phi) (v) \stackrel{?}{=} (\phi^{**} \circ \mathrm{ev})(v) = \phi^{**}(\mathrm{ev}_v)\]
in $W^{**}$ for all $v\in V$. But, for all $\ell\in W^{*}$,
\[ \mathrm{ev}_{\phi(v)}(\ell)=\ell(\phi(v)) = \phi^*(\ell)(v) = (\mathrm{ev}_v \circ \phi^*)(\ell) = \phi^{**}(\mathrm{ev}_v)(\ell),\]
so the equality holds.
\end{ex}

In the homework, we will discuss some more examples from linear algebra. For example, for a pair of vector spaces $W\leq V$, there are isomorphisms $V\cong W\oplus V/W$, but no natural isomorphism of the sort. On the bright side, we will see that if $V$ has an inner product, then $V$ and $V^*$ are naturally isomorphic in a suitable sense.

\begin{comment}
\subsection{Equivalence of categories}

There is an obvious notion of isomorphism of categories: two categories $\cC$ and $\cD$ are isomorphic if there are covariant functors $F:\cC\to \cD$ and $G:\cD\to \cC$ such that $G\circ F= 1_{\cC}$ and $F\circ G = 1_{\cD}$. The following more flexible notion is much more useful:

\begin{defn} Let $\cC$ and $\cD$ be categories. A pair of covariant functors $F:\cC\to \cD$ and $G:\cD\to \cC$ is an \DEF{equivalence of categories} if there are natural isomorphisms $G\circ F\Rightarrow 1_{\cC}$ and $F\circ G \Rightarrow 1_{\cD}$. We say that $\cC$ and $\cD$ are \emph{equivalent} if there exists an equivalence of categories between them.
\end{defn}

This notion has a more concrete characterization:

\begin{fact} A pair of covariant functors $F:\cC\to \cD$ and $G:\cD\to \cC$ is an equivalence of categories if and only
~if
\begin{itemize}
\item for every $A,B\in\Ob(\cC)$, the map $\Hom_{\cC}(A,B) \to \Hom_{\cD}(F(A),F(B))$ given by $\alpha\to F(\alpha)$ is a bijection, and,
\item for every $D\in \Ob(\cD)$, there is some $C\in\Ob(\cC)$ such that $D$ is isomorphic to $F(C)$.
\end{itemize}
\end{fact}
We won't really use this fact, but it does add some meaning to this notion.


%\begin{eg} Partial functions
%\end{eg}

\begin{ex} Recall the category $\MaT{K}$ of matrices over a field: the objects are positive natural numbers, and the morphisms from $m \to n$ are $n\times m$ matrices over $K$. For convenience, let $\vsp{K}$ be the category of finite-dimensional $K$-vector spaces with the zero-dimensional ones and all morphisms to and from them removed.

It turns out that $\MaT{K}$ is equivalent to $\vsp{K}$: we can take a functor $F:\MaT{K} \to \vsp{K}$ that sends $n$ to $K^n$ and $M$ to multiplication by $M$, and we can construct a functor $G:\vsp{K} \to \MaT{K}$ by picking a basis for every $V\in \vsp{K}$, sending $V$ to its dimension, and every transformation to the matrix in the corresponding bases for the source and target.
\end{ex}
\end{comment}

\sec{$R$-Modules}


\Sept{8}


\sssec{Left vs right vs both}

Recall that a left $R$-module is an abelian group $M$ with an action map $R\times M \to M$ written $(r,m)\mapsto rm$ such that $r(sm) = (rs)m$, along with two distributive properties and the condition that $1$ acts as the identity.
A \DEF{right module} over $R$ is defined similarly; we usually write the action as $(r,m)\mapsto mr$, and we have $(mr)s = m(rs)$, along with distributive and identity properties. The point is that when we act by a product $rs\in R$, we can think of it as an iterated action; in a left module, the left factor acts last while in a right module the right factor acts last.

\begin{defn} If $R$ is a ring, the \DEF{opposite ring} \Def{$R^{\mathrm{op}}$} is the ring with the same underlying set and same addition, but with multiplication given by $r \cdot_{R^{\mathrm{op}}} s = s \cdot_{R} r$.
\end{defn}

A right $R$-module is exactly the same thing as a left $R^{\mathrm{op}}$-module (except our convention for writing the action). In particular, if $R$ is commutative, then a left $R$-module is exactly the same thing as a right $R$-module, and we will just say ``module'' in this case. By default, in general, when we say module, we will mean left $R$-module.

\begin{ex}
Let $R$ be a ring. The collection \Def{$M_n(R)$} of $n\times n$ matrices with entries in $R$ forms a ring that in general is not commutative. The set \Def{$R^n$} of column vectors of length $n$ with entries in $R$ is naturally a left $M_n(R)$-module. The collection of row vectors of length $n$ with entries in $R$ is naturally a right $M_n(R)$-module. We can also identify this latter action with a right module action on $R^n$ by transposing any column vector into a row vector, acting, then transposing back:
\[ v \cdot M = (v^T M)^T=M^T v.\]
\end{ex}

We can think of $R$-module structures in a different way. To prepare, let's record a lemma.

\begin{lem}
If $M$ is an abelian group, then \Def{$\End_{\Ab}(M)$} $:=\Hom_{\Ab}(M,M)$ forms a ring with pointwise addition and composition as multiplication. More generally, if $M$ is a left $R$-module, then \Def{$\End_{R}(M)$}$:=\Hom_{\Mod{R}}(M,M)$ forms a ring (with the aforementioned operations).
\end{lem}
\begin{proof}
The first statement is a special case of the first, since an abelian group is the same thing as a $\Z$-module, so we'll prove the second. Let $f,g\in \End_R(M)$. Since 
  \[(f+g)(rm+n) = f(rm+n)+g(rm+n)= rf(m)+f(n) +rg(m)+g(n) = r(f+g)(m) + (f+g)(n)\]
we see that $f+g\in \End_{R}(M)$. It's easy to see that $\End_{R}(M)$ is an abelian group under $+$.
Associativity of multiplication is a special case of associativity of composition of functions. For distributive laws, we have
\[\begin{aligned} ((f+g)h) (m) &= (f+g)(h(m)) =f(h(m)) + g(h(m)) = (fh)(m) + (gh)(m) 
\\ (f(g+h)) (m) &= f(g(m)+h(m)) = f(g(m)) + f(h(m)) = (fg) (m) + (fh)(m);
\end{aligned}\]
for the latter distributive law, it was crucial that we are dealing with homomorphisms of abelian groups. We also have the identity map on $M$ as a mutliplicative identity.
\end{proof}


\begin{exer} Show that there is a ring isomorphism $\End_R(R) \cong R^{\mathrm{op}}$.
\end{exer}

\begin{prop}
Let $R$ be a ring and $(M, +)$  an abelian group. There is a bijective correspondence
\[ \xymatrix@R=.6em{ \{ R-\text{module actions} \ R \times M \to M \text{(with given +)} \}  \ar@{<->}[r] & \{ \text{ring homomorphisms} \ \rho: R \to \End_{\Z}(M) \}\\
\cdot  \ar@{|->}[r] & \rho(r)(m) = r\cdot m \\
r\cdot m = \rho(r)(m)  & \ar@{|->}[l] \rho.}
\]
\end{prop}
\begin{proof}
We clearly have a bijection as long as the maps are well-defined.

Given an $R$-module action $\cdot$, one distributive property translates to the condition that $\rho(r)$ is $\Z$-linear; the identity condition means $\rho(1_R)$ is the identity function on $M$, which is the $1$ element in $\End_\Z(M)$; the other distributive condition means $\rho$ preserves addition; and the associativity condition means $\rho$ preserves multiplication. Thus, $\rho$ is a ring homomorphism. And conversely. 
\end{proof}

It turns out that we often have a left module structure and a right module structure on something in a compatible way.

\begin{defn} Let $R$ and $S$ be rings. An $(R,S)$-\DEF{bimodule} is an abelian group $M$ equipped with a left $R$-module structure and a right $S$-module structure that commute with each other:
\[ (r \cdot m) \cdot s = r \cdot (m \cdot s) \qquad \text{for all}\ m\in M, r\in R, s\in S.\]
\end{defn} 

\begin{ex} Here are some basic sources of bimodules:
\begin{enumerate}
\item If $R$ is a ring, then $M=R$ is an $(R,R)$-bimodule in the obvious way. More generally, if $\phi:A\to R$ is a ring homomorphism, then $R$ is an $(R,A)$-bimodule by
\[ s \cdot r \cdot a = sr\phi(a) \qquad \text{for} \ r,s\in R, a\in A;\]
equally well, $R$ is an $(A,R)$ or $(A,A)$-bimodule.
\item If $R$ is a commutative ring and $M$ is any left module, then $M$ is also a right module by the same action, and $M$ is an $(R,R)$-bimodule with these structures. I.e., starting with an action $r\cdot m$, we set $m\cdot s$ to be $s\cdot m$, and \[(r\cdot m) \cdot s = s\cdot (r\cdot m) = sr \cdot m = rs \cdot m = r\cdot (s\cdot m) = r \cdot (m\cdot s).\]
\item Every left $R$-module is automatically an $(R,\Z)$-bimodule in a unique way: \[(r\cdot m) \cdot n= \underbrace{(r\cdot m) + \cdots + (r\cdot m)}_{n \ \text{times}} = r \cdot (\underbrace{m+\cdots + m}_{n \ \text{times}}) = r \cdot (m \cdot n) \qquad \text{for $n\in \Z_{\geq 0}$},\]
and similarly for $n\leq 0$.
Likewise, every right $R$-module is automatically a $(\Z,R)$-bimodule.
\end{enumerate}
\end{ex}

\begin{ex} For a ring $R$, the set of column vectors of length $n$, $R^n$, is a $(M_n(R),R)$-bimodule. However, if we take the natural left action together with the right action $v\cdot M = M^T v$ discussed above, we do not get a bimodule structure, since $(M\cdot v)\cdot N = N^T M v$ generally differs from $M \cdot (v \cdot N) = M N^T v$.
\end{ex}

Sometimes, when we want to keep track of various module and bimodule structures, we may write something like \Def{$_R M _S$} to indicate that $M$ is an $(R,S)$-bimodule, or $_R M$ to indicate that $M$ is a left $R$-module.

\ssec{Kernels, images, and exact sequences}

To every homomorphism $\phi:M\to N$ in $\Mod{R}$, the kernel \Def{$\ker(\phi)$}$\subseteq M$ and image \Def{$\im(\phi)$}$\subseteq N$ are in $\Mod{R}$, and the inclusion maps are homomorphisms of $R$-modules. It is surprisingly convenient to keep track of and compare these data in terms of exact sequences.

\begin{defn}
 A sequence of $R$-modules and $R$-module maps of the form
  $$
  \cdots \xra{d_{i+1}}  M_i \xra{d_i} M_{i-1} \xra{d_{i-1}} \cdots
    $$
    (possible infinite, possibly not)
    is a \DEF{chain complex}, or just \DEF{complex} for short, if $d_i \circ d_{i+1} = 0$ for all $i$ or, equivalently, $\im(d_{i+1}) \subseteq \ker(d_i)$ for all $i$. 
    
    A chain complex is \DEF{exact} at $M_i$ if  $\im(d_{i+1}) = \ker(d_i)$; it is exact if it is exact at every module that has a map in and a map out.
\end{defn}


\Sept{10}


\begin{ex} Let $A$ be a $b\times a$ matrix and $B$ be a $c\times b$ matrix of real numbers. The sequence of maps
\[ \R^a \xra{A\cdot} \R^b \xra{B\cdot} \R^c \]
is a complex if and only if $BA=0$; equivalently, the columns of $A$ are in the solution space (nullspace) of $B$.
It is exact if and only if the columns of $A$ span the solution space of~$B$.
\end{ex}


    \begin{rem} 
    \begin{itemize}
    \item A sequence of the form $M \xra{g} N \to 0$ is exact if and only if $g$ is surjective.
    \item A sequence of the form  $0 \to M \xra{f} N$ is exact if and only if $f$ is injective.
    \item A sequence of the form  $0 \to M \xra{h} N \to 0$ is exact if and only if $h$ is an isomorphism.
    \item A sequence of the form $0 \to M \to 0$ is exact if and only if $M=0$.
    \end{itemize}
\end{rem}

\begin{defn}
\begin{itemize}
\item      A \DEF{left exact sequence} is an exact sequence of the form
      $$
      0 \to M' \xra{i} M \xra{g} M'' 
      $$
This means $i$ is injective and  $M'\cong \im(i)=\ker(g)$. 


\item      A \DEF{right exact sequence} is an exact sequence of the form
      $$
      M' \xra{f} M \xra{p} M'' \to 0
      $$
This means $p$ is onto and $\im(f)=\ker(p)$,
so, $M'' \cong M/\ker(p) = M/\im(f)$. We denote $M/\im(f)=\coker(f)$ and call it the \DEF{cokernel} of $f$. Thus in a right exact sequence as above, $M''\cong \coker(f)$.

\item      A \DEF{short exact sequence} (\DEF{SES}) is an exact sequence of the form
      $$
      0 \to M' \xra{i} M \xra{p} M'' \to 0
      $$
             Note that in a short exact sequence $M'\cong \ker(p)$ and $M''\cong \coker(i)$. In particular, $M''\cong M/M'$.
We also say that $M$ is an \DEF{extension} of $M'$ and $M''$ if it fits in a short exact sequence as above.
\end{itemize}
    \end{defn}

\begin{ex} Let $A$ be a $b\times a$ matrix and $B$ be a $c\times b$ matrix of real numbers. The sequence of maps
\[ 0 \to \R^a \xra{A\cdot} \R^b \xra{B\cdot} \R^c \]
is a left exact sequence if and only if the columns of $A$ form a basis for the null space of $B$.
\end{ex}

\sssec{Presentations} Recall that a set of elements $B$ in a module $M$ is a \DEF{free basis} if every element of $M$ can be written as a (finite) $R$-linear combination of elements of $B$ in a unique way, and a module $M$ is a \DEF{free module} if it admits a free basis (which almost never is unique, by the way). As mentioned before, a free module is isomorphic to a direct sum of copies of the ring (considered as a module), which we may write as $R^n$ or \Def{$R^{\oplus\Gamma}$} for some index $\Gamma$; such a free module has as a \DEF{standard basis} $\{$\Def{$e_\lambda$}$\}_{\lambda\in\Lambda}$ consisting the elements that have a 1 in the $\lambda$ coordinate and 0 in each of the others. Free modules are also characterized by a universal property:

If $F$ free with basis $B$, then for any module $M$, and any function $f:B\to M$, there is a unique module homomorphism $\phi:F\to M$ such that the diagram commutes:
\[\xymatrix{ & F \ar@{-->}[dr]^-{\phi} & \\ B \ar@{^{(}->}[ur]^-{\subseteq} \ar[rr]^-{f} & & M \;}\]
i.e., any homomorphism is uniquely and freely specified by its values on the basis.

Note that a set of elements $\{m_\lambda\}_{\lambda\in \Lambda} \subseteq M$ generates $M$ if and only if the homomorphism 
\[\xymatrix@R=.7em{ R^{\Lambda} \ar[r] & M \\
e_{\lambda} \ar@{|->}[r] & m_{\lambda} }\]
is surjective. The kernel of such a map consists of the set of $\Lambda$-tuples $(r_\lambda)$ such that $\sum_{\lambda\in\Lambda} r_\lambda m_\lambda =0$; this is called the module of \DEF{relations} on the elements $\{m_\lambda\}$.

\begin{defn} A \DEF{presentation} of a module $M$ consists of a set of elements $\{m_\lambda\}$ that generates $M$, and a set of relations on $\{m_\lambda\}$ that generates the  whole module of relations on $\{m_\lambda\}$.
\end{defn}

We can express the data of a presentation in terms of a right exact sequence. Namely, if $\{m_\lambda\}_{\lambda\in \Lambda}$ is a generating set of $M$, and $\{ (r_\lambda)_\gamma\}_{\gamma\in \Gamma}$ generates the module of relations on our generating set, then
\[ R^{\oplus\Gamma} \to R^{\oplus\Lambda} \to M \to 0\] is a right exact sequence, where the standard basis of $R^{\oplus\Lambda}$ maps to $\{m_\lambda\}$ and the standard basis of $R^{\oplus\Gamma}$ maps to $\{ (r_\lambda)_\gamma\}$. Conversely, a right exact sequence of the form
\[ R^{\oplus\Gamma} \to R^{\oplus\Lambda} \to M \to 0\] 
 is equivalent to the data of a presentation.

\sssec{Split exact sequences}

    
    
    Given modules $M'$ and $M''$, we have the ``trivial'' SES
$$
0 \to M' \xra{\iota} M' \oplus M'' \xra{\pi} M'' \to 0
$$
where $\iota$ is the canonical inclusion and $\pi$ is the canonical projection. The following result gives equivalent conditions for 
when a SES is equivalent to a split one.

\begin{thm}[The splitting theorem] \label{prop93}
Given a SES of left $R$-modules 
\[0 \to M' \xra{i} M \xra{p} M'' \to 0,\]
 the following are equivalent:

\begin{enumerate}
 \item There is a commutative diagram where each  vertical arrow is an isomorphism
$$
\xymatrix{
0 \ar[r] & M' \ar[r]^-i \ar[d]^-{\id} & M \ar[r]^-p \ar[d]^-{\theta}  & M'' \ar[r] \ar[d]^-{\id}  & 0 \\
0 \ar[r] & M' \ar[r]^-{\iota}           & M' \oplus M'' \ar[r]^-\pi            & M'' \ar[r]           & 0.
}
$$

\item There is an isomorphism $\theta: M \xra{\cong} M' \oplus M''$ such that $\theta \circ i = \iota$ and $\pi \circ \theta = p$. 

\item The map $i$ has a left inverse $q$ in $\Mod{R}$.

\item The map $p$ has a right inverse $j$ in $\Mod{R}$.

\item There are maps 
$q: M \to M'$ and $j: M'' \to M$ such that 
$q \circ i = \id_{M'}$, 
$p \circ j = \id_{M''}$, and
$i \circ q + j \circ p = \id_M$.
\end{enumerate}

If these equivalent conditions hold, we call the SES a \DEF{split exact sequence}.
\end{thm}




\begin{proof}

(1) $\Leftrightarrow$ (2) follows by definition of commutative diagram.


(1) $\Rightarrow$ (5): The main idea is that there are obvious splitting maps for the bottom SES. Define $\pi'$ to be the canonical projection $\pi':M'\oplus M''\to M', (m',m'')\mapsto m'$ and $\iota''$ to be the inclusion $\iota'':M''\to M'\oplus M'', m''\mapsto (0,m'')$. Notice that $\pi'\circ \iota =\id_{M'}$ and $\pi\circ \iota''=\id_{M''}$ and $i\circ \pi'+ \iota''\circ p=\id_{M'\oplus M''}$.

\Sept{13}


We can use this to set $q=\pi'\circ\theta$ and $j= \theta^{-1}\circ \iota''$ and check
\begin{eqnarray*}
q\circ i &=\pi'\circ\theta \circ i =\pi'\circ \iota =\id_{M'}\\
p\circ j& =p\circ \theta^{-1}\circ \iota'' =\pi\circ \iota''=\id_{M''}
\end{eqnarray*}
\begin{eqnarray*}
i\circ q+j\circ p &=i\circ \pi'\circ\theta+ \theta^{-1}\circ \iota''\circ p=\theta^{-1}\circ ( \theta\circ i\circ \pi'+\iota''\circ p \circ \theta^{-1})\circ \theta\\
&=\theta^{-1}\circ (\iota\circ \pi'+\iota''\circ\pi)\circ \theta =\theta^{-1}\circ \id_{M'\oplus M''} \circ \theta=\id_M.
\end{eqnarray*}

(5) $\Rightarrow$ (3, 4) is clear. 

(3) $\Rightarrow$ (2): Given such a $q$, define  $\theta(m) = (q(m), p(m))$. It is clear $\theta \circ i = \iota$ and $\pi \circ \theta = p$. 
We will now show that $\theta$ is injective: if $\theta(m)=0$ then $p(m)=0$ so $m\in \im(i)$ therefore $m=i(m')$ for some $m'\in M'$. But now $0=q(m)=q(i(m'))=m'$ so $m'=0$ and thus $m=0$. 

We next show that $\theta$ is surjective: $(m',m'')\in M'\oplus M''$. Since $p$ is onto, then there exists some $u\in M$ so thar $p(u)=m''$. Let $m=i(m')+u-i(q(u))$. Then 
\[
\begin{split}
\theta(m) &=(q(i(m'))+q(u)-q(i(q(u))), p(i(m'))+p(u)-p(i(q(u))))\\
&= (m'+q(u)-q(u),m''+0-0)=(m',m'').
\end{split}
\]
Therefore $\theta$ is bijective, so it is an isomorphism. 

The proof that (4) $\Rightarrow$ (2) is similar, and omitted.
\end{proof}

We can also use splittings to show exactness.

\begin{prop}
Given a complex of $R$-modules of the form
\[0 \to M' \xra{i} M \xra{p} M'' \to 0,\]
if there are maps 
$q: M \to M'$ and $j: M'' \to M$ such that 
$q \circ i = \id_{M'}$, 
$p \circ j = \id_{M''}$, and
$i \circ q + j \circ p = \id_M$, then
the complex is exact, and hence split exact.
\end{prop}
\begin{proof}
Since $i$ has a left inverse, it is injective, and since $p$ has a right inverse, it is surjective. To show exactness in the middle, let $m\in \ker(p)$. Then \[m= (i\circ q)(m) + (j\circ p)(m) = i(q(m))\in \im(i).\qedhere\]
\end{proof}






\begin{rem} The proof in the previous example actually shows that, for any ring $R$, a   SES whose right-most term is free is split exact.
\end{rem}




\begin{ex} Here is an example of a non-split exact sequence: 
Take $R$ to be any (commutative) integral domain and $r \in R$ any non-zero, non-unit element. Then, using that $R$ is a domain, the
sequence
$$
0 \to R \xra{r} R \to R/r \to 0
$$
is exact (where the second map is the canonical surjection).
But it cannot by split exact: If it were, then we would have an isomorphism $R \cong R \oplus R/r$ of modules
and so in particular there would be an ideal $I$ of $R$ isomorphic as a module to $R/r$. But then  $r I = 0$ and since $R$ is a domain, 
this could only happen if $I = 0$, which would mean $r$ is a unit.

For example
$$
0 \to \Z \xra{2} \Z \to \Z/2 \to 0
$$
is an exact, but not split exact, sequence of $\Z$-modules.
\end{ex}

\begin{ex} Suppose $R = k$ is a field. Then every short exact sequence of $R$-modules 
\[0 \to W\to V \to V/W \to 0\]
splits.
\end{ex}




\ssec{Homomorphisms of $R$-modules}

\sssec{Structure of $\Hom_R(M,N)$}

In general, we write \Def{$\Hom_R(M,N)$} for the set $\Hom_{\Mod{R}}(M,N)$ of $R$-module morphisms between two left $R$-modules $M$ and $N$.

It turns out that the set of homomorphisms between two $R$-modules has additional structure.

\begin{prop}
Let $R$ be a ring, and $M,N$ be two left $R$-modules.
\begin{enumerate}
\item $\Hom_R(M,N)$ is an abelian group by pointwise addition, i.e.,
\[ (\alpha+\beta)(m) := \alpha(m) + \beta(m)  \qquad \alpha,\beta\in\Hom_R(M,N),\ m\in M.\]
\item If $R$ is commutative, then $\Hom_R(M,N)$ is an $R$-module via the action
\[ (r\alpha) (m) := r \alpha(m) = \alpha(rm) \qquad \alpha\in \Hom_R(M,N),\ r\in R, \ m\in M.\]
\item More generally, 
\begin{itemize}
\item if $M$ is a $(R,S)$-bimodule, then $\Hom_R(M,N)$ is a left $S$-module by the action $(s \alpha)(m) = \alpha(ms)$;
\item if $N$ is a $(R,T)$-bimodule, then $\Hom_R(M,N)$ is a right $T$-module by the action $(\alpha t)(m)=\alpha(m)t$;
\item if $M$ is a $(R,S)$-bimodule and $N$ is a $(R,T)$-bimodule, then $\Hom_R(M,N)$ is a $(S,T)$-bimodule by the previous two actions.
\end{itemize}
\end{enumerate}
\end{prop}

\begin{proof}
(1) is easy to check, and similar to what we checked with module endomorphisms.

Let's consider (2): The first thing to note is that $r\alpha(m)=\alpha(rm)$ by linearity of $\alpha$. Let us check that the map $r\alpha$ defined this way is an $R$-module morphism:
\[ (r\alpha)(m+m') = r \alpha(m+m') = r(\alpha(m) +\alpha(m'))=r\alpha(m) +r\alpha(m') = r\alpha(m) + r\alpha(m')\]
\[ (r\alpha)(sm)= r \alpha(sm) = rs \alpha(m) = sr \alpha(m) = s (r\alpha(m));\]
note that commutativity of $R$ is essential here.

The distributive rules are straightforward, and $((rs) \alpha)(m) = rs\alpha(m) = (r(s\alpha))(m)$, so $(rs)\alpha=r(s\alpha)$.

For (3), let's just focus on the first case. To see $s\alpha$ is $R$-linear, addition is similar to above, and
\[ (s\alpha)(rm) = \alpha(rms) = r\alpha(ms) = r (s\alpha)(m).\]
Let's check the associativity property for the action: given $s,s'\in S$, \[(s s')\alpha(m) = \alpha(m s s') = s' \alpha(m s) = (s(s' \alpha))(m).\]
The other axioms are straightforward.
\end{proof}


 

The bonus module structures in case (3) are often useful, even for commutative rings. However, for many statements below we will just focus on cases (1) and (2) above for clarity.

\begin{ex} Let $K$ be a field.
Since $K$ is commutative, $\Hom_K(K,K[x])$ and $\Hom_K(K[x],K)$ are $K$-vector spaces.
The polynomial ring $K[x]$ is a $(K,K[x])$-bimodule. This gives $\Hom_K(K,K[x])$ a $K[x]$-module structure by postmultiplication: e.g., if $\alpha$ is the $K$-linear map such that $\alpha(1)=f(x)$, and $g(x)\in K[x]$, then $g(x) \alpha$ is the map that sends $1$ to $f(x)g(x)$.
Likewise, $\Hom_K(K[x],K)$ a $K[x]$-module structure by premultiplication: e.g., if $\alpha$ is the $K$-linear map such that $\alpha(x^i)=\gamma_i\in K$ , then $x \alpha$ is the map that sends $x^i$ to $\gamma_{i+1}$.
\end{ex}



\Sept{15}

\sssec{Hom as functors}

\begin{defn}[Covariant Hom] Let $R$ be a ring and $M$ be an $R$-module. There is a covariant functor \index{$\Hom_R(M,-)$}\index{covariant Hom functor}
\[ \Hom_R(M,-) : \Mod{R} \to \Ab\]
that maps each module $A$ to  $\Hom_R(M,A)$, and each morphism $A\xra{f} B$ to the homomorphism {\Def{$\Hom_{R}(M,f)$}  $=:$ \Def{$f_*$}} of ``postcomposition by $f$'':
		$$\xymatrix@R=1mm{\Hom_{R}(M,A) \ar[r]^{f_*} & \Hom_{R}(M,B) \\g \ar@{|->}[r] & f\circ g
		\\ M \xrightarrow{g}A & M \xrightarrow{g} A \xrightarrow{f} B.  }$$
		
If $R$ is commutative, then we consider $\Hom_R(M,-)$ as a functor from $ \Mod{R} \to \Mod{R}$ by the same rule.
\end{defn}
There are some things to check to verify that this is a functor.
\begin{proof}
We need to check that $f_*$ is a valid morphism in $\Ab$, or in $\Mod{R}$ in the commutative case.
Given $g,h\in \Hom_{R}(A,B)$, we have \[f_*(g+h)(m)=f((g+h)(m))=f(g(m)+h(m))=f(g(m))+f(h(m)) = f_*(g)(m) + f_*(h)(m).\]
If $R$ is commutative,
\[ f_*(rg)(m) = f(g(rm)) = r f(g(m)) = (r f_*)(g)(m).\]

We also need to see that these satisfy the functor axioms. We have $(1_A)_*(g) = 1_A \circ g = g$, so $(1_A)_*$ is the identity map on $\Hom_R(M,A)$. Given $A\xra{g}B\xra{f} C$, and $h\in \Hom_R(M,A)$, \[(fg)_* (h) = f\circ g \circ h = f \circ (g_*(h))= f^*(g^*(h)) = (f_* \circ g_*)(h).\qedhere\]
\end{proof}

\begin{rem}If $M$ is an $(R,S)$-bimodule, then consider $\Hom_R(M,-)$ as a functor from $ \Mod{R} \to \Mod{S}$ by the same rule.\end{rem}

\begin{defn}[Contravariant Hom] Let $R$ be a ring and $M$ be an $R$-module. There is a contravariant functor \index{$\Hom_R(-,M)$}\index{contravariant Hom functor}
\[ \Hom_R(-,M) : \Mod{R} \to \Ab\]
that maps each module $A$ to  $\Hom_R(A,M)$, and each morphism $A\xra{f} B$ to the homomorphism {\Def{$\Hom_{R}(f,M)$}  $=:$ \Def{$f^*$}} of ``precomposition by $f$'':
		$$\xymatrix@R=1mm{\Hom_{R}(M,B) \ar[r]^{f_*} & \Hom_{R}(M,A) \\g \ar@{|->}[r] &  g\circ f
		\\ B \xrightarrow{g}M & A \xrightarrow{f} B \xrightarrow{g} M.  }$$
		
	If $R$ is commutative, then we consider $\Hom_R(-,M)$ as a functor from $ \Mod{R} \to \Mod{R}$ by the same rule.
\end{defn}
There are some things to check to verify that this is a functor.
\begin{proof}
We need to check that $f^*$ is a valid morphism in $\Ab$, or in $\Mod{R}$ in the commutative case.
Given $g,h\in \Hom_{R}(A,B)$, we have \[f^*(g+h)(m)=(g+h)(f(m)) = g(f(m)) + h(f(m)) = f^*(g)(m) + f^*(h)(m).\]
If $R$ is commutative,
\[ f^*(rg)(m) = rg(f(m)) = (r f^*(g))(m).\]

We also need to see that these satisfy the functor axioms. We have $(1_A)^*(g) = g \circ 1_A = g$, so $(1_A)^*$ is the identity map on $\Hom_R(A,M)$. Given $A\xra{g}B\xra{f} C$, and $h\in \Hom_R(C,M)$, \[(fg)^* (h) =h\circ  f\circ g = f^*(h) \circ g = g^*(f^*(h)) = (g^* \circ f^*)(h).\qedhere\]
\end{proof}
\begin{rem}If $M$ is an $(R,S)$-bimodule, then consider $\Hom_R(M,-)$ as a functor from $ \Mod{R} \to \Mod{S^{\mathrm{op}}}$ by the same rule.
\end{rem}

\sssec{Examples of Hom}

\begin{ex} Let $R$ be a ring. Then, by the universal property of free modules, since $\{1\}$ is a free basis for $R$ as an $R$-module, the map
\[\xymatrix@R=.5em { \Hom_R(R,M) \ar[r]^-{\psi_M} &M \\
\phi \ar@{|->}[r] &\phi(1) }\]
is a bijection. Moreover, this is an isomorphism of abelian groups in general, and of $R$-modules in the commutative case:
\[ \psi_M(\alpha+\beta) = (\alpha+\beta)(1) = \alpha(1) + \beta(1) = \psi_M(\alpha) + \psi_M(\beta)\]
\[\psi_M(r\alpha) = (r\alpha)(1) = r\alpha(1) = r\psi_M(\alpha).\]
Even better, in the commutative case, the collection of isomorphisms $\psi_M$ form a natural isomorphism $\psi: \Hom_R(R,-) \Rightarrow 1_{\Mod{R}}$. For this, we need to check that, given $\beta:M\to N$, the following diagram commutes:
\[\xymatrix{ \Hom_R(R,M) \ar[r]^-{\beta_*} \ar[d]^-{\psi_M} & \Hom_R(R,N) \ar[d]^-{\psi_N}\\ M \ar[r]^-{\beta}  & N.}\]
Along either path, we get $\alpha \mapsto \beta(\alpha(1))$, so this is indeed the case.
\end{ex}



\Sept{17}

\begin{ex}
Similarly, if $F=R^{\oplus \Lambda}$ is a free module, then $\Hom_R(R^{\oplus \Lambda},M) \cong M^{\oplus \Lambda}$, where $M^{\times \Lambda}= \prod_{\lambda\in \Lambda} M$ by the map that sends a morphism to its tuple of values on the standard basis: as abelian groups, and as $R$-modules in the commutative case. 

We can interpret the right-hand side as the values of a functor: set $F(M) = M^{\times \Lambda}$, and for $f:M\to N$, set $F(f)$ to be the map given by $f$ on each coordinate. Interpreted like so, the isomorphisms again form a natural isomorphism.
\end{ex}

\begin{prop}
Let $\{M_\lambda\}_{\lambda\in \Lambda}$ be a family of $R$-modules, and $N$ be an  $R$-module.
There are isomorphisms of abelian groups
\[\begin{aligned} \Hom_R(\bigoplus_{\lambda\in \Lambda} M_\lambda, N) &\cong \prod_{\lambda\in \Lambda} \Hom_R(M_\lambda,N)\\
\Hom_R(N,\prod_{\lambda\in \Lambda} M_\lambda) &\cong \prod_{\lambda\in \Lambda} \Hom_R(N,M_\lambda)\end{aligned}\]
Moreover, these are isomorphisms of $R$-modules if $R$ is commutative.
\end{prop}
\begin{proof}
Since $\bigoplus_{\lambda\in \Lambda} M_\lambda$ is the coproduct of $\{M_\lambda\}_{\lambda\in \Lambda}$ in $\Mod{R}$, we have a bijection for every $R$-module $N$
\[\xymatrix@R=.6em { \Hom_R(\bigoplus_{\lambda\in \Lambda} M_\lambda, N) \ar[r] & \prod_{\lambda\in \Lambda} \Hom_R(M_\lambda,N) \\
\phi \ar@{|->}[r] &(\phi \circ \iota_\lambda).}\]
We only have to observe that these maps preserve the abelian group and/or $R$-module structures.
Similarly, since $\prod_{\lambda\in \Lambda} M_\lambda$ is the product of $\{M_\lambda\}_{\lambda\in \Lambda}$ in $\Mod{R}$, we have a bijection for every $R$-module $N$
\[\xymatrix@R=.6em { \Hom_R(N,\prod_{\lambda\in \Lambda} M_\lambda) \ar[r] & \prod_{\lambda\in \Lambda} \Hom_R(N,M_\lambda) \\
\phi \ar@{|->}[r] &(\pi_\lambda \circ \phi),}\]
and one verifies the additivity / linearity of this map.
\end{proof}


\begin{ex} As an important special case of the previous example, if $R$ is commutative, and $R^{\oplus\Gamma}$ and $R^{\oplus\Lambda}$ are free modules, then every $R$-linear homomorphism $\alpha:R^{\oplus\Gamma} \to R^{\oplus\Lambda}$ is given by left multiplication by the (possibly infinite) $\Lambda \times \Gamma$ matrix where the $\gamma$ column is the $\Lambda$-tuple $(\alpha(e_\gamma))_\lambda$.
\end{ex}


\begin{exer} Show that when $R$ is not necessarily commutative, if we give $\Hom_R(R^{\oplus\Lambda},M)$ the $R$-module structure via the $(R,R)$-bimodule structure on $R^{\oplus\Lambda}$, the isomorphisms $\Hom_R(R^{\oplus\Lambda},M)\cong M^{\times \Lambda}$ are natural isomorphisms of $R$-modules.
\end{exer}

\begin{ex} Let $R$ be a commutative ring, and consider the module $R/I$ for some ideal $I$. For every module $M$, there is an isomorphism $\Hom_R(R/I,M) \cong \ann_M(I)$, where $\ann_M(I)$ is the set of elements $m\in M$ such that $Im=0$.

Indeed, every $R$-module homomorphism from $R/I$ is determined by the image of $1$, so the map $\Hom_R(R/I,M) \to M$ of evaluation at 1 is injective. The image consists of the set of elements $m\in M$ for which the map $r\mapsto rm$ is well-defined; this is the collection of elements that satisfy $Im=0$.

Again, we can think of the right hand side as a functor $F:\Mod{R}\to\Mod{R}$ where  on objects $F(M) = \ann_M(I)$, and on morphisms $M\xra{\alpha} N$ maps to the restriction of $\alpha$ to $\ann_M(I)$. This is a natural isomorphism again.  
\end{ex}

\begin{ex} For a field $K$, the functor $\Hom_K(-,K)$ is exactly the ``vector space dual'' functor $(-)^*$.
\end{ex}

%
%\begin{ex}
%Let $R$ be a ring, and $M$ a left $R$-module. Then $\Hom_R(R,M)$ is a left $R$-module as well: 
%\begin{itemize}
%\item when $R$ is commutative the action is by $(r \phi)(s) = \phi(rs) = r\phi(s)$
%\item in general, the $R$-module structure comes from the right $R$-module structure on $R$, so we have to premultiply the input on the right: $(r\phi)(s) = \phi(sr)$, but in the commutative case, it's the same thing as the above.
%\end{itemize}
% There is an isomorphism
%\[\xymatrix@R=.6em { \Hom_R(R,M) \ar[r]^-{\psi} &M \\
%\phi \ar@{|->}[r] &\phi(1)\\
%(r\mapsto rm) & \ar@{|->}[l] m }\]
%This is injective, since $\phi$ is determined by the image of $1$, and surjective since the map $r \mapsto rm$ is $R$-linear for any $m\in M$. To see that $\psi$ is $R$-linear, we have $\psi(r\phi)=(r\phi)(1) = \phi(1\,r) = \phi(r) = r \phi(1) = r \psi(\phi)$.
%\end{ex}


\ssec{Exact functors and left exactness of Hom}

\begin{defn} Let $R,S$ be rings. A covariant functor $F:\Mod{R} \to \Mod{S}$ is \DEF{additive} if the function
\[\xymatrix@R=.5em{ \Hom_R(M,N) \ar[r] & \Hom_S(F(M),F(N)) \\ f \ar@{|->}[r] & F(f)}\]
is a homomorphism of abelian groups. Likewise, a contravariant functor $G:\Mod{R} \to \Mod{S}$ is {additive} if the function
\[\xymatrix@R=.5em{ \Hom_R(M,N) \ar[r] & \Hom_S(F(N),F(M)) \\ f \ar@{|->}[r] & F(f)}\]
is a homomorphism of abelian groups.
\end{defn}


Additive functors preserve a number of basic properties, e.g., zero morphisms go to zero morphisms, and the zero module maps to the zero module (since it's characterized by the fact that its identity map is its zero map).

\begin{exer} The covariant and contravariant Hom functors are additive functors.
\end{exer}

\begin{defn} Let $F:\Mod{R}\to\Mod{S}$ be an additive covariant functor.
\begin{itemize}
\item $F$ is \DEF{right exact}  if
whenever
$$
M' \xra{i} M \xra{p} M'' \to 0
$$
is exact, then so is
$$
F(M') \xra{F(i)}  F(M) \xra{F(p)} F(M'') \to 0.
$$
 (Recall $F(0) = 0$ since $F$ is additive.)

\item $F$ is \DEF{left exact} if 
whenever
$$
0 \to M' \xra{i} M \xra{p} M'' 
$$
is exact, then so is
$$
0 \to F(M') \xra{F(i)}  F(M) \xra{F(p)} F(M'').
$$

\item $F$ is \DEF{exact} if it is both left and right exact.
\end{itemize}
\end{defn}

\begin{rem}
An exact functor takes any SES to a SES.
\end{rem}

\begin{defn} Let $G:\Mod{R}\to\Mod{S}$ be an additive contravariant functor.
\begin{itemize}
\item $G$ is \DEF{right exact}  if
whenever
$$
0 \to M' \xra{i} M \xra{p} M''
$$
is exact, then so is
$$
G(M'') \xra{G(p)}  G(M) \xra{G(i)} G(M') \to 0.
$$

\item $G$ is \DEF{left exact} if 
whenever
$$
M' \xra{i} M \xra{p} M'' \to 0
$$
is exact, then so is
$$
0 \to G(M'') \xra{G(p)}  G(M) \xra{G(i)} G(M').
$$

\item $G$ is \DEF{exact} if it is additive and both left and right exact.
\end{itemize}
\end{defn}

\begin{exer} The definitions above all stay unchanged if for each condition we start with a short exact sequence. For example, a covariant additive functor $F$ is left exact if for every short exact sequence
	$$\xymatrix{0 \ar[r] & A \ar[r]^-f & B \ar[r]^-g & C \ar[r] & 0}$$
	of $R$-modules,
	$$\xymatrix{0 \ar[r] & F(A) \ar[r]^-{F(f)} & F(B) \ar[r]^-{F(g)} & F(C)}$$
	is exact.
\end{exer}

\begin{rem} If $F,G:\Mod{R} \to \Mod{S}$ are naturally isomorphic additive functors, then $F$ is exact if and only if $G$ is exact. Indeed, given a short exact sequence
\[ 0 \to M' \xra{i} M \xra{p} M'' \to 0\]
we obtain a commutative diagram
\[\xymatrix{
0 \ar[r] & F(M') \ar[r]^{F(i)} \ar[d]^-{\theta'} & F(M) \ar[r]^{F(p)} \ar[d]^-{\theta} & F(M'') \ar[r]\ar[d]^-{\theta''} & 0 \\
0 \ar[r] & G(M') \ar[r]^{G(i)}  & G(M) \ar[r]^{G(p)}  & G(M'') \ar[r] & 0 }\]
where $\theta',\theta,\theta''$ isomorphisms.
Then if the top row is exact, $G(i)=\theta F(i) (\theta')^{-1}$ is injective, $G(p)=\theta''  F(p) \theta^{-1}$ is surjective, and \[x\in \ker G(p) = \ker(\theta''  F(p) \theta^{-1}) \iff \theta^{-1} (x) \in \ker F(p) \iff \theta^{-1} (x) \in \im F(i) \iff x\in \im(\theta F(i) (\theta')^{-1}) = \im G(i).\]

Similarly for ``left exact'' or ``right exact''.
\end{rem}


\begin{thm} Let $M$ be an $R$-module.
\begin{enumerate}
\item The functor $\Hom_R(M,-)$ is left exact.
\item The functor $\Hom_R(-,M)$ is left exact.
\end{enumerate}
\end{thm}
%\begin{comment}

\Sept{20}
\begin{proof} 
\begin{enumerate}
\item Let \[ 0 \to A \xra{i} B \xra{p} C\]
be  exact. We need to show that
\[ 0 \to \Hom_R(M,A) \xra{i_*} \Hom_R(M,B) \xra{p_*} \Hom_R(M,C)\]
is exact.
\begin{itemize}
\item $i_*$ is injective: Let $f\in \Hom_R(M,A)$ be nonzero, so $f(m)\neq 0$ for some $m\in M$. Then $i_*(f)(m) = i(f(m))\neq 0$ since $i$ is injective, so $i_*(f)\in \Hom_R(M,B)$ is nonzero.
\item $\im(i_*)\subseteq \ker(p_*)$: Let $g\in \Hom_R(M,B)$ be in the image of $i_*$, so we can write $g=i_*(f)$ for some $f\in \Hom_R(M,A)$. We have $p_*(i_*(f))=p\circ i \circ f=0$.
\item $\ker(p_*)\subseteq\im(i_*)$: Let $g\in \Hom_R(M,B)$ be in the kernel of $p_*$, so $p\circ g=0$. Then, for every $m\in M$, $g(m)\in \ker(p) = \im(i)$. As $i$ is injective, $i$ induces an isomorphism from $i$ to the image of $A$ in $B$, so there is an $R$-module homomorphism $q:\im(A) \to A$ such that $i \circ q=1_{\im(A)}$. Thus, we obtain an $R$-module map $f:= q\circ g :M\to A$ such that $i_*(f) = i \circ q \circ g = g$.
\end{itemize}
\item Let \[ A \xra{i} B \xra{p} C \to 0\]
be  exact. We need to show that
\[ 0 \to \Hom_R(C,M)\xra{p^*}  \Hom_R(B,M) \xra{i^*} \Hom_R(A,M)  \]
is exact.
\begin{itemize}
\item $p^*$ is injective: Let $f\in \Hom_R(C,M)$ be nonzero, so $f(c)\neq 0$ for some $m\in M$. Then, since $p$ is surjective, there is some $b\in B$ such that $p(b)=c$, and hence $p^*(f)(b)=f(p(b))=f(c)\neq 0$, so $p^*(f)\neq 0$.

\item $\im(p^*)\subseteq \ker(i^*)$: Let $g\in \Hom_R(B,M)$ be in the image of $p_*$, so we can write $g=p^*(f)$ for some $f\in \Hom_R(M,C)$. We have $i^*(p^*(f))=f \circ p \circ i=0$.

\item $\ker(i^*)\subseteq\im(p^*)$: Let $g\in \Hom_R(B,M)$ be in the kernel of $i^*$, so $g\circ i=0$. Thus, as $g|_{\im(i)} = 0$, we can factor $g=\overline{g} \circ \pi$, where $\pi:B\to B/\im(i)=B/\ker(p)$ is the quotient map, and $\overline{g}:B/\im(i) \to M$. Note that, since $p$ is surjective, writing $p=\overline{p} \circ \pi$, the map $\overline{p}:B/\im(i) = B/\ker(p) \to C$ is an isomorphism, so there is a map $j:C\to B/\ker(p)$ such that $ j \circ \overline{p}$ is the identity on $B/\im(i)$, so $j\circ p = j\circ \overline{p} = \circ \pi = \pi$. Set $f=\overline{g} \circ j$. We then have $p^*(f)= \overline{g} \circ j \circ p = \overline{g} \circ \pi=g$. Thus $g\in \im(p^*)$.\qedhere\end{itemize}
\end{enumerate}
\end{proof}

\begin{ex} Neither Hom functor is exact. For example, consider the short exact sequence
\[ 0 \to \Z \xra{2} \Z \to \Z/2\Z \to 0.\]
If we apply $\Hom_\Z(\Z/2\Z ,-)$ to this sequence, we get
\[ 0 \to 0 \to 0 \to \Z/2\Z \to 0.\]
This is exact up until $\Z/2\Z$ (which agrees with the left exactness), but not at $\Z/2\Z$.
Likewise, apply $\Hom_\Z(-,\Z/2\Z)$ to get
\[ 0 \to \Z/2\Z \xra{1} \Z/2\Z \xra{0} \Z/2\Z \to 0.\]
This is again exact up to the last $\Z/2\Z$, but not there.
\end{ex}

We can use left exactness to compute various Hom modules. 

\begin{ex} Let $R$ be commutative, and $M$ be a finitely presented $R$-module with presentation
\[  R^m \xra{A\cdot} R^n \to M \to 0.\]
Then $\Hom_R(M,R)$ sits in a left exact sequence
\[ 0\to \Hom(M,R) \to \Hom_R(R^n,R) \xra{\Hom_R(A\cdot, R)} \Hom_R(R^m,R). \]
We have $\Hom_R(R^n,R)$ is free with free basis given by the coordinate functions $\{e_1^*,\dots,e_n^*\}$; likewise for $\Hom_R(R^m,R)$ with basis $\{ \bar{e}_1^*,\dots,\bar{e}_m^*\}$ (we will write bars for basis elements in $R^m$). In these bases, to compute the $j$th column of the matrix,
we have that $e_j^*$ maps to $e_j^* A$, and to compute $e_j^*A$ in terms of the given basis (to find the entry in the $i$th row), we observe $(e_j^*A)(\bar{e}_i)$ is $A_{ji}$, so the map $\Hom_R(A\cdot, R)$ is given by $A^T \cdot$.  We get a left exact sequence
\[ 0\to \Hom(M,R) \to R^n \xra{A^T \cdot} R^m,\]
so $\Hom(M,R)\cong \ker(A^T)$.
\end{ex}

\

%\begin{comment}


\ssec{Tensor products}

\

\Sept{22}

\sssec{Definition of tensor product}

\begin{defn} 
For a ring $R$, a right $R$-module $M$, a left $R$-module $N$, and an abelian group $A$, a function
$$
b: M \times N \to A
$$
is called \DEF{$R$-balanced biadditive} if the following conditions hold:
\begin{enumerate}
\item $b(m+m', n) = b(m, n) + b(m', n)$ for all $m, m' \in M$, $n \in N$,
\item $b(m,n +n') = b(m, n) + b(m, n')$ for all $m \in M$, $n, n' \in N$, and
\item $b(mr, n) = b(m, rn)$ for all $m \in M$, $n, \in N$, and $r \in R$.
\end{enumerate}

Assume $R$ is commutative and $A$ is an $R$-module
(not just an abelian group). Such a pairing $b$ is called
  \DEF{$R$-bilinear} if we also have
\begin{enumerate}
  \item[(4)]
    $b(mr, n) = b(m, rn) = rb(m,n) \text{ for all $m \in M$, $n, \in N$, and $r \in R$.}$
\end{enumerate}
\end{defn}

Conditions (1) and (2) alone are the biadditive part, and condition (3) is the balancedness. Condition (4) says that the biadditive map $b$ is an $R$-linear function in either argument if we fix the other one.

\begin{ex} 
\begin{enumerate}
\item If $R$ is any ring, the map $f:R\times R \to R, f(r,s)=rs$ is $R$-balanced biadditive, and bilinear if $R$ is commutative.
%\item for an $R$-module $M$, $f:R^2\times M \to M^2, f((r,s),m)=(rm,sm)$
\item For $R$ commutative, an ideal $I$, and a left module $M$, the map $f:(R/I)\times M\to M/IM, f(\ov{r},m)=\ov{rm}$ is $R$-bilinear.
\item For $K$ a field, $f:K^n\times K^n \to K$ given by the usual dot product is $K$-bilinear. Recall that we can view $K^n$ as a right $M_n(K)$ module via $v \cdot A = A^T v$ and as a left $M_n(K)$ module via $A\cdot v = Av$. With these structures, $f$ is $M_n(K)$-balanced biadditive. The balanced part is the least obvious one:
\[ f(v \cdot A,w) = (A^T v) \cdot w = v^T A w = v \cdot (Aw) = f(v, A\cdot w).\]
\end{enumerate}
\end{ex}

We now define tensor products using a universal property.

\begin{defn}
\label{def:tensorprod}
  Let $R$ be a (not necessarily commutative) ring, let $M$ be a right $R$-module, let $N$ be a left $R$-module. 

An abelian group $M \otimes_R N$ together with an $R$-balanced biadditive map $h : M\times N \to M\otimes_R N$ is called a \DEF{tensor product} of $M$ and $N$ if it has the following universal property: for any abelian group $A$ and $R$-balanced biadditive map $b : M\times N \to A$, there exists a unique abelian group homomorphism $\alpha : M \otimes_R N\to A$ such that $b = \alpha \circ h$.

\[
\xymatrix{
& M\otimes_R N  \ar@{-->}[dr]^{\exists ! \alpha}&\\
 M\times N \ar[ur]^h \ar[rr]^b& & A
}
\]
\end{defn}

\begin{lem} 
If $(X,h)$, $(Y,k)$ are two tensor products for $M$ and $N$, then there is a unique isomorphism of abelian groups $\alpha:X\to Y$ such that $k= \alpha\circ h$.
\end{lem}
\begin{proof}
The following diagram is a rough guide for the argument:
\[ \xymatrix{ & X \ar@{-->}[r]^{\alpha} & Y\ar@{-->}[r]^{\beta } & X . \\ M\times N \ar[ur]^-{h} \ar[urr]^-{k} \ar[urrr]_-{h} }\]
Applying the universal property of $(X,h)$ with the $R$-balanced biadditive map $k$, we get a unique abelian group homomorphism $\alpha$ above that makes its triangle commute; in particular, the uniqueness statement is clear. Likewise, applying the universal property of $(Y,k)$ with $h$, we get an abelian group homomorphism $\beta$ that makes its triangle commute. Then, $\beta\circ\alpha$ is an abelian group homomorphism such that $(\beta\circ\alpha) \circ h = h$, and the identity map is another. By the uniqueness property of $(X,h)$, $\beta\circ \alpha$ is the identity. A similar argument shows that $\alpha \circ \beta$ is the identity too, so $\alpha$ is an isomorphism.
\end{proof}

\begin{thm}
\label{thm:tensor}
Let $R$ be a (not necessarily commutative) ring, let $M$ be a right $R$-module, let $N$ be a left $R$-module.  Then a tensor product $M\otimes_R N$ exists and is given by defining an abelian group $M \otimes_R N$ by generators and relations as follows:
\index{$m\otimes n$}
\begin{itemize}
\item The generators are all expressions of the form $m \otimes n$ for $m \in M$ and $n \in N$. 
\item The relations are
\begin{enumerate}
\item  $(m + m') \otimes n = m \otimes n + m' \otimes n$ for all $m, m' \in M$ and $n \in N$,
\item $m \otimes (n + n') = m \otimes n + m \otimes n'$ for all $m \in M$ and $n, n' \in N$, and
\item $(mr) \otimes n = m \otimes (rn)$ for all $m \in M$, $n\in N$, and $r \in R$.
\end{enumerate}
\end{itemize}
Equivalently, $M \otimes_R N$ is the quotient 
$$
\frac{\bigoplus_{(m,n) \in M \times N} \Z \cdot (m \otimes n)}{(Y)}
$$
where 
$$
Y =  \{(m + m') \otimes  n) -m \otimes n - m' \otimes n \} \cup
\{m \otimes  (n +n') - m \otimes n - m \otimes n'\} \cup
\{(mr) \otimes  n  - m \otimes (rn)\} .
$$
Further we define $h: M \times N \to M \otimes_R N$ to be the function $h(m,n)=m \otimes n$. 

Then the pair $(M \otimes_R N,h)$ defined above is the tensor product of $M$ and $N$.
\end{thm}




\begin{proof}
It is immediate from the construction that $h$ is $R$-balanced biadditive. 
Given a biadditive map  $b: M \times N \to A$, define $\tilde{b}: \bigoplus_{(m,n) \in M \times N} \Z  \cdot (m \otimes n) \to A$ to be the unique homomorphism of abelian groups  
sending the basis element $m \otimes n$  to $b(m,n)$. Since $b$ is biadditive, we have
$$
\tilde{b}((m + m') \otimes  n -m \otimes n - m' \otimes n) =
b(m + m',n) - b(m,n) - b(m',n) = 0,
$$
$$
\tilde{b}(m \otimes  (n +n') - m \otimes n - m \otimes n)
= b(m, n+n') - b(m,n) - b(m,n') = 0,
$$
and
$$
\tilde{b}((mr) \otimes  n  - m \otimes (rn)) = b(mr,n) - b(m,rn) = 0.
$$
Thus $\tilde{b}(<Y>) = 0$  and so it induces a homomorphism of abelian groups
$$
\alpha: M \otimes_R N \to A.
$$
It is evident from the construction that $\alpha \circ h = b$. Since the image of $B$ generates $M \otimes_A N$ as an abelian group, $\alpha$ is the unique
homomorphism satisfying this equation.

If $\beta$ is any abelian group homomorphism with $\beta\circ h=b$, we have $\beta(m\otimes n) = \beta(h(m,n))=b(m,n)=\alpha(m\otimes n)$. Since the elements of the form $m\otimes n$ generate $M\otimes_R N$ as an abelian group, we must have $\beta=\alpha$.
\end{proof}

Note that the map induced by a biadditive map $b$ sends $m\otimes n\mapsto b(m,n)$.

\begin{rem} In this explicit construction, every element is a \emph{sum} of \DEF{simple tensors} (elements of the form $m\otimes n$) but in general, not every element is itself a simple tensor.
\end{rem}

\begin{rem} While the construction of tensor products may feel easier to work with at first, it is important to keep in mind that it is hard to tell when two combinations of simple tensors are equal. In general, when we want to define a map from a tensor product, it is better to use the universal property, since we don't have to worry about well-definedness. However, to define a map into a tensor product, using the concrete description is often easier.
\end{rem}



\begin{exer}
In $M\otimes_RN$ we have $0_M\otimes n=0_{M\otimes_R N} =m\otimes 0_N$ for each $m\in M,n\in N$.
\end{exer}


\Sept{24}
%\begin{comment}

\begin{ex} I claim $\Z/m\Z \otimes_\Z \Z/n\Z \cong \Z/g\Z$ where $g = \mathrm{gcd}(m,n)$.
\end{ex}


\begin{proof}
Define a function
$$
b: \Z/m\Z \times \Z/n\Z \to \Z/g\Z
$$ 
by $b(\overline{i}, \overline{j}) = \overline{ij}$. It is not hard to see that $b$ is well-defined (exercise!) 
and $\Z$-balanced biadditive.
By the universal property, it therefore induces a homomorphism of abelian groups
$$
\a: \Z/m\Z \otimes_\Z \Z/n\Z \to \Z/g\Z
$$ 
such that $\a(\overline{i} \otimes \overline{j}) = \overline{ij}$. 

Now define a homomorphism $\phi: \Z \to \Z/m\Z \otimes_\Z \Z/n\Z$ by sending $1$ to $1 \otimes 1$. Notice that 
$$
\phi(g) = g \cdot (1 \otimes 1) = g \otimes 1 = 1 \otimes g.
$$
Recall that $g = i m + j n$ for some $i,j \in \Z$. So
$$
g \otimes 1 = i m \otimes 1 + 1 \otimes j n = 0 \otimes 1 + 1 \otimes 0 = 0 + 0 = 0.
$$
So, $\phi$ induces a homomorphism
$$
\beta = \overline{\phi}: \Z/g\Z \to \Z/m\Z \otimes_\Z \Z/n\Z
$$
with $\beta(\overline{i}) = \overline{i} \otimes 1 = 1 \otimes \overline{i}$.

We have $\a(\beta(\overline{i})) = \a(\overline{i} \otimes 1) = \overline{i}$ so that $\a \circ \beta = \id$. 

A typical element of  $\Z/m\Z \otimes \Z/n\Z$ has the form $\sum_t \overline{i_t} \otimes \overline{j_t}$. We have 
$$
\beta(\alpha(\sum_t \overline{i_t} \otimes \overline{j_t}))
= \sum_t \overline{i_t} \cdot \overline{j_t} \otimes 1
= \sum_t \overline{i_t}   \otimes  \overline{j_t}
$$
and so $\beta \circ \alpha = \id$. 
\end{proof}


\sssec{Module structure of tensor product}

\begin{prop} \begin{enumerate}
\item If $R$ is commutative, and $M$ and $N$ are $R$-modules, then
\begin{enumerate}
\item $M\otimes_R N$ is an $R$-module via the action
\[ r \cdot (\sum_i m_i \otimes n_i) = \sum_i (r m_i) \otimes n_i = \sum_i m_i \otimes (r n_i).\]
\item The natural map $h: M\times N \to M\otimes_R N$ is $R$-bilinear.
\item For any $R$-module $A$ and $R$-bilinear map $b: M\times N \to A$, there is a unique $R$-module homomorphism $\alpha: M\otimes_R N \to A$ such that $b=\alpha\circ h$.
\end{enumerate}
\item If $M$ is an $(S,R)$-bimodule, and $N$ is an $R$-module, then consider $M\times N$ as an $S$-module by the action $s (m,n) = (sm,n)$. We have
\begin{enumerate}
\item $M\otimes_R N$ is an $S$-module via the action
\[ s \cdot (\sum_i m_i \otimes n_i) = \sum_i (s m_i) \otimes n_i.\]
\item The natural map $h: M\times N \to M\otimes_R N$ is $S$-linear.
\item For any $S$-module $A$ and $S$-linear $R$-balanced biadditive map $b: M\times N \to A$, there is a unique $S$-module homomorphism $\alpha: M\otimes_R N \to A$ such that $b=\alpha\circ h$.
\end{enumerate}
\item If $M$ is an $R$-module, and $N$ is an $(R,S)$-bimodule, then consider $M\times N$ as a right $S$-module by the action $ (m,n)s = (m,ns)$. We have
\begin{enumerate}
\item $M\otimes_R N$ is a right $S$-module via the action
\[ s \cdot (\sum_i m_i \otimes n_i) = \sum_i m_i \otimes (n_i s).\]
\item The natural map $h: M\times N \to M\otimes_R N$ is right $S$-linear.
\item For any right $S$-module $A$ and right $S$-linear $R$-balanced biadditive map $b: M\times N \to A$, there is a unique right $S$-module homomorphism $\alpha: M\otimes_R N \to A$ such that $b=\alpha\circ h$.
\end{enumerate}
\end{enumerate}
\end{prop}
\begin{proof} 
Let's consider case (2).

For (a), the first thing we need to show that the action of an element $s\in S$ on $M\otimes_R N$ is a well-defined function. To do this, consider the map $\mu_s: M\times N \to A$ given by the rule $\mu_s(m,n) = sm\otimes n$. We claim that this is $R$-balanced biadditive. Indeed,
\[ \mu_s(m+m',n) = (s(m+m')) \otimes n = (sm+sm') \otimes n = sm \otimes n + sm' \otimes n = \mu_s(m,n) + \mu_s(m',n),\]
similarly $\mu_s(m,n+n')=\mu_s(m,n)+\mu_s(m,n')$, and
\[ \mu_s(mr,n) = smr\otimes n = sm \otimes rn = \mu_s(m,rn).\]
Thus, we obtain a well-defined map $M\otimes_R N\to M\otimes_R N$ that sends $m\otimes n\mapsto sm \otimes n$, and the given formula follows. It is easy to check that this action satisfies the module axioms.

For (b), we already know this map is additive. To see that it is $S$-linear, we compute
\[ h(s(m,n))=h(sm,n) = sm\otimes n = s(m\otimes n) =s h(m,n).\]

For (c), we know that since $f$ is $R$-balanced biadditive map there exists a unique additive map $\alpha$ such that $b=\alpha\circ h$. We just need to show that this map is $S$-linear:
\[\begin{aligned}
 &\alpha(s(\sum_i m_i\otimes n_i )) = \alpha(\sum_i sm_i \otimes n_i) = \sum_i \alpha( sm_i \otimes n_i) = \sum_i \alpha(h(sm_i , n_i))\\ &= \sum_i b(sm_i,n_i) = s(\sum_i b(m_i,n_i))=s(\sum_i \alpha(m_i\otimes n_i)) = s(\alpha(\sum_i m_i \otimes n_i)).
 \end{aligned}\]
 
 Case (3) is quite analogous. Case (1) is a special case of (2): we consider $M$ as an $(R,R)$-bimodule. The extra equality in (a) follows from  $rm \otimes n = mr \otimes n = m \otimes rn$. For (b) and (c), we note that $R$-bilinear is equivalent to $R$-balanced biadditive plus $R$-linear with respect to the module structure given in case (2).
\end{proof}

We can take tensor products of maps as well.


\begin{lem}
	Let $f: M \to M'$ be a homomorphism of right $R$-modules and  $g: N \to N'$ be a homomorphism of left $R$-modules. There exists a unique homomorphism of abelian groups \index{$f\otimes g$}$f \otimes g : M \otimes_R N \to M' \otimes_R N'$ such that
	$$(f \otimes g)(m \otimes n) = f(m) \otimes g(n)$$
	for all $m \in M$ and $n \in N$.\index{$f \otimes g$}\index{tensor product of maps}
	
	If $R$ is commutative, this map is $R$-linear.
	
	If $M$ and $M'$ are $(S,R)$-bimodules, and $f$ is also $S$-linear, then this map is an $S$-module homomorphism.
\end{lem}

\begin{proof}
	The function 
	$$\xymatrix@R=2mm{M \times N \ar[r] & M' \otimes_R N'\\ (m,n) \ar@{|->}[r] & f(m) \otimes g(n)}$$
	is $R$-balanced biadditive (and bilinear when $R$ is commutative), so the universal property of tensor products gives the desired $R$-module homomorphism, which is unique. In the bimodule case, $S$-linearity follows from observing that the function displayed above is $S$-linear on the first argument.
\end{proof}



\begin{defn} Let $R$ be a ring and $M$ be a right $R$-module. There is an additive covariant functor \index{$M\otimes_R -$}
\[M\otimes_R -: \Mod{R} \to \Ab\] that on objects sends $N$ to $M\otimes_R N$, and on morphisms sends $f:N\to N'$ to the map $1_M \otimes f$.

If $R$ is commutative, we can consider $M\otimes_R -$ as a functor from $\Mod{R} \to \Mod{R}$.

If $M$ is a $(S,R)$-bimodule, we can consider $M\otimes_R -$ as a functor from $\Mod{R} \to \Mod{S}$.
\end{defn}
\begin{proof}
Well definedness of the maps comes from the lemma. Given $A\xra{g} B\xra{f} C$, we have \[(1_M \otimes (fg) )(m\otimes a) = m\otimes (fg)(a) = (1_M \otimes f)(1_M \otimes g)(m\otimes a),\] so $(1_M \otimes (fg)) - (1_M \otimes f)(1_M \otimes g)$ vanishes on a generating set for $M\otimes_R A$, and hence is zero. Similarly for the identity property.

For additivity, we observe that \[(1_M \otimes (f+g))(m\otimes n) = m\otimes (f+g)(n) = m\otimes f(n) + m\otimes g(n)= ((1_M\otimes f)+(1_M \otimes g))(m\otimes n),\]
and since simple tensors generate, we have $1_M \otimes (f+g) = 1_M \otimes f + 1_M \otimes g$.
\end{proof}

\begin{rem} We can equally well discuss $- \otimes_R N : \Mod{R^\op} \to \Ab$ (or other targets when we have more structure akin to above).
\end{rem}

\Sept{27}


The key to unlocking more examples of tensor will be to prove that it is right exact.

\begin{thm} Let $M$ be a right $R$-module. The functor $M \otimes_R - : \Mod{R} \to \Ab$ is right exact.
\end{thm}
\begin{proof} Let \[ A \xra{i} B \xra{p} C \to 0\]
be  exact. We need to show that
\[ M\otimes_R A \xra{1_M \otimes i}  M\otimes_R B \xra{1_M \otimes p} M\otimes_R C \to 0  \]
is exact.
\begin{itemize}
\item $1_M \otimes p$ is surjective: Given $\sum_i m_i \otimes c_i \in M\otimes_R C$, we can find $b_i\in B$ such that $p(b_i)=c_i$ for all $i$; then $(1_M \otimes p)(\sum_i m_i \otimes b_i)=\sum_i m_i \otimes c_i$.

\item $\im(1_M \otimes i)\subseteq \ker(1_M \otimes p)$: We have $(1_M \otimes p)(1_M \otimes i) = 1_M \otimes (pi) = 1_M \otimes 0 = 0$.

\item $\ker(1_M \otimes p)\subseteq \im(1_M \otimes i)$: From above, the map $1_M\otimes p$ induces a surjection $\alpha:(M\otimes_R B)/\im(1_M \otimes i) \to M\otimes_R C$ that maps $m\otimes b \mapsto m\otimes p(b)$. We will construct an inverse for this map.

Consider the map 
\[\xymatrix@R=.5em{\mu: M\times C \ar[r] & (M\otimes_R B)/\im(1_M \otimes i)& \\
(m,c) \ar@{|->}[r] & m\otimes b &\text{for some $b$ with $p(b)=c$.}}\]
To see this is well-defined, note that if $p(b)=p(b')=c$, then $p(b-b')=0$, so $b-b'=i(a)$ for some $a\in A$, so 
\[ (m\otimes b) - (m\otimes b') = m\otimes(b-b') =m\otimes i(a) \in \im(1_M\otimes i).\]
We then check $\mu$ is $R$-balanced biadditive:
\[ \mu(m+m',c)=(m+m')\otimes b = m\otimes b + m' \otimes b = \mu(m,c) + \mu(m',c).\]
If $p(b)=c$ and $p(b')=c'$, then $p(b+b')=c+c'$, so
\[ \mu(m,c+c')=m\otimes (b+b') = m\otimes b + m\otimes b' = \mu(m,c) + \mu(m,c'),\]
and, if $p(b)=c$, then $p(rb)=rc$, so
\[ \mu(mr, c) = mr\otimes b = m \otimes rb = \mu(m,rc).\]
Thus, $\mu$ induces an additive homomorphism $\beta:M\otimes_R C \to(M\otimes_R B)/\im(1_M \otimes i) $. By construction, we have $\beta\circ \alpha (m\otimes b) = m\otimes b$ for all simple tensors, and thus this is the identity map since simple tensors generate.

Since $\alpha$ has a left inverse, it follows that $\alpha$ is injective, so $\im(1_M \otimes i)$ is equal to the kernel of $1_M \otimes p$.\qedhere
\end{itemize}
\end{proof}


\sssec{Examples of tensors}

\begin{prop}
Let $R$ be a ring. There is a natural isomorphism between $R\otimes_R -$ and the identity functor on $\Mod{R}$. In particular, for every $R$-module $M$, there is an $R$-module isomorphism $R\otimes_R M \cong M$ for every (left) $R$-module $M$.
\end{prop}
\begin{proof}Note that $R$ is an $(R,R)$-bimodule, so $R\otimes_R M$ is again an $R$-module.
Now, 	$$\xymatrix@R=2mm{R \times M \ar[r] & M \\
	(r,m) \ar@{|->}[r] & rm}$$
	is biadditive (by distributive laws), $R$-balanced (by associativity module axiom), and $R$-linear, so it
	induces a homomorphism of $R$-modules $\xymatrix{R \otimes_R M \ar[r]^-{\varphi_M} & M.}$ By construction, $\varphi_M$ is surjective. Moreover, the map 
	$$\xymatrix@R=1mm{M \ar[r]^-{f_M} & R \otimes_R M \\ m \ar@{|->}[r] & 1 \otimes m}$$ 
	is a homomorphism of $R$-modules, since 
	\[f_M(a+b) = 1 \otimes (a+b) = 1 \otimes a + 1 \otimes b\]
	\[ f_M(ra) = 1 \otimes (ra) =  r\otimes a = r(1 \otimes a) = rf_M(a).\]
	For every $m \in M$, $\varphi_M f_M(m) = \varphi_M(1 \otimes m) = 1m = m$, and for every simple tensor, $f_M \varphi_M (r \otimes m) = f_M(rm) = 1 \otimes (rm) = r \otimes m$. This shows that $\varphi_M$ is an isomorphism.
	
	Finally, given any $f \in \Hom_R(M,N)$, since $f$ is $R$-linear we conclude that the diagram
	$$\xymatrix@R=5mm@C=5mm{
	 R \otimes_R M \ar[rrr]^-{\varphi_M} \ar[dd]_-{1 \otimes f} &&& M \ar[dd]^-{f} \\
	&&&&\\
	 R \otimes N \ar[rrr]_-{\varphi_N} &&& N
	}$$
	commutes, as $r\otimes m \mapsto rf(m)$ either way, so our isomorphism is natural.
\end{proof}



\begin{prop} Let  $R$ be a ring, $\{M_\lambda\}_{\lambda\in \Lambda}$ be  a family of right $R$-modules, $N$ be a left $R$-module. There is an isomorphism
$$
\phi: \left(\bigoplus_{\lambda\in \Lambda} M_\lambda \right) \otimes_R N \xra{\cong} \bigoplus_{\lambda\in \Lambda} \left(M_\lambda \otimes_R N\right)
$$
that sends $(m_i)_{i \in I} \otimes n$ to $(m_i \otimes n)_{i \in I}$.
This is an isomorphism of abelian groups in general, of $R$-modules in the commutative case, of $S$-modules if each $M_\lambda$ is an $(S,R)$-bimodule, and of right $S$-modules if $N$ is an $(R,S)$-bimodule.
\end{prop}


\begin{proof} Define 
$$
b: \left(\bigoplus_{\lambda\in\Lambda} M\right) \times N\to \bigoplus_{\lambda\in\Lambda}  \left(M_\lambda \otimes_R N\right)
$$
by 
$$
b((m_\lambda) , n) = (m_\lambda \otimes n).
$$
The map $b$ is $R$-balanced biadditive in general, and linear with respect to the specified action in each of the other cases. Thus, it induces a morphism $\phi$ of the specified type.

To show $\phi$  is an isomorphism, we construct an inverse. 
For each $i$ we define a pairing
$$
b_\lambda: M_\lambda \times N \to \left(\bigoplus_{\lambda\in\Lambda} M_\lambda \right) \otimes_R N 
$$
by $b_\lambda(x, n) = \iota_\lambda(x) \otimes n$, where $\iota_\lambda: M_\lambda \to \left( \bigoplus_{\lambda\in\Lambda} M_\lambda \right)$ is the canonical inclusion map.
Then $b_\lambda$ is $R$-balanced biadditive in general, and linear with respect to the specified action in each of the other cases and hence induces a morphism
$\psi_i:  M_\lambda \otimes_R N \to \left(\bigoplus_{\lambda \in \Lambda} M_\lambda \right) \otimes_R N$.

By the universal mapping property for coproducts the maps $\psi_\lambda, \lambda\in \Lambda$ 
determine a morphism 
$$
\psi:  \bigoplus_{\lambda\in\Lambda} (M_\lambda \otimes_R N) \to \left(\bigoplus_{\lambda \in \Lambda} M_\lambda \right) \otimes_R N.
$$

It is easy to see that both $\psi \circ \phi$ and $\phi \circ \psi$ are the identity maps by observing that they act as the identity on simple tensors.
\end{proof}

\begin{rem} The same property holds on the right side of the tensor.
\end{rem}

\begin{ex} If $F=R^{\oplus \Lambda}$ is a free module, and $M$ is any $R$-module, then $R^{\oplus \Lambda} \otimes_R M \cong M^{\oplus \Lambda}$, and this isomorphism is natural in $M$.
\end{ex}

\begin{ex} As a special case, $R^{\oplus \Gamma} \otimes_R R^{\oplus \Lambda}$ is a free module on the basis $\{e_\gamma \otimes e_\lambda \ | \ (\gamma,\lambda)\in \Gamma\times\Lambda\}$. 

Even more concretely, if $K$ is a field, $K^m \otimes_K K^n \cong K^{m\times n}$ is isomorphic to the collection of $m\times n$ matrices, by the isomorphism that takes $e_i\otimes e_j$ to the matrix that has a $1$ in the $i,j$ entry and zeroes elsewhere. This morphism then sends $(a_1,\dots,a_m)\otimes (b_1,\dots,b_n)$ to $[a_i b_j]$, the outer product of these matrices. Observe that the simple tensors correspond exactly to the matrices of rank at most one.
\end{ex}


\begin{rem}
Let $R$ be a ring, $M$ be a right $R$-module, and $N$ be a left $R$-module. We can compute $M\otimes_R N$ by taking a presentation of $M$
\[ R^{\oplus \Gamma} \xra {\phi} R^{\oplus \Lambda} \to M \to 0\]
and tensoring with $N$ to get
\[ N^{\oplus \Gamma} \xra{} N^{\oplus \Lambda} \to M \otimes_R N \to 0,\]
so $M\otimes_R N$ is isomorphic to the cokernel of the map $N^{\oplus \Gamma} \to N^{\oplus \Lambda}$ induced by $\phi$. We can also compute $M\otimes_R N$ by taking a presentation of $M$
\[ R^{\oplus \Xi} \xra {\psi} R^{\oplus \Omega} \to N \to 0\]
and tensoring with $M$ to get
\[ M^{\oplus \Xi} \xra{} M^{\oplus \Omega} \to M \otimes_R N \to 0,\]
so $M\otimes_R N$ is isomorphic to the cokernel of the map $M^{\oplus \Gamma} \to M^{\oplus \Lambda}$ induced by $\psi$. 
\end{rem}

\begin{ex}
Let $R$ be a commutative ring, $I$ an ideal, and $M$ a module. There is an isomorphism
$R/I \otimes_R M \cong M/IM$. Indeed, if $I=(\{f_\gamma\})$, then we have a presentation
\[ R^{\oplus \Gamma} \xra{\cdot [ \{f_\gamma\} ] } R \to R/I \to 0,\]
so
\[ M^{\oplus \Gamma} \xra{\cdot [ \{f_\gamma\} ] } M \to R/I \otimes_R M \to 0\]
is exact. The image of the first map is just $IM$, so we obtain the isomorphism.
\end{ex}

\Sept{29}


Tensor is not exact in general.

\begin{ex} Consider the short exact sequence 
\[ 0 \to \Z \xra{2} \Z \to \Z/2\Z \to 0\] and apply $-\otimes_{\Z} \,\Z/2\Z$. We obtain the complex

\[ 0 \to \Z/2\Z \xra{0} \Z/2\Z \xra{1} \Z/2\Z \to 0,\]
which is exact at the last two $\Z/2\Z$'s but not at the first one.
\end{ex}

The following properties are also useful properties for computing tensors.

\begin{exer} Let $R$ be commutative and $M$ and $N$ be $R$-modules. There is an isomorphism $M\otimes_R N \cong N \otimes_R M$.
\end{exer}

\begin{exer} Let $R$ and $S$ be rings. Let $L$ be a right $R$-modules, $M$ be an $(R,S)$-bimodule, and $N$ be an $S$-module. Then $(L\otimes_R M) \otimes_S N \cong L \otimes_R (M\otimes_S N)$.
\end{exer}

An important case of the tensor functor is tensoring with a ring.

\begin{defn}
Let $\phi: R \to S$ be a ring homomorphism. The functor
\[ S \otimes_R - : \Mod{R} \to \Mod{S}\]
is called the functor of \DEF{extension of scalars} from $R$ to $S$.
\end{defn}

Observe that $S$ is an $(S,R)$-bimodule, so this functor does indeed return $S$-modules. By the discussion above, extension of scalars turns an $R$-module into the $S$-module with the same presentation.


\sssec{Hom tensor adjointess}

The Hom and tensor functors are closely related.

\begin{thm} Let $R,S$ be rings, and $A$ be an $(R,S)$-bimodule, $B$ be an $S$-module, and $C$ be and $R$-module. There is an isomorphism

\[ \Hom_R(A\otimes_S B, C) \cong \Hom_S(B,\Hom_R(A,C)).\]

Moreover, these isomorphisms are natural in each argument.
\end{thm}
\begin{proof}
Take 
\[\eta:\Hom_R(A\otimes_S B, C) \to \Hom_S(B,\Hom_R(A,C)) \]
by the rule $\eta(\phi)(b)(a) = \phi(a\otimes b)$. We check

The map $\eta(\phi)(b)$ that sends $a\mapsto \phi(a\otimes b)$ for fixed $b$ is $R$-linear: addition is fine and \[\eta(\phi)(b)(ra)=\phi(ra \otimes b) = \phi(r(a\otimes b))=r\phi(a \otimes b) = r \eta(\phi)(b)(a). \]

The map $\eta(\phi)$ that sends $b \mapsto \phi(-\otimes b)$ is $S$-linear: addition is fine and
\[ \eta(\phi)(sb)(a)= \phi(a \otimes sb) = \phi(as \otimes b) = (s\eta(\phi)(b))(a).\]

Now take
\[ \mu: \Hom_S(B,\Hom_R(A,C)) \to \Hom_R(A\otimes_S B, C) \]
by the rule $\mu(\psi)(a\otimes b) = \psi(b)(a)$.
We need to check that $\mu(\psi)$ is well-defined and $R$-linear: to do this we check that the map $A\times B \to C$ given by $(a,b)\mapsto \psi(b)(a)$ is $S$-balanced biadditive and $R$-linear on the left factor (omitted). 

We then see that $\mu$ and $\eta$ are mutually inverse: 
\[(\mu\circ\eta)(\phi)(a\otimes b) = \eta(\phi)(b)(a) = \phi(a\otimes b)\]
\[ (\eta\circ \mu)(\psi)(b)(a) = \mu(\psi)(a\otimes b) = \psi(b)(a).\]

What do the naturality claims mean? First, this is a natural isomorphism as a functor of $A$:
\[ \Hom_R( - \otimes_S B, C) \stackrel{\cong}{\implies} \Hom_S( B,\Hom_R(-,C)),\]
and likewise for $B$ and $C$. We won't write these out, but they are straghtforward.
\end{proof}

Hom-tensor adjunction has a nice consequence in terms of extension of scalars.

\begin{defn} Let $\phi:R\to S$. There is a functor\index{$\mathrm{Res}_\phi$}
\[ \mathrm{Res}_{\phi}: \Mod{S} \to \Mod{R}\]
called the functor of \DEF{restriction of scalars} that maps an $S$-module $M$ to the $R$-module that is the same abelian group as $M$ with action $r \cdot m = \phi(r) m$, and is the identity mapping on morphisms.
\end{defn}

When $\phi$ is injective, this restriction is literally just restricting the action. Evidently, this functor is exact, as it does nothing on the level of abelian groups, and exactness can be characterized there.

\begin{prop} Let $\phi:R\to S$ be a ring homomorphism. Let $M$ be an $R$-module and $N$ be an $S$-module. There is an isomorphism
\[ \Hom_R(M,\mathrm{Res}_\phi(N)) \cong \Hom_S(S\otimes_R M , N).\]
These isomorphisms are natural in $M$ and in $N$.
\end{prop}
\begin{proof}
Consider $S$ as an $(S,R)$-bimodule, where the right action is through $\phi$. With this structure, $\Hom_S(S,N) \cong \mathrm{Res}_\phi(N)$. Thus, this follows from Hom-tensor adjunction.
\end{proof}


\Oct{1}


\sssec{Multilinear maps}

Let $R$ be a commutative ring. Associativity of tensor implies that for any finite set of modules $M_1,\dots,M_n$, we can tensor them all together and it doesn't matter how we group them. 

Observe that for any $R$-modules $M$ and $N$,  if $M$ is generated by $m_1,\dots,m_a$ and $N$ is generated by $n_1,\dots,n_b$, then $M\otimes_R N$ is generated by $\{m_i \otimes n_j \ | \ i=1,\dots, a; j=1,\dots,b\}$: we can write any element as a sum of simple tensors, and write each simple tensor $m\otimes n = (\sum_i r_i m_i) \otimes (\sum_j s_j b_j) = \sum_{i,j} r_i s_j (m_i \otimes n_j)$.

Likewise, by a straightforward induction on $n$, in $M_1\otimes_R \cdots\otimes_R M_n$, every element is a sum of simple tensors, and an $R$-linear combination of simple tensors of generators of the modules $M_i$.

\begin{defn}
Let $R$ be a commutative ring, and $M_1,\dots,M_n,N$ be $R$-modules. We say that a map $\psi: M_1 \times \cdots \times M_n \to N$ is \DEF{multilinear} or \DEF{$R$-multilinear} if it is  $R$-linear in each argument: i.e., for each $i$,
\[ \psi(m_1,\dots,r m_i + m'_i,\dots,m_n) = r \psi(m_1,\dots,m_i,\dots,m_n) + \psi(m_1,\dots,m'_i,\dots,m_n).\]
\end{defn}

Note that when $n=2$, this is just the notion of $R$-bilinear. 

\begin{prop} There is a multilinear map
\[ h: M_1 \times \cdots \times M_n \to M_1 \otimes_R \cdots  \otimes_R M_n \] that satisfies the following universal property: for any multilinear map $\psi: M_1 \times \cdots \times M_n \to N$, there is an $R$-linear map $\alpha: M_1 \otimes_R \cdots  \otimes_R M_n \to N$ such that $\psi = \alpha\circ h$.
\end{prop}

\begin{proof}
 For the map $h$, we take $h(m_1,\dots,m_n) = m_1\otimes \cdots \otimes m_n$. Then, if such a map $\alpha$ exists, we must have $\alpha(m_1\otimes\cdots\otimes m_n)= \psi(m_1,\dots,m_n)$; since simple tensors generate, $\alpha$ is unique if it exists. For existence, we can proceed by induction on $n$. For any fixed $m_n\in M_n$, the map $\psi$ is a multilinear map on the first $n-1$ arguments, so by the inductive hypothesis, we obtain an $R$-linear map $M_1 \otimes_R \cdots  \otimes_R M_{n-1} \to N$ that sends $m_1\otimes \dots \otimes m_{n-1} \mapsto \psi(m_1,\dots,m_n)$. Since we have such a map for each $m_n$, we get a map $M_1 \otimes_R \cdots  \otimes_R M_{n-1} \times M_n \to N$ that we can check to be bilinear, and this induces the desired map.
\end{proof}

\begin{comment}
\begin{defn} Let $M$ and $N$ be $R$-modules.
\begin{enumerate}
\item A multilinear map $\psi: M^{\times n} \to R$ is \DEF{symmetric} if permuting any two entries does not affect the value of $\psi$.
\item A multilinear map 
$\psi: M^{\times n} \to R$ is \DEF{skew-symmetric} if permuting any two entries negates the corresponding value of $\psi$.
\end{enumerate}
\end{defn}

\begin{defn} Let $R$ be a commutative ring, and $M$ be an $R$-module. 
\begin{enumerate}
\item
The $n$th \DEF{symmetric power} of $M$ is the $R$-module $\mathrm{Sym}^n(M) = M^{\otimes n} / (Y)$, where $Y$ is the submodule generated by differences of simple tensors that are $1$ in all but two entries and the two nontrivial entries are switched. We often just write $m_1 m_2 \cdots m_n$ for the class of $m_1\otimes m_2 \otimes\cdots m_n$.
\item The $n$th \DEF{exterior power} of $M$ is the $R$-module $\bigwedge^n(R) = M^{\otimes n} / (Z)$, where $Y$ is the submodule generated by sums of simple tensors that are $1$ in all but two entries and the two nontrivial entries are switched. We often just write $m_1 \wedge m_2 \wedge \cdots \wedge m_n$ for the class of $m_1\otimes m_2 \otimes\cdots m_n$.
\end{enumerate}
\end{defn}


For example, \[\mathrm{Sym}^2(M) = M\otimes_R M / \langle \{m\otimes n - n\otimes m \ | \ m,n\in M\}\rangle\] and \[\bigwedge^2(M) = M\otimes_R M / \langle \{m\otimes n + n\otimes m \ | \ m,n\in M\}\rangle.\]

\begin{thm} Let $M$ be an $R$-module.
\begin{enumerate}
\item
 There is a symmetric multilinear map $h:M^{\times n} \to \mathrm{Sym}^n(M)$ such that for every symmetric multilinear map $\psi: M^{\times n} \to N$, there is a unique $R$-module homomorphism $\alpha:\mathrm{Sym}^n(M) \to N$ such that $\psi=\alpha\circ h$.
 \item
 There is a skew-symmetric alternating multilinear map $h:M^{\times n} \to \bigwedge^n(M)$ such that for every skew-symmetric multilinear map $\psi: M^{\times n} \to N$, there is a unique $R$-module homomorphism ${\alpha:\bigwedge^n(M) \to N}$ such that $\psi=\alpha\circ h$.
 \end{enumerate}
 \end{thm}
\begin{proof}
\begin{enumerate}
\item By construction, there is a map $h$ that comes from the map to the tensor product followed by the quotient map. It is symmetric and multilinear by construction. By the universal property for tensor, there is a map from $M^{\otimes n} \to N$; since it is symmetric, $Y$ maps to $0$, so it factors through $\mathrm{Sym}^n(M)$.
\item Similar.\qedhere
\end{enumerate}
\end{proof}

We can compute symmetric and exterior powers of free modules concretely.

\begin{prop} Let $F= R^{\oplus n}$ be a free module with basis $\{e_1,\dots,e_t\}$.
\begin{enumerate}
\item $\mathrm{Sym}^t(F)$ is a free $R$-module with basis $\{ e_1^{a_1} \cdots e_n^{a_n} \ | \ a_1+\cdots+a_n=t\}$.
\item  If $2$ is invertible in $R$, then $\bigwedge ^n F$ is a free $R$-module with basis \[\{ e_{i_1} \wedge e_{i_2} \wedge\cdots \wedge e_{i_t} \ | \ 1\leq i_1 < i_2 <\cdots < i_t \leq n\}.\]
\end{enumerate}
\end{prop}
\begin{proof}
\begin{enumerate}
\item We note first that $M^{\otimes n}$ is generated by the simple tensors where each entry is a basis vector. In $\mathrm{Sym}^t(F)$ in the class of any such simple tensor, there is one where all the $e_1$'s come before all the $e_2$'s, etc., so the displayed set is in fact a generating set. This gives a surjection from the free module to the symmetric power. 

On the other hand, there a map from $F^{\otimes t}$ to the free module with the specified basis that sends $((r_{11},\dots,r_{n1}),\dots,(r_{1t},\dots,r_{nt}))$ to the vector whose $e_1^{a_1} \cdots e_n^{a_n}$ coefficient is the $x_1^{a_1}\cdots x_n^{a_n}$-coefficient of $(r_{11}x_1+\cdots+ r_{n1}x_n)\cdots (r_{1t}x_1+\cdots +r_{nt}x_n)$ in $R[x_1,\dots,x_n]$. %One can write this map out explicitly: the $e_1^{a_1} \cdots e_n^{a_n}$ coefficient is the sum of 
%the products $\prod_{i=1}^t r_{\alpha_i, i}$ indexed
%over all tuples $\alpha=(\alpha_1,\dots,\alpha_t)\in \{1,\dots,n\}^t$ such that $i$ occurs $a_i$ times for each $i\in \{1,\dots, n\}$. 
This map is multilinear and symmetric. This then induces a map from the symmetric product that is an inverse of the given map.
\item We observe that any simple tensor/simple wedge of basis elements that involves the same basis element twice is zero, since it plus itself is zero and 2 is a unit. We can then express any element as a linear combination of wedges over distinct basis elements and such that they are in increasing order. We get the surjection from the specified free module analogously to the symmetric case.

We define a map from $F^{\times t}$ to the specified free module that sends the tuple of vectors \[((r_{11},\dots,r_{n1}),\dots,(r_{1t},\dots,r_{nt}))\] to the vector whose $e_{i_1} \wedge \cdots \wedge e_{i_t}$ coefficient is the determinant of the matrix consisting of rows $i_1,\dots,i_t$ of the coefficient matrix. This is easily seen to be multilinear and skew-symmetric, and induces the inverse of the first map.\qedhere
\end{enumerate}
\end{proof}

\begin{rem}
Our three flavors of tensor are all recipes for rings. Given an $R$-module $M$, \index{$T(M)$}$T(M):=\bigoplus_{n\in \N} M^{\otimes n}$, the \DEF{tensor algebra} is a ring where the operation in  given by concatenating simple tensors: \[(m_1\otimes \cdots \otimes m_a)  \cdot (m_1 \otimes \cdots \otimes m_b) = m_1 \otimes \cdots \otimes m_a \otimes m_1 \otimes \cdots \otimes m_b.\]
One first checks that this gives a well-defined rule for multiplication $M^{\otimes a} \times M^{\otimes b} \to M^{\otimes a+b}$ using universal property of tensor, then that this gives a well-defined rule for multiplication $T(M)\times T(M) \to T(M)$ using universal property of coproduct. When $M$ is free, this is a noncommutative version of a polynomial ring in the basis elements.

Likewise \index{$S(M)$}$S(M):=\bigoplus_{n\in \N} \mathrm{Sym}^n(M)$, the \DEF{symmetric algebra} and \index{$\bigwedge(M)$}$\bigwedge(M):=\bigoplus_{n\in \N} \bigwedge^n(M)$, the \DEF{exterior algebra} are rings with operations induced by concatenation. The symmetric algebra of a free module is isomorphic to a polynomial ring in the basis elements, and the exterior algebra of a free module is a ``skew commutative'' version of a polynomial ring.
\end{rem}
\end{comment}


\sssec{Tensor products of rings}

\begin{prop}
Let $A$ be a commutative ring, and $R$ and $S$ be commutative $A$-algebras. Then the tensor product $R\otimes_A S$ is a commutative ring, where the multiplication on simple tensors is given by $(r\otimes s) \cdot (r'\otimes s') = rr' \otimes ss'$.
\end{prop}
\begin{proof}
We need to show that there is a well-defined map that corresponds to this formula for multiplication. Note that the map \[ \xymatrix@R=.5em{R\times S \times R \times S \ar[r] & R\otimes_A S\\
(r,s,r',s') \ar@{|->}[r] & rr' \otimes ss'}\]
is multilinear over $A$. Thus, we get a well defined map
\[ \xymatrix@R=.5em{R\otimes_A S \otimes_A R \otimes_A S \ar[r] & R\otimes_A S\\
r\otimes s \otimes r' \otimes s' \ar@{|->}[r] & rr' \otimes ss'.}\]
Thinking of $R\otimes_A S \otimes_A R \otimes_A S = (R\otimes_A S) \otimes_A (R \otimes_A S)$ and precomposing with the natural map from product to tensor product, we get 
a well defined $A$-bilinear map
\[ \xymatrix@R=.5em{(R\otimes_A S) \times (R \otimes_A S) \ar[r] & R\otimes_A S\\
(r\otimes s , r' \otimes s') \ar@{|->}[r] & rr' \otimes ss'.}\]
The bilinearity of this map translates into the distributive laws. Commutativity and associativity of multiplication can be checked on simple tensors, since these generate, and for each these follow from the same properties in $R$ and $S$. $1\otimes 1$ is an evident multiplicative identity.
\end{proof}

\begin{exer} If $R$ and $S$ are commutative rings, then $R\otimes_{\Z} S$ is the coproduct of $R$ and $S$ in the category of commutative rings. Moreover, if $R$ and $S$ are commutative $A$-algebras, then $R\otimes_A S$ is the coproduct of $R$ and $S$ in the category of commutative $A$-algebras.
\end{exer}

\begin{prop} If $A$ is a commutative ring, and $R$ is an $A$-algebra, then $A[x_1,\dots,x_n] \otimes_A R\cong R[x_1,\dots,x_n]$ as rings.
\end{prop}
\begin{proof}
Consider the map $A[x_1,\dots,x_n] \times R \to R[x_1,\dots,x_n]$ given by $(f(\mathbf{x}),r) \mapsto r f(\mathbf{x})$. This is $A$-bilinear, so we get an induced map on the tensor product. It is evidently additive, and also clearly multiplicative, so it is a ring homomorphism.

Consider the structure of $A[x_1,\dots,x_n]$ as an $A$-module. Every element is an $A$-linear combination of monomials in a unique way, so the monomials form a free basis. Similarly for $R[x_1,\dots,x_n]$. Thus, we have 
\[A[x_1,\dots,x_n] \otimes_A R = (\bigoplus_{\alpha} A x^\alpha) \otimes_A R \cong \bigoplus_{\alpha} R x^\alpha = R[x_1,\dots,x_n],\]
where the middle isomorphism is the extension of scalars isomorphism that sends $x^\alpha \otimes 1$ to $x^\alpha$, so this isomorphism is the same map considered above; hence our map is an isomorphism.\end{proof}

\begin{ex} $A[x]\otimes_A A[x] = A[x,y]$. \end{ex}

\begin{prop} If $A$ is a commutative ring, $R$ is an $A$-algebra, and $S=A[x_1,\dots,x_n]/I$ is an $A$-algebra, then 
\[ R\otimes_A S \cong \frac{R[x_1,\dots, x_n] }{I R[x_1,\dots,x_n]}.\]
\end{prop}
%\begin{proof}
%Consider the right exact sequence 
%\[ I \to A[x_1,\dots,x_n] \to S \to 0\]
%and tensor with $R$ to get the right exact sequence
%\[ R\otimes_A I \to R[x_1,\dots x_n] \to R\otimes_A S \to 0.\]
%Thus, $\displaystyle R\otimes_A S \cong \frac{R[x_1,\dots, x_n] }{\mathrm{im}(R \otimes_A I)}$.
%It suffices to observe that the image of $R\otimes_A I$ in $R[x_1,\dots,x_n]$ consists of all sums of elements of the form $r a$ with $a\in I$, which is exactly the ideal generated by $I$ in $R[x_1,\dots,x_n]$.
%\end{proof}

\Oct{4}

\ssec{Projective, injective, and flat modules}

\sssec{Projective modules}

\begin{defn} An $R$-module $P$ is {\em projective} if given any surjective homomorphism of modules $p: N \onto N''$ and a homomorphism $f: P \to N''$, there
is a homomorphism $g: P \to N$ such that $p \circ g = h$. In other words, given the solid arrows in the diagram
$$
\xymatrix{
& P \ar[d]^f \ar@{-->}[dl]_{\exists g} \\
N \ar[r]^p & N'' \ar[r] & 0 \\
}
$$
in which the bottom row is exact, there exists at least one dotted arrow that causes the triangle to commute.
\end{defn}

\begin{prop}
\label{prop:freeimpliesproj}
 Every free $R$-module is projective.
\end{prop}

\begin{proof} Suppose $P$ is free with basis $B$ and let a diagram as in the definition be given. Since $p$ is surjective, for each $b \in B$, we can find an element $n_b \in N$
such that $f(b) = p(n_b)$. Since $B$ is a basis, the assignment $b \mapsto n_b$ extends uniquely to an $R$-module homomorphism $g: P \to N$. 
The triangle commutes since $p \circ g$ and $f$ agree on $B$. 
\end{proof}

We will see soon that the converse is false.

\begin{prop} \label{prop1111}
For a ring $R$ and module $P$, the following are equivalent:
\begin{enumerate}
\item $P$ is projective,
\item the functor $\Hom_R(P, -)$ is exact,
\item every short exact sequence of the form $0 \to N' \to N \to P \to 0$ is split,
\item every surjective $R$-module homomorphism $p: N \onto P$ has a right inverse, and
\item $P$ is a summand of a free $R$-module; i.e., there is an $R$-module $Q$ such that $F=P \oplus Q$ is a free $R$-module.
\end{enumerate}
\end{prop}


\begin{proof} 
Since $\Hom_R(P, -)$ is left exact for any module $P$, $\Hom_R(P,-)$ is exact if and only if it preserves surjections.
The definition of ``projective'' is just an unpackaging of the property that $\Hom_R(P, -)$ preserves surjections. 
The equivalence of (1) and (2) is thus essentially by definition. 

The equivalence of (3) and (4) follows from the Splitting Theorem. Note that given an surjective map $p: N \onto P$, we may form the short exact sequence $0 \to \ker(p) \to N \xra{p} P \to 0$.

Suppose (1) holds and  $p: N \onto P$ is onto. Applying the definition with $f = \id_P$ and $p = p$ gives an $R$-linear map $g$ 
such that $p \circ f = \id_P$.  So (1) $\Rightarrow$ (4).

To see (1) implies (4), let $p:N\onto P$ be surjective, and consider the identity map on $P$. By (1), the identity map factors through $p$, so $p$ has a right inverse.

Assume (3) holds. By choosing a generating set for $P$ (e.g., all of $P$) we may find a surjection $p: F \onto P$ with $F$ a free $R$-module.
This map splits by assumption, and thus  $P \oplus \ker(p) \cong F$, so that (5) holds. So (3) $\Rightarrow$ (5).

Assume (5) holds. Say $F = P \oplus Q$ is free, and let a diagram as in the definition be given. Let $\pi: F \onto P$ be the canonical surjection.
Since $F$ is projective (by the example above), there is a $h: F \to N$ so that $p \circ h = f \circ \pi$. 
Define $g: P \to N$ to be $h \circ \iota$ where $\iota: P \to F$ sends $x$ to $(x,0)$. Then
$p(g(x)) = p(h(x,0)) = f(\pi(x,0)) = f(x)$. So $P$ is projective (i.e. (1) holds). 
\end{proof}


\begin{rem} \label{rem1111}
  The proof of (5) $\Rightarrow$ (1) shows more than advertised: it shows that if $P$ is a summand of projective $R$-module, then $P$ is projective.
\end{rem}

\begin{ex} Let $R = \Z[\sqrt{-5}]$ and let $P$ be the ideal $(2, 1 + \sqrt{-5})$. We claim $P$ is projective as an $R$-module, but not free.

It's not free since an ideal in an integral domain is free as a module if and only if it is principal (exercise). And you should have seen in 818 that this ideal is not principal.

To prove it is projective I will prove it is a summand of a free module. Let
$$
\pi: R^2 \onto P
$$
be the map given by the row vector $[2, 1 + \sqrt{-5}]$; that is
$\pi(x,y) = 2x + (1+\sqrt{-5})y$, which is clearly onto. Define $j: P \to R^2$ to be the map
$$
j(z) = (-z, 3z/(1 + \sqrt{-5})).
$$
The target of $j$ really is $R^2$ since for $z = 2 \alpha + (1+\sqrt{-5}) \beta$
we have
$$
j(z) = (-z, (1-\sqrt{-5})\alpha + 3\beta) \in R^2,
$$
using that $3 \cdot 2 = (1-\sqrt{-5}) (1+\sqrt{-5})$. We have
$$
\pi(j(z)) = -2z + 3z = z;
$$
that is, $p$ is a split surjection with splitting $j$. It follows that 
$$
R^2 \cong P \oplus \ker(\pi),
$$
and hence $P$ is projective. 
\end{ex}

\begin{ex}
\label{ex:hairyball}
Let
$$
R = \R[x,y,z]/(x^2 + y^2 + z^2 -1)
$$
and let $P$ be the kernel of the map
$$
\pi: R^3 \xra{[x,y,z]} R.
$$
$\pi$ is in fact a split surjection, since $\pi \circ j = \id_R$ where $j(r) = (xr, yr, zr)$. This also follows because $R$ is projective. 
So we have
$$
R^3 \cong P \oplus R
$$
and in particular this shows $P$ is projective. 

It's not free; can you prove it? Tip: Hairy Ball Theorem.
\end{ex}

\Oct{6}


The following technical result is sometimes useful:

\begin{exer} Let $R$ be a ring and  $\{M_\lambda\}_{\lambda \in \Lambda}$ a family of $R$-modules.  The coproduct (direct sum) $\bigoplus_{\lambda \in \Lambda} M_\Lambda$ of this family is projective if and only if each $M_\lambda$ is projective. 
\end{exer}

\sssec{Injective modules}

Injective is the dual notion for projective.

\begin{defn} An $R$-module $E$ is \DEF{injective} if given solid arrows as in the diagram
$$
\xymatrix{
0 \ar[r] & N' \ar[d]^f \ar[r]^i & N \ar@{-->}[dl]^{\exists g} \\ 
& E \\
}
$$
in which the top row is exact, there exists at least one dotted arrow that causes the triangle to commute.
\end{defn}

\begin{ex} If $K$ is a field, then $K$ is an injective $K$-module. Given a diagram
$$
\xymatrix{
0 \ar[r] & W \ar[d]_-f \ar[r]^i & V \ar@{-->}[dl] \\ 
& K \\
}
$$
of $K$ vector spaces, there is a splitting $q$ of $i$, and we can take $f\circ q$ as the desired map.
\end{ex}

\begin{ex}
$\Z$ is not an injective $\Z$-module: there is no $\Z$-linear map making the diagram commute below
$$
\xymatrix{
0 \ar[r] & \Z \ar[d]_-1 \ar[r]^2 & \Z \ar@{-->}[dl] \\ 
& \Z \\
}
$$
since such a map would send $2$ to $1$.
\end{ex}

%No generality would be lost if we took $i$ to be the inclusion of a submodule in this definition. So, $E$ is injective if and only if
%given a submodule $N'$ of $N$ and an $R$-map $f: N' \to E$, we can extend $f$ to $N$: there is an $R$-map $g: N \to E$ such that
%$g|_{N'} = f$. 


\begin{prop}
\label{prop:injective}
 The following are equivalent for an $R$-module $E$:
\begin{enumerate}
\item $E$ is injective,
\item the functor $\Hom_R(-, E)$ is exact, 
\item every short exact sequence of the form $0 \to E \to N \to N'' \to 0$ is split, and
\item every injective $R$-module homomorphism of the form $j: E \to M$ has a left inverse.
% (i.e., there is an $R$-map $p: M \to E$ so that $p \circ j = \id_E$).
\end{enumerate}
\end{prop}


\begin{proof} As with the previous proposition, the equivalence of (1) and (2) is essentially by definition, since $\Hom_R(-, E)$ is left exact for any module $E$, so this functor is right exact if and only if it takes injections $i:N'\to N$ to surjections $\Hom_R(i,E):\Hom_R(N,E)\to \Hom_R(N',E)$. 
Likewise, the equivalence of (3) and (4) follows from the Splitting Theorem. 

The proof of  (1) $\Rightarrow$ (4) is  very similar to the analogous proof for the proposition involving projective modules above: if $E$ is injective
and $j: E \to M$ is an injective $R$-linear map, then
$$
\xymatrix{
0 \ar[r] & E \ar[d]_{\id_E} \ar[r]^j & M \ar@{-->}[dl]^{\exists q} \\ 
& E \\
}
$$
can be completed, and $q \circ j = \id_E$ for any such completion. 


Assume (4) and let a diagram as in the definition of ``injective'' be given. Form the module 
$$M = \frac{E \oplus N}{\{(f(n'), -i(n')) \mid n' \in N' \}}.$$
(This is called a \DEF{pushout} of $E$ and $N$.)
%(I leave it to you to check that the denominator is a submodule of $E \oplus N$.)
Let $j: E \to M$ be the map sending $a$ to the class of $(a,0)$ and 
let $h: N \to M$ be the map sending $n$ to the class of $(0,n)$. Then the diagram below commutes
  $$
  \xymatrix{
    N' \ar[r]^i \ar[d]_f & N\ar[d]^h \\
    E \ar[r]^j & M}
  $$
and I claim that $j$ is injective. The former is clear by construction of $M$: given $n' \in N'$, we
  have $j(f(n'))- h(i(n'))   = (f(n'), -i(n')) = 0 \in M$.  
If $j(a) = \overline{(a,0)} = 0$ in $M$, then there is an $n' \in N'$ such
that $f(n') = a$ and $i(n') = 0$. But $i$ is injective and hence $a = 0$. 

By assumption (i.e. statement (4)), there is a map $q: M \onto  E$ such that $q \circ j = \id_E$. Define $g: N \to E$ as $g := q \circ h$.
Then $g \circ i = q \circ h \circ i = q \circ j \circ f  = \id_E \circ f = f$. 

This proves $E$ is injective. 
\end{proof}


\begin{exer} If $\{M_\lambda\}_{\lambda\in \Lambda}$ is a collection of modules, then $\prod_{\lambda\in\Lambda} M_\lambda$ is injective if and only if each $M_\lambda$ is injective.
\end{exer}


\begin{ex} If $K$ is a field and $R$ is a $K$-algebra, then $\Hom_K(R,K)$ is an injective $R$-module. Indeed, $\Hom_R(-,\Hom_K(R,K))\stackrel{\cong}{\implies} \Hom_K(R\otimes_R - ,K) \stackrel{\cong}{\implies} \Hom_K( - ,K)$, which is exact, since $K$ is an injective $K$-vector space.
\end{ex}



\begin{ex} Suppose $R$ is an integral domain and $E$ is an injective $R$-module. The $E$ is \DEF{divisible}: for every $x\in E$ and $r\in R\smallsetminus 0$, there is a $y\in E$ such that $x=ry$.
To see this, just apply the definition to the diagram
$$
\xymatrix{
0 \ar[r] & R \ar[d]_-{x} \ar[r]^r & R \ar@{-->}[dl]^{\exists g} \\ 
& E \\
}
$$
\end{ex}



\begin{thm}[Baer's criterion]
\label{thm:baer}
 For any ring $R$, an $R$-module  $E$ is injective if and only if every diagram of the form represented below in solid arrows
$$
\xymatrix{
0 \ar[r] & J \ar[d]_-f \ar[r]^\iota & R \ar@{-->}[dl]^{\exists g} \\ 
& E \\
}
$$
where $J$ is an ideal of $R$  and $\iota$ is the inclusion map,  can be completed by some dashed homomorphism $g$  to a commutative diagram.
\end{thm}

\begin{proof} One direction is immediate from the definition.

  Suppose each diagram as in the statement can be completed and let a diagram
$$
\xymatrix{
0 \ar[r] & N' \ar[d]_-f \ar[r]^i & N \ar@{-->}[dl]^{\exists g} \\ 
& E \\
}
$$
as in the definition of ``injective'' be given. 
For simplicity of notation, we may assume $i$ is the inclusion of a submodule $N'$ of $N$ into $N$. We need to show that given
an $R$-map $g: N' \to E$,  there is an $R$-map
$g: N \to E$ such that $g|_{N'} = f$.

Consider pairs $(M,h)$ such that $N' \subseteq M \subseteq N$ and $h: M \to E$ is an $R$-map 
such that $h|_{N'} = f$. Let $\cS$ be the collection of all such pairs, 
and partially order it by $(M_1, h_1) \leq (M_2, h_2)$ if and only if $M_1 \subseteq M_2$ and $h_2|_{M_1} = h_1$. 
The set $\cS$ is non-empty since $(N', f)$ belongs to it. 

Let us show $\cS$ satisfies the hypotheses of Zorn's Lemma. Suppose $\{(M_i, h_i)\}_{i \in I}$ is a totally ordered subset of $\cS$. Then $M := \cup_{i \in I} M_i$ is a 
submodule of $N$ (since the collection is totally ordered) and the function $h: M \to E$ defined as $h(m) = h_i(m)$ for any $i$ such that $m \in M_i$ is a well-defined
$R$-map (again, since the collection is totally ordered). So $(M,h) \in \cS$ and $(M,h) \geq (M_i, h_i)$ for all $i$. 

By Zorn's Lemma, $\cS$ has a maximal element $(M, h)$. It suffices to prove $M = N$. If not pick $x \in N \smallsetminus M$ and let $T = M + Rx$. I will show $h$ 
can be extended to $T$, arriving at a contradiction:

Set $I = \{r \in R \mid rx \in M\}$. The map $R \xra{x} T$ (sending $r$ to $tx$) restricts to a map $I \xra{x} M$ by definition of $I$, and so we have a commutative
square
$$
\xymatrix{
  I \ar[r]^{\subseteq} \ar[d]^x & R \ar[d]^x \\
  M \ar[r]^{\subseteq} & T \\
}
$$
By assumption the map $\a: I \to E$ given as the composition $I \xra{x} M \xra{h} E$ extends to a map
$\b: R \to E$. This gives a diagram
$$
\xymatrix{
  I \ar[r]^{\subseteq} \ar[d]^x & R \ar[d]^x \ar[rdd]^\beta \\
  M \ar[r]^{\subseteq} \ar[rrd]_{h} & T \ar@{-->}[rd] \\
  && E
}
$$
in which the inner square and the outer quadrilateral both commute.
I claim there is an $R$-map $\gamma: T \to E$ (the dashed arrow in the diagram) causing both triangles to commute. It is given abstractly by the fact that the square in this
diagram is a push-out. 
Define $\gamma: T \to E$ by $\gamma(m + rx) = h(m) + \beta(r)$ for $m \in M$ and $r \in R$.  
I leave it to you to prove $\gamma$ is well-defined (note that $m + rx$ can equal $m' + r'x$ without $m' = m$
and $r = r'$) and an $R$-map. Granting this, we clearly have $\gamma|_M = h$. So $(M,h) < (T, \gamma)$ in $\cS$,  a contradiction. It must be the $M = N$, and 
so we have proven $E$ is injective. 
\end{proof}

\Oct{8}

\begin{cor} For a PID, $E$ is an injective $R$-module if and only if it is divisible.
\end{cor}

\begin{proof} We already proved one direction (for any domain). 
Assume $E$ is divisible. By Baer's Criterion and the fact that every ideal in $R$ is principal by assumption, we just need to show every diagram of the form
$$
\xymatrix{
0 \ar[r] & Rr \ar[d]^f \ar[r]^\iota & R \ar@{-->}[dl]^{\exists g} \\ 
& E \\
}
$$
can be completed, where $r$ is any element of $R$. If $r = 0$, we may take $g = 0$. If $r \ne 0$, then let $f(r)=x\in E$. Since $E$ is divisible there is $y\in E$ such that $x=ry$, Now define $g:R\to E$ by $g(u)=uy$ and notice that $(g\circ\iota)(r)=g(r)=ry=x=f(r)$ hence $g\circ\iota=f$ for any element of $(r)$ since this is true for the generator~$r$.
\end{proof}

\begin{ex} Using the above criterion, $\Q$, $\Q/\Z$, and $\C^\times$ 
are injective $\Z$-modules.  
\end{ex}

Not every divisible module is injective.

\begin{ex}
Let $K$ be a field,  $R=K[x,y]$, and $Q=K(x,y)$ be the fraction field. Since $Q$ is divisible, $Q/R$ is as well. However, $Q/R$ is not injective.

Let $I=(x,y)$, and consider the map $f:I\to Q/R$ given by $f(ax+by)=a [\frac{1}{y}]$. To see that this is well defined, note that if $ax+by=cx+dy$, then $(a-c)x=(d-b)y$, so $y|(a-c)$, as $K[x,y]$ is a UFD; then $f(ax+by)-f(cx+dy) = (a-c)[\frac{1}{y}] =0$. It is easy to see that $f$ is $R$-linear.

We claim that $f$ cannot be extended to $g:R\to Q/R$. Indeed, given an extension $g$, write $g(1)=[\frac{a}{b}]$ with $a,b\in R$  and $a/b$ in lowest terms (which makes sense since $R$ is a UFD). Note that $b$ cannot be a unit, since then $a/b\in R$, so $[\frac ab]=0$, so $g$ is the zero map and hence $f$ is the zero map, which it is not. We have $0=f(y) = g(y) = y g(1) = y [\frac{a}{b}]$, so $\frac{ya}{b}\in R$. Thus $b|y$ in $R$, so without loss of generality, $b=y$. We also have $ [\frac{1}{y}]= f(x) = g(x) = x g(1)= x[\frac ay]$, which means that $ \frac{1-ax}{y} \in R$, so $y|(1-ax)$, which is a contradiction. 
\end{ex}

Note that every $R$-module admits a surjection from a projective $R$-module: there is a surjection from a free module. The dual statement is true for injectives as well.

\begin{prop} Let $M$ be an $R$-module. There exists an injective module $E$ and an injective homomorphism $i:M\to E$.
\end{prop}
\begin{proof}
First, we deal with the case that $R=\Z$.

Observe that if $\Z x$ is a nonzero cyclic group, there is a nonzero additive map $M\to \Q/\Z$: map $x$ to $[1/n]$ for some $n$ that divides the order of $x$ if finite, or to an arbitrary $[1/n]$ if infinite. Now, for an arbitrary abelian group $A$, for any nonzero $x$ we have
\[ 0 \to \Z x \to A \to A/ \Z x \to 0\]
exact, so since $\Q/\Z$ is injective, we can extend any map from $\Z x \to \Q/\Z$ to a map from $A$. For every nonzero $a\in A$, fix an additive map $\phi_a: A \to \Q/\Z$ and let $\phi:A \to \prod_{a\in A\smallsetminus 0} \Q/\Z$ be given by $\phi(x) = (\phi_a(x))_{a\in A\smallsetminus 0}$. By construction this is an injective homomorphism, and $\Q/\Z$ is an injective module.

Now let $R$ be arbitrary, and $M$ be an $R$-module. Considering $M$ as an abelian group, there is an injective abelian group $D$ and additive map $j:M\to D$ by the case above. By left exactness of $\Hom$, there is an injection $\Hom_\Z(R,M) \xra{j_*} \Hom_\Z(R,D)$. This map is $R$-linear: \[ r j_* (\alpha)(s) = r(j \alpha)(s)=j\alpha(sr) = j (r \alpha)(s)= j_*(r \alpha)(s).\]
Furthermore, there is an injection $M \cong \Hom_R(R,M) \subseteq \Hom_\Z(R,M)$. Put together, we obtain an $R$-linear injection $M\to \Hom_\Z(R,D)$. 

It remains to see that $\Hom_\Z(R,D)$ is an injective $R$-module. But \[\Hom_R(- , \Hom_\Z(R,D)) \stackrel{\cong}{\implies} \Hom_\Z(R\otimes_R - , D)  \stackrel{\cong}{\implies} \Hom_\Z ( - ,D)\] is exact, so this is the case.
\end{proof}

\sssec{Flat modules}

\begin{defn} An $R$-module $N$ is \DEF{flat} if for every injective homomorphism of right $R$-modules $M\xra{f} M'$, the induced map $M\otimes_R N \xra{f\otimes 1_N} M' \otimes_R N$ is injective.
\end{defn}

Since tensor is right exact, a module $N$ is flat if and only if $-\otimes_R N$ is an exact functor.


\begin{exer}\label{direct sum of flats}
	Given a family of $R$-modules $\{ M_\lambda \}_{\lambda \in \Lambda}$, $\bigoplus_\lambda M_\lambda$ is flat if and only if every $M_\lambda$ is flat.
\end{exer}



All projectives are flat.

\begin{thm}\label{projectives are flat}
	Every projective $R$-module is flat.
\end{thm}

\begin{proof}
	First, recall that $- \otimes_R R$ is naturally isomorphic to the identity functor, and thus exact. This shows that $R$ is flat, and thus any free module, being a direct sum of copies of $R$, must also be flat. Finally, every projective module is a direct summand of a free module. Direct summands of flat modules are flat, so every projective module is flat.
\end{proof}

\begin{prop} If $R$ is a commutative ring, and $S$ is any multiplicatively closed set, then $S^{-1}R$ is a flat $R$-module.
\end{prop}
\begin{proof}
You showed on the homework that the functor $- \otimes_R S^{-1}R$ is naturally isomorphic to the localization functor, which is exact.
\end{proof}

\begin{comment}

\begin{lem}\label{flat implies torsion free}
	If $R$ is a domain and $M$ is a flat $R$-module, then $M$ is torsion free.
\end{lem}

\begin{proof}
	For $r\in R\smallsetminus 0$, consider the short exact sequence
	\[ 0 \to R \xra{r} R \to R/rR \to 0\]
	and tensor with $M$ to get
	\[ 0 \to M \xra{r} M \to M/rM \to 0\]
	which is exact by flatness. Injectivity of the map of multiplication by $r$ on $M$ means that no element of $M$ is killed by $r$.
\end{proof}

In general, the converse does not hold.

\begin{ex}
	Let $k$ be a field and $R = k[x,y]$. Consider the ideal $\fm = (x,y)$. This is a submodule of the torsion free module $R$, and thus torsion free. However, it is not flat. For example, when we apply $R/\fm \otimes_R -$ to the inclusion	$\fm \subseteq R$ we obtain a map of $R/\fm$-vector spaces
	$$\xymatrix{\fm/\fm^2 \ar[r] & R/\fm}.$$
	This map cannot possibly be injective: $\fm / \fm^2$ is a $2$-dimensional $R/\fm$-vector space, and $R/\fm$ is $1$-dimensional.
\end{ex}

We can test whether a given module if flat by looking at the finitely generated submodules.

\begin{thm}
	If every finitely generated submodule of $N$ is flat, then $N$ is flat.
\end{thm}


\begin{proof}
	Let $i\!: A \longrightarrow B$ be an injective map of $R^\op$-modules. We want to show that
	\[A \otimes_R N \xra{i \,\otimes \,1_N}  B \otimes_R N\]
	is injective. Suppose that $u \in \ker (i \otimes 1_N)$. We are going to construct a finitely generated submodule $j\!: M \subseteq N$ and an element $v \in A \otimes_R M$ such that $v \in \ker(i \otimes 1_M)$ and $u = ( 1_A \otimes j)(v)$. Once we do that, our assumption will say that $ i \otimes 1_M$ is injective, so $v = 0$ and thus $u = 0$. This will show that $i \otimes 1_N$ is also injective, and $N$ is flat.
	
	Note that even if a combination of simple tensors in $B\otimes_R N$ makes sense as an element in $B\otimes_R N'$ for some submodule $N'\subseteq N$ (i.e., each $N$ coefficient is in $N'$) and is zero in $B\otimes_R N$, it may not be zero in $B\otimes_R N'$, since the map from $B\otimes_R N'$ to $B\otimes_R N$ may not be injective.
	
	Let's say that $u = a_1 \otimes n_1 + \cdots + a_t \otimes n_t$. Consider the tensor product $B \otimes_R N$ as a quotient of the free abelian group with generating set consisting of elements of $B \times N$ by the submodule $Y$ with all the necessary relations we need to impose. This gives us a short exact sequence
	$$\xymatrix{0 \ar[r] & Y \ar[r] & F \ar[r]^-\pi & B \otimes_R N \ar[r] & 0.}$$
	The fact that $i(a_1) \otimes  n_1 + \cdots + i(a_t) \otimes n_t= 0$ means we can rewrite this element as $\pi(y)$ for some $y \in Y$. This element $y$ is a $\Z$-linear combination of elements of finitely many $(b,n) \in B \times N$. Let $m_1, \ldots, m_s$ be all the $N$-coordinates of those elements.

	Now we take $M$ to be the finitely generated submodule of $N$ generated by $n_1, \ldots, n_t$ and $m_1, \ldots, m_s$, and $v = a_1 \otimes n_1 + \cdots + a_t \otimes n_t \in A \otimes_R M$. Now
	$$(1_A \otimes j)(a_1 \otimes n_1 + \cdots + a_t \otimes n_t) = a_1 \otimes n_1 + \cdots + a_t \otimes n_t \in A \otimes_R N,$$
	and
	$$(i \otimes 1_M)(a_1 \otimes n_1 + \cdots + a_t \otimes n_t) = i(a_1) \otimes n_1 + \cdots + i(a_t) \otimes n_t = 0,$$ 
	as desired.
\end{proof}





The converse does over a PID.

\begin{lem}\label{pid flat iff torsion free}
	If $R$ is a principal ideal domain, an $R$-module $M$ is flat if and only if it is torsion free.
\end{lem}

\begin{proof}
	Suppose $M$ is a torsion free finitely generated $R$-module. The structure theorem for PIDs says that $M$ must be isomorphic to a direct sum of copies of cyclic modules. The cyclic module $R/I$ has torsion (all the elements are killed by $I$) unless $I = 0$. Therefore, $M$ must be isomorphic to a direct sum of copies of $R$, and thus free. Thus, $M$ is flat.
	
	Now let $M$ be any torsion free $R$-module. All of the finitely generated submodules of $R$ are also torsion free, and thus flat.
\end{proof}


\end{comment}


\sec{Simplicity, semisimplicity, and representation theory}

\ssec{Group rings and representations} We will take a brief aside to discuss an important class of examples of modules. 

\sssec{Representations}



\begin{defn} Let $G$ be a group. A \DEF{representation} of $G$ over a field $K$ is a $K$-vector space $V$ equipped with a group homomorphism $\rho:G\to \Aut_K(V)$. More generally, a representation of $G$ over a ring $R$ is an $R$-module $V$ equipped with group homomorphism $\rho:G\to \Aut_R(V)$. We may also say that $G$ \DEF{acts linearly} on $V$.
\end{defn}

One often simply says that $V$ is a representation of $G$ if the homomorphism $\rho$ is understood.

\begin{rem} We can think of this data in a number of different ways. 
\begin{enumerate}
\item Given a representation $(V,\rho)$, the map 
\[\xymatrix@R=.4em{ G \times  V \ar[r] & V \\
(g,m) \ar[r] & g\cdot v :=\rho(g)(v)}\]
satisfies the properties
\begin{enumerate}
\item $e \cdot v= v$
\item $gh \cdot v = g \cdot (h \cdot v)$
\item $g \cdot (v+w)=(g \cdot v) + (g \cdot w)$
\item $g \cdot rv = r (g\cdot v)$,
\end{enumerate}
In particular, the first two conditions say that $G$ acts on $V$ in the sense of group action on a set, and the last two say that the action of any element is by an $R$-linear map. Conversely, any such function $\psi$ yields a representation $(V,\rho)$.

\item If $V=R^n$ is free, then $\Aut_R(V) \cong \GL_n(R)$\index{$\GL_n(R)$}, where $\GL_n(R)$ is the group of $n\times n$ invertible matrices with entries in $R$. By a slight abuse of notation, we will say that a group homomorphism $G\to \GL_n(V)$ is a representation of $G$.
\end{enumerate}
\end{rem}

\begin{ex}
\begin{enumerate}
\item For any group $G$, and any $R$-module $V$, there is the \DEF{trivial representation} $\rho:G\to \Aut_R(V)$ where $\rho(g)=1_V$ for all $g\in G$. In this action, every element acts trivially on $M$.
\item Any representation on  $V = R$  is determined by specifying a group homomorphism $\rho:G \to \Aut_R(R)\cong R^\times$. 

For example, if $G = C_n=\langle g\rangle$ (the multiplicative cyclic group of order $n$)  and $R = \C$, there are $n$ possible such homomorphisms, determined by  $\rho(g)=e^\frac{2\pi k i}{n}$ where $0 \leq k \leq n-1$.

Another important example of a rank 1 representation is the \DEF{sign representation} of the symmetric group $S_n$, given by the group homomorphism which assigns to each permutation its sign, regarded as an element of the arbitrary ring $R$.

\item The symmetric group $S_n$ acts on a free $R$-module with basis $b_1,\dots,b_n$ by permuting coordinates: $\rho(\sigma)(b_i) = b_{\sigma(i)}$. For a concrete example, $S_3$ acts on $\R^3$, where, for example $(132) \cdot (a_1,a_2,a_3) = (a_2,a_3,a_1)$.


\item Let $G = D_{2n}$, symmetries of the equilateral polygon on $n$ vertices.    Then $G$ acts linearly on $V=\R^2$ by rotations and reflections.
    If $G$ is generated by $r$ (rotation by $2\pi/n$) and $l$ (reflection about the $y$-axis), then
    the associated group homomorphism $\rho: G \to\GL_2(\R)$ maps
         \[\rho(r)=\begin{bmatrix}
      \cos(2 \pi/n) & -\sin(2 \pi/n)  \\ \sin(2 \pi/n)  & \cos(2 \pi/n)  \end{bmatrix}
      \qquad
\rho(l)=\begin{bmatrix}
      -1  & 0 \\ 0 & 1 \end{bmatrix}.
      \]
      
  \item Let $R=K$ be a field,$V =K^2$, and let $G=(K,+)$. We see that the assignment 
\[
\rho:G\to  \GL_2(K) \quad 
\rho(\lambda)=\begin{bmatrix} 
1 & 0  \\ \lambda & 1
\end{bmatrix}
\]
is a representation. In particular, if $K=\F_p$, this is a representation of $C_p$. \end{enumerate}\end{ex}

\begin{defn} If $\rho:G \to \Aut_R(V)$ and $\omega:G\to \Aut_R(W)$ are $R$-linear representations of $G$ on $V$ and $W$ respectively then a \DEF{$G$-equivariant map} from $V$ to $W$ is an $R$-module homomorphism $f: V \to W$ such that $f(gv)=gf(v)$ for all $v\in V$. Equivalently the following diagram commutes:
\[
\xymatrix{
V \ar[r]^f \ar[d]_{\phi(g)}& W \ar[d]^{\psi(g)}\\
V \ar[r]^f & W
}
\]
\end{defn}

\begin{defn} If $\rho:G \to \Aut_R(V)$ is a representation, a submodule $W\leq V$ is \DEF{$G$-stable} if $\rho(g)(W)\subseteq W$ for all $g\in G$.
\end{defn}

\begin{ex}
For $G=S_n$ acting by permuting a basis as above, $\{(\lambda,\dots,\lambda) \ | \ \lambda\in K\}$ and \[\{(\lambda_1,\dots,\lambda_n) \ | \ \lambda_1+\cdots+\lambda_n=0\}\] are stable subspaces.
\end{ex}

\begin{ex}
For $G=(K,+)$ acting on $K^2$ as above, $\{(0,\lambda) \ | \ \lambda\in K\}$ is a stable subspace.
\end{ex}

\begin{prop}
Fix a group $G$ and a ring $R$. The collection of left $R$-linear representations of $G$ and $G$-equivariant maps between them forms a category which we will denote $\Rep{R}{G}$.
\end{prop}

%\begin{comment}
\Oct{13}

\sssec{Group rings and modules}


\begin{defn}
For any ring $R$ and group $G$,   we define the  {\em group ring} $R[G]$ as follows: 
As a set, $R[G]$ is the free left $R$-module with basis $G$; that is, 
$$
R[G]=\left\{ \sum_g r_g g \mid  r_g = 0_R \text{ for all by a finite number of }g's \right\}.
$$
We define addition as module addition; that is,
  $$
  \left(\sum_g r_g g \right) + \left(\sum_h s_h h \right) = \sum_{f \in G} \left(r_f + s_f \right) f.
  $$
  Multiplication is the unique pairing that obeys the distributive laws
  and is such that $R$ is a subring, $1_RG$ is a subgroup of $(R[G]^\times, \cdot)$, and every element of $R$ commutes with every element of $G$.
  In general, we have
  $$
  \left(\sum_g r_g g\right) \cdot \left(\sum_h s_h h \right) = \sum_{f \in G} \left(\sum_{\substack{(g,h) \in G \times G\\ gh= f}} r_gs_h\right)   f.
  $$
where the inner sum is over pairs of group elements whose product is $f$. 
\end{defn}

\begin{rem}
As a matter of notation,
the element $1_R g$ will be written as just $g$ and the element $r e_G$ as just $r$,
so that we will regard $G$ and $R$ as subsets of $R[G]$. They overlap in the one element $1_R e_G$ which will be written as just $1$.
\end{rem}

\begin{rem}
When $R$ is commutative (in particular when $R$ is a field), $R[G]$ is an $R$-algebra called the {\em group $R$-algebra} of $G$.\end{rem}

\begin{exer} For any ring $R$ and $G = C_n$, prove there is a ring isomorphism 
\[R[C_n] \cong R[x]/(x^n -1).\] 
\end{exer}

%\begin{exer}  For any $R$ and group $G$, show there is an isomorphism $R[G]^{op} \cong R^{op}[G]$. (Hint: Send $g$ to $g^{-1}$.)
%\end{exer}

\begin{prop}[Universal Mapping Property of group rings]
\label{prop815}
  Let $R, A$ be rings and $G$ a group.
  Given a ring homomorphism $\iota: R \to A$ and a group
  homomorphism $f: G \to (A^\times, \cdot)$, such that for every $r\in R, g\in G$ we have that $\iota(r)$ and $f(g)$ commute in $(A,\cdot)$,
there is a unique ring homomorphism $\a: R[G] \to A$ such that $\a|_R = \iota$
  and $\a|_G = f$. Explicitly, $\a$ is given by
   $$
  \a\left(\sum_g r_g g\right) = \sum_g \iota(r_g) f(g).
  $$
\end{prop}

\begin{proof} Most of this follows from noticing that $R[G]$ is a coproduct. Indeed, we can vie $R[G]$ as an internal direct sum $R[G]=\bigoplus_{g\in G} Rg$ and hence it is the coproduct for the family $\{Rg\}_{g\in G}$ where each $Rg\cong R$. For each $g\in G$ set up an $R$-module homomorphism $f_g: Rg\to A$ by mapping $f_g(r_gg)=\iota(r_g)f(g)$. Then the definition of coproduct gives a unique $R$-module homomorphism 
\[
\a: R[G]=\bigoplus_{g\in G} Rg \to A \text{ such that } \a|_{Rg}=f_g.
\]
From the way we defined the maps $f_g$ we can deduce that $\a|_R = \iota$
  and $\a|_G = f$ and
   $$
  \a\left(\sum_g r_g g\right) = \sum_g \iota(r_g) f(g).
  $$
  It remains to check that this map is in fact a ring homomorphism, i.e. it preserves multiplication. This can be done using the formula for $\alpha$ above and the fact that $\iota(R)$ and $f(G)$ commute in $A$.
\end{proof}

\begin{rem}
If we assumed that $A$ is an $R$-algebra in the proposition above, then we would not need the commutativity condition as $\iota(R)$ is in the center of $A$ so it commutes with everything.
\end{rem}


\begin{lem} 
\label{lem:repsmods}
Let $R$ be a ring, $V$ a left $R$-module, and $G$ a group.
There is a bijection
$$\xymatrix@R=.4em{ \left\{{\begin{array}{c} \textrm{$R$-linear representations} \\ \textrm{of $G$ on $V$} \end{array} }\ \right\} \ar@{<->}[r] & \left\{ { \begin{array}{c}  R[G]\text{-module structures on $V$} \\ \text{(extending given action of $R$)} \end{array} } \right\} }.
$$

Moreover, if $V$ and $W$ are representations, then $\psi:V\to W$ is $G$-equivariant if and only if it is $R[G]$-linear.
\end{lem}
\begin{proof}
Given an $R[G]$-module structure on $V$, for every $g\in G$, there is a map $m_g: V\to V$ given by $v\mapsto g\cdot v$. We have $m_g  (rv) = g (rv) = rg (v) = r m_g(v)$, so $m_g$ is $R$-linear. Moreover, the map $\rho:G\to \End_R(V)$ that sends $g\mapsto m_g$ preserves multiplication and identity: $\rho(gh)(v) = gh v = g(hv) = \rho(g) \rho(h) (v)$ and $\rho(e)(v) = v$. Thus, we obtain an $R$-linear representation $\rho:G\to \Aut_R(V)$.


Conversely, recall that a module structure on an abelian group is equivalent to a ring homomorphism to its endomorphism ring over $\Z$. Given a representation $\rho:G\to \Aut_R(V)$ by considering $\Aut_R(V)\subseteq \End_\Z(V)$ we get a group homomorphism $f$ to the unit subgroup of $\End_\Z(V)$. The action of $R$ on $V$ gives a ring homomorphism $\iota: R \to \End_\Z(V)$. For $r\in R$ and $g\in G$, we have 
\[ (f(g) \circ \iota(r))(v) = f(g)(rv) = \rho(g)(rv) = r \rho(g)(v) = (\iota(r) \circ f(g))(v)\]
for all $v\in V$. Thus, by the universal property, we get a well-defined ring homomorphism $R[G] \to \End_\Z(V)$, and hence an $R[G]$-module structure, which is easily seen to follow the formula above.

We leave the final claim as an exercise.
\end{proof}

\begin{rem}
We can think of these bijections as yielding mutually inverse functors $F:\Rep{R}{G} \to \Mod{R[G]}$ and $F^{-1}:\Mod{R[G]} \to \Rep{R}{G}$.
\end{rem}

\ssec{Simple modules and finite length modules}

\sssec{Simple modules}

Now we proceed to discuss some smallness conditions on modules. The first key notion is that of a simple module. Simple modules are the atoms in module theory.

\begin{defn} An $R$-module $M$ is \DEF{simple} if there are no nonzero proper submodules of $M$.
\end{defn}

\begin{lem} Let $M$ be a nonzero $R$-module. The following are equivalent:
\begin{enumerate}
\item $M$ is simple
\item $Rm= M$ for all $m\in M\smallsetminus 0$
\item $M\cong R/I$ for some maximal left ideal $I$.
\end{enumerate}
\end{lem}
\begin{proof}
If $M$ is simple, and $m\neq 0$, then $0\neq Rm \subseteq M$ implies $Rm=M$, so (1) implies (2). Conversely, if  $0 \neq N\subsetneqq M$, and $m\in N$ is nonzero, then $Rm$ is nonzero and contained in $N$, hence not equal to $M$, so (2) implies (1).

For a left  ideal $I$, the submodules of $R/I$ are in bijective correspondence with the left $R$-submodules of $R$ that contain $I$, i.e., the left ideals that contain $I$. It is then clear that if $I$ is a maximal left ideal, then $R/I$ is simple, so (3) implies (1). 
On the other hand, if $M$ is simple then it is cyclic (since (1) implies (2)), so $M\cong R/I$ for some left ideal $I$, and if $I \subsetneqq J$ for some proper left ideal $J$, then $0 \neq J/I \subsetneqq R/I$; thus (1) implies (3).
\end{proof}

\begin{ex}
\begin{enumerate}
\item If $K$ is a field, a $K$-vector space is simple if and only if it is $1$-dimensional. Moreover, if $R$ is a $K$-algebra, then any $R$-module that is $1$-dimensional as a vector space is a simple $R$-module as well.

\item If $R$ is commutative, then an $R$-module $M$ is simple if and only if $M$ is isomorphic to a field.

\item Let $R=\R[D_{2n}]$, and $V$ be the natural $2$-dimensional representation by reflections and rotations. Then $V$ is a simple $R$-module, since there are no $D_{2n}$-stable subspaces.

\item Let $K$ be a field, or more generally a division ring, and let $R=M_n(K) \cong \End_K(K^n)$. The module $M=K^n$ of column vectors is a simple $R$-module
Indeed, if $v=(a_1,\dots,a_n)\neq 0$, say $a_i\neq 0$; then $a_i^{-1} E_{ij} v = e_i \in M$, and since $M$ is generated by the standard vectors $e_i$, $M = Rv$.
\end{enumerate}
\end{ex}



\begin{lem}[Schur's Lemma] Let $R$ be a ring, and $M,N$ be two simple $R$-modules. Then every nonzero $R$-module homomorphism $\phi:M\to N$ is an isomorphism. In particular, $\End_R(M)$ is a division ring.
\end{lem}
\begin{proof} For the first assertion, let $f:M\to N$ be $R$-linear and nonzero. Then $\ker(f)\neq M$, so $\ker(f)=0$ by simplicity, and $\im(f) \neq 0$, so $\im(f) = N$.

For the second, recall that $\End_R(M)$ is a ring. If $f\in \End_R(M)$ is nonzero, then by the first part, it is an isomorphism, so it has a two-sided inverse in $\End_R(M)$.
\end{proof}


\sssec{Finite length modules}

Given a short exact sequence 
\[ 0 \to A \to B \to C \to 0\]
we may think of the middle module $B$ as built out of $A$ and $C$; we call $B$ an \DEF{extension} of $A$ and $C$. 
Suppose that a module has a  finite sequence of submodules
\[ 0 = M_0 \subseteq M_1 \subseteq M_2 \subseteq \cdots \subseteq M_n = M\]
we call such a sequence a \DEF{filtration}. Then $M_1$ is an extension of $M_0$ and $M_1/M_0$, $M_2$ is an extension of $M_1=M_1/M_0$ and $M_2/M_1$, and so on. We might think of $M$ as built from $M_1/M_0, M_2/M_1,\dots,M_n/M_{n-1}$ like so.


A module has finite length if it can be built from finitely many simple modules in this way.

\begin{defn}
	A module $M$ has \DEF{finite length} if it has a filtration of the form
	\[ 0 = M_0 \subseteq M_1 \subseteq M_2 \subseteq \cdots \subseteq M_n = M\]
	with $M_{i+1}/M_i$ simple for each $i$; such a filtration is called a \DEF{composition series} of \emph{length} $n$. We say a composition series is \DEF{strict} if $M_i \neq M_{i+1}$ for all $i$. Two composition series are \DEF{equivalent} if the collections of composition factors $M_{i+1}/M_i$ are the same up to reordering. The \DEF{length} of a finite length module $M$, denoted \Def{$\ell(M)$}, is the minimum of the lengths of a  composition series~of~$M$. If $M$ has does not have finite length, we say that $M$ has infinite length, or $\ell(M) = \infty$.
\end{defn}

\begin{ex}
Let $K$ be a field and $V=K^2$. Then any filtration of the form $0 \subseteq W \subseteq V$ where $W$ is a line through the origin is a strict composition series.
\end{ex}

\begin{rem} Let \[ 0 \to A \xra{i} B \xra{p} C\to 0\]
be a short exact sequence. Given filtrations / composition series / strict composition series
\[ A_\bullet: \qquad 0 = A_0 \subseteq A_1 \subseteq A_2 \subseteq \cdots \subseteq A_n = A\]
and
\[ C_\bullet: \qquad 0 = C_0 \subseteq C_1 \subseteq C_2 \subseteq \cdots \subseteq C_n = C\]
we can make a filtration / composition series / strict composition series of $B$ by
\[ 0 = i(A_0) \subseteq i(A_1) \subseteq i(A_2) \subseteq \cdots \subseteq i(A_n) = i(A) = p^{-1}(C_0) \subseteq p^{-1}(C_1) \subseteq p^{-1}(C_2) \subseteq \cdots \subseteq p^{-1}(C_n) = B.\]

Conversely, given a filtration / composition series / strict composition series of $B$ that contains $i(L)$ as a term, we can obtain filtrations / composition series / strict composition series of $A$ and $C$ by applying $i^{-1}$ to the terms up through $i(L)$ and applying $p$ to the terms from $i(L)$ on. However, not every filtration / composition series of a module will contain a fixed submodule as a term.
\end{rem}


\begin{thm}[Jordan-Holder theorem]
Let $M$ be a module of finite length.
\begin{enumerate}
		\item If $L\subseteq M$ is a proper submodule, then $\ell(L) < \ell(M)$.
		\item If $L\subseteq M$ is a nonzero submodule and $\overline{M}=M/L$, then $\ell(\overline{M})< \ell(M)$.
		\item Any filtration of $M$ can be refined to a composition series.
%	\item Any two composition series of $M$ admit refinements that are equivalent.
	\item All strict composition series for $M$ are equivalent, and hence have the same length.
\end{enumerate}
\end{thm}


\begin{proof}
If $m := \ell(M)$, consider a strict composition series of $M$ of length $m$, say
	\[M_\bullet: \qquad  0 = M_0 \subsetneqq M_1 \subsetneqq M_2 \subsetneqq \cdots \subsetneqq M_m = M.\]
	
\begin{enumerate}
	\item Consider the filtration
	$$L_\bullet: \qquad  0 = M_0 \cap L \subseteq M_1 \cap L \subseteq M_2 \cap L \subseteq \cdots \subseteq M_m \cap L = L.$$
	By the Second Isomorphism Theorem, its composition factors satisfy
	$$\frac{M_{i+1} \cap L}{M_i \cap L} = \frac{M_{i+1} \cap L}{(M_{i+1} \cap L) \cap M_i} \cong \frac{M_{i+1} \cap L + M_i} {M_i}.$$
	The right hand side is a submodule of $M_{i+1}/M_i$, which by assumption is simple, so our filtration is in fact a composition series of length $n$. Then for any $i$ either 
	$$\frac{M_{i+1} \cap L}{M_i \cap L} = 0 \quad \textrm{ or } \quad \frac{M_{i+1} \cap L}{M_i \cap L}\cong \frac{M_{i+1}}{M_i}.$$
	We claim that the latter case does not hold for all $i$: if it did, we would have $0=M_0 = M_0 \cap L$, and inductively $M_{i+1} \cap L = M_{i+1}$ for all $i$ and in particular for $i=m-1$, we have $M = M\cap L$, contradicting that $L$ is proper. Thus, for some $i$, the first case holds. We can then skip that $i$ and obtain a composition series of length less than $n$, so $\ell(L)<m$.	
	

	\Oct{20}
	
	\item Consider the filtration
	\[ \overline{M}_\bullet: \qquad 0 = \frac{M_0 + L }{L} \subseteq \frac{M_1 + L }{L} \subseteq \cdots \subseteq \frac{M_n + L }{L} = \overline{M}.\]
	The factors satisfy
	\[ \frac{(M_{i+1} + L )/ L }{(M_i + L) / L} \cong \frac{M_{i+1} + L}{M_{i} + L} \cong \frac{M_{i+1} + (M_i+L)}{M_i+L}\cong \frac{M_{i+1} }{M_{i+1} \cap( M_i + L)},\] and since $M_i \subseteq M_{i+1} \cap (M_i+L)$, these are quotient modules of the simple module $M_{i+1}/M_i$, so this is a composition series. Then for any $i$ either 
	$$\frac{(M_{i+1} + L )/ L }{(M_i + L) / L}  = 0 \quad \textrm{ or } \quad \frac{M_{i+1} }{M_{i+1} \cap( M_i + L)}  \cong  \frac{M_{i+1}}{M_i}.$$

	We claim that the latter case does not hold for all $i$: if it did, we would have then $M_{i+1} \cap (M_i+L) = M_{i}$ for all $i$, so \[ \frac{M_{i+1} + L}{M_{i+1}} \cong \frac{L+M_i}{(L+M_i) \cap M_{i+1}} =\frac{L+M_i}{M_i}\]
	for all $i$, and hence $L \cong (L+M_0)/M_0 \cong (L+M_n)/M_n \cong 0$, contradicting that $L\neq 0$. Thus, for some $i$, the first case holds, and we can skip that $i$ to obtain a composition series of length less than $n$, so $\ell(\overline{M})<m$.
	
		 
	\item We proceed by induction on length again. Given a filtration of $M$, we can suppose that there is some nonzero proper submodule $L$ in the filtration, since otherwise we could just take any composition series. Then $L$ and $\overline{M}$ has length less than $M$. The filtration up to $L$ can be refined to a strict composition series by the induction hypothesis, and the filtration from $L$ to $M$ taken mod $L$ can be refined to a strict composition series for $\overline{M}$; pulling back as in the remark above, we get the strict composition series we want.
	
	
	\item We show by induction on $m$ that for any module of length $m$, all of its strict composition series are equivalent.
	Assume that $\ell(M)=m$. If $m=1$, the claim is clear since we are dealing with a simple module.
	Suppose that
	$$N_\bullet: \qquad 0 = N_0 \subsetneqq N_1 \subsetneqq \cdots \subsetneqq N_n = M$$
	is another strict composition series for $M$, so $n\geq m$. If $N_{n-1}=M_{m-1}$, then since $\ell(M_{m-1})\leq m-1$ the two composition series we have for $M_{m-1}$ are equivalent by induction, so the two given series are equivalent. 
	
	If $N_{n-1} \neq M_{m-1}$, since $M/M_{m-1}$ is simple, $M_{m-1}$ is not properly contained in $N_{n-1}$, so the image of $M_{m-1}$ in $M/N_{n-1}$ is nonzero, so equals all of $M$, which means that  $N_{n-1}+M_{m-1} = M$. Set $K=N_{n-1}\cap M_{m-1}$. By the second isomorphism theorem, we then have
	\[\frac{M}{M_{m-1}} = \frac{M_{m-1}+N_{n-1}}{M_{m-1}} \cong \frac{N_{n-1}}{K}\]
	and similarly $M/N_{n-1} \cong M_{m-1} / K$, and both of these modules are simple.
	
	
	Fix a strict composition series for $K$:
$$K_{\bullet}: \qquad 0 = K_0 \subsetneqq K_1 \subsetneqq \cdots \subsetneqq K_k = K$$
and extend to a strict composition series for $M_{m-1}$:
$$K'_{\bullet}: \qquad 0 = K_0 \subsetneqq K_1 \subsetneqq \cdots \subsetneqq K_k = K \subsetneqq M_{m-1}.$$
Since we also have the strict composition series 
$$M_{\bullet \leq m-1} : \qquad 0 = M_0 \subsetneqq M_1 \subsetneqq M_2 \subsetneqq \cdots \subsetneqq M_{m-1}$$
of length $m-1$, we must have that $k=m-2$ and $K'_{\bullet}$ is equivalent to $M_{\bullet \leq n-1}$. Thus, the composition factors of $M_{\bullet \leq m-1}$ are those of $K$ plus one copy of $M_{m-1} / K \cong M/N_{n-1}$.

 Now, 
$$K''_{\bullet}: \qquad  0 = K_0 \subsetneqq K_1 \subsetneqq \cdots \subsetneqq K_{m-2} = K \subsetneqq N_{n-1}$$
is a strict composition series for $N_{n-1}$, so $n=m$. Then, $K''_{\bullet}$ is equivalent to the strict composition series 
	$$N_{\bullet \leq n-1}: \qquad 0 = N_0 \subsetneqq N_1 \subsetneqq \cdots \subsetneqq N_{n-1}.$$
Thus, the composition factors of $N_{\bullet \leq n-1}$ are those of $K$ plus one copy of $N_{n-1} / K \cong M/M_{n-1}$.

It follows that the composition series $M_\bullet$ and $N_\bullet$ are equivalent.	\qedhere
	\end{enumerate}
\end{proof}

\begin{ex}
\begin{enumerate}
\item If $K$ is a field, then a $K$-vector space of dimension $n$ is a $K$-module of length $n$.
\item If $R$ is a $K$-algebra, and $M$ is an $R$-module that as a $K$-vector space has dimension $n$, then $\ell(M)\leq n$, since the vector space dimension of a proper submodule is strictly smaller.
\item The ring $R=K[x]$ does not have finite length as a module over itself.
\item $\Z/p^n$ has length $n$ as a $\Z$-module, with strict composition series \[ 0 \subseteq \langle \overline{p^{n-1}} \rangle \subseteq \cdots \subseteq \langle \overline{p} \rangle \subseteq \Z/p^n.\]
\end{enumerate}


\ssec{Chain conditions}

\begin{defn}
We say a poset $(P,\leq)$ satisfies the \DEF{ascending chain condition} or \DEF{ACC} if every totally ordered nonempty subset of $P$ has a maximum element. Similarly,  $(P,\leq)$ satisfies the \DEF{descending chain condition} or \DEF{DCC} if every totally ordered nonempty subset of $P$ has a minimum element.
\end{defn}

\begin{rem} For a poset $(P,\leq)$, the following are equivalent:
\begin{enumerate}
\item Every totally ordered nonempty subset has a maximum element (i.e., $P$ has ACC)
\item Every totally ordered subset indexed by $\N$, $p_1 \leq p_2 \leq p_3 \leq \cdots$ has a maximum element (i.e., $\exists k: p_k=p_{k+1} = \cdots$)
\item Every nonempty subset of $P$ has a maximum element.
\end{enumerate}
Indeed, (3) $\Rightarrow$ (1) $\Rightarrow$ (2) is clear. Given a totally ordered nonempty subset with no maximum, one can inductively keep choosing larger elements and obtain a countable such subset, so (2) $\Rightarrow$ (1). If any totally ordered nonempty subset of $P$ has a maximum element, then the same property holds for any nonempty subset $Q$ of $P$, so by Zorn's Lemma, such a $Q$ has a maximum element. The analogous equivalences hold with DCC.\end{rem}

Note that the condition (3) asserts that any nonempty subset of $P$ has an element that is maximal \emph{within the subset}, not maximal \emph{within $P$}.

\begin{defn} Let $R$ be a ring and $M$ be an $R$-module.
\begin{enumerate}
\item We say that $M$ is \DEF{Noetherian} if the poset of submodules of $M$ partially ordered by containment has ACC.
\item We say that $M$ is \DEF{Artinian} if the poset of submodules of $M$ partially ordered by containment has DCC.
\item We say that $R$ is \DEF{left Noetherian} if $R$ is Noetherian as a left $R$-module; i.e., the poset of left ideals of $R$ under containment has ACC.
\item We say that $R$ is \DEF{left Artinian} if $R$ is Artinian as a left $R$-module; i.e., the poset of left ideals of $R$ under containment has DCC.
\end{enumerate}
\end{defn}

\begin{comment}

If $R$ is commutative, left ideals and right ideals are the same, so we will jsut say $R$ is Noetherian or Artinian.

\begin{ex} Let $K$ be a field, and $R=K[x]$ be a polynomial ring in one variable. Then $R$ is Noetherian: $R$ is a PID, so any ideal $I$ is determined by a collection of irreducible factors with multiplicities of a generator of $I$. Moreover, $I \subseteq J$ if and only if a generator of $J$ has as its irreducible factors a subset of those of $I$, and with smaller or equal multiplicities. It is clear that any increasing chain must stabilize. On the other hand, $R$ is not Artinian: we have a descending chain
\[ (x) \supsetneqq (x^2) \supsetneqq (x^3) \supsetneqq (x^4) \supsetneqq \cdots\]
that does not stabilize.
\end{ex}

\begin{exer}
Let $0 \to M' \to M \to M''\to0$ be a short exact sequence. Then $M$ has ACC (resp DCC) if and only if $M'$ and $M''$ have ACC (resp. DCC).
\end{exer}

The Noetherian condition is intimately tied to finite generation.

\begin{prop} Let $M$ be an $R$-module. Then $M$ has ACC if and only if every submodule of $M$ is finitely generated.
\end{prop}
\begin{proof}
Suppose $M$ has ACC. Let $P$ be the set of finitely generated submodules of $M$ partially ordered by inclusion. Clearly $P$ is nonempty (since it contains $R m \subseteq M$ for any $m\in M$, for example), so by hypothesis, $P$ has a maximal element $N$. We claim that $N=M$; otherwise, if $m\in M\smallsetminus N$, then $N \subsetneqq N+Rm$, and $N+Rm$ is finitely generated (by the finite generating set of $N$ union $\{m\}$), which contradicts maximality of $N$ in $P$.

Now suppose that every submodule of $M$ is finitely generated. Given a countable ascending chain of submodules
\[ M_1 \subseteq M_2 \subseteq M_3 \subseteq M_4 \subseteq \cdots\]
let $N=\bigcup _{n\in \N} M_n$; this is a submodule of $M$. Take a finite generating set $\{n_1,\dots,n_t\}$ for $N$. For each $i=1,\dots,t$, we have $n_i \in M_j$ for some $j$. Since there are finitely many $n_i$'s there is some $M_j$ that contains them all. But then $M_j=N$, so the chain stabilizes (i.e., achieves a maximum element).
\end{proof}




\begin{prop} Let $R$ be left Noetherian. Then a module is finitely generated if and only if it is left Noetherian. In particular, in a left Noetherian ring, every submodule of a finitely generated module is finitely generated.
\end{prop}
\begin{proof} For the first statement, the ``if'' implication holds in general without the hypothesis on $R$. For the other implication, observe that there are short exact sequences
\[ 0 \to R^{n-1} \to R^{n} \to R \to 0 \]
for all $n>0$. So, by the exercise above and induction on $n$, every finitely generated free module is Noetherian. Now, if $M$ is finitely generated, there is a short exact sequence of the form
\[ 0 \to K \to R^{n} \to M \to 0\]
so by the exercise above again, $M$ is Noetherian.

The second statement follows from the first as a submodule of a Noetherian module is Noetherian, again by the exercise.
\end{proof}

Now we tie these chain conditions to length.

\begin{prop} A module $M$ has finite length if and only if it is both Noetherian and Artinian.
\end{prop}
\begin{proof}
Assume that $M$ has finite length. Suppose that $M$ is not Noetherian. Then there is a chain 
\[ M_0 \subsetneq M_1 \subsetneqq M_2 \subseteqq  \cdots\]
Since each $M_i$ is a submodule of $M$, its length is finite, and is a nonnegative integer. Then $\ell(M_0) < \ell(M_1) < \ell(M_2) < \cdots \leq \ell(M)$, which yields a contradiction. The argument that $M$ is Artinian is similar.

Now assume that $M$ is both Noetherian and Artinian. We will construct a composition series for $M$. We can assume that $M\neq 0$. Consider the collection of proper submodules of $M$. This is nonempty, so has a maximal element $M^1$ by the Noetherian hypothesis. We must have $M/M^1$ is simple, or else there is a module in between $M^1$ and $M$. Using Noetherianity again, if $M^1\neq 0$ (we're done otherwise), there is a maximal proper submodule of $M^1$; call it $M^2$. This process yields a descending chain with simple quotients, and this must stop (i.e., yield $M^i=0$ for some $i$) by the Artinian hypothesis. Thus, there is a composition series for $M$.
\end{proof}

\ssec{Semisimple modules}

We now study an important condition that is somewhat orthogonal (yet somewhat related) to our chain conditions. The condition of finite length, and to some extent the Noetherian and Artinian conditions, were related to how a module is made out of building blocks, or how big it is in terms of its pieces. The condition of semisimplicity says that a module is composed of basic building blocks in the simplest possible way.

\begin{defn} For any ring $R$, a left $R$-module $M$ is called \DEF{semisimple} if it is a (possibly infinite) direct sum of simple modules. The empty direct sum is allowed, so that the $0$ module {\em is} considered to be semisimple. %This is equivalent to the condition that $M$ is isomorphic to a (possibly infinite) internal direct sum of simple submodules.
\end{defn}


\begin{ex} Let $M$ be a finitely general $\Z$-module. Then by the FTFGAG, $M$ is isomorphic to
  $\Z^r \oplus \Z/p_1^{e_1} \oplus \cdots \oplus \Z/p_n^{e_n}$ for some $r \geq 0$, $n \geq 0$, primes $p_i$ and positive integers $e_i$.
  Such a module is semisimple if and only if $r = 0$ and $e_i = 1$ for all $i$.
\end{ex}





\begin{ex} Every module over a division ring $D$ is semisimple because any such module has a basis, hence it is a free module. %which gives that it is the internal direct sum of the one-dimensional subspaces spanned by the elements of any chosen  basis.
\end{ex}


\begin{lem} 
\label{lem:matrixsemisimple}
Let $D$ be a division ring and set $R = \Mat_n(D)$ for some $n \geq 1$. I claim $R$ is semisimple as a left module over itself. 
\end{lem}
  \begin{proof}
  For each $1 \leq i \leq n$, let $I_i$ denote the subset of $R$ consisting of matrices whose only non-zero entires belong to the $i$-th column.
The rules for matrix addition and multiplication show that $I_i$ is a left ideal (i.e., a left submodule) of $R$. Moreover, there is evident bijection between $I_i$ and $D^n$ (column vectors) and this bijection is an isomorphism of left $R$-modules. We proved $D^n$ is simple as an $R$-module  and hence so is $I_i$. Finally, $R$ is the internal direct sum of $I_1, \dots, I_n$:
\[
R=I_1\oplus \cdots \oplus I_n
\]
because 
 each matrix $X$ is uniquely a sum of the form $X_1 + \cdots + X_n$ with $X_i \in M_i$.
\end{proof}

\begin{prop}[Equivalent conditions for semisimple modules]
 \label{prop93b} 
For any ring $R$ and left $R$-module $M$, the following are equivalent:
  \begin{enumerate}
  \item $M$ is semisimple,
  \item every submodule of $M$ is a summand; i.e., for every submodule $N$ of $M$ there is a submodule $N'$ such that
    $M=N\oplus N'$ is the internal direct sum of $N$ and $N'$,
  \item every injective $R$-map $i: M' \to M$ is split has a left inverse,
  \item every s.e.s of the form $0 \to M' \to M \to M'' \to 0$ is split exact,
  \item every surjective $R$-map $p: M \to M''$ has a right inverse.
  \end{enumerate}
\end{prop}

\begin{proof} 
The equivalence of (3), (4), and (5) is given by the Splitting Theorem.

$(2) \Rightarrow (3)$ holds since given an injective map $i$ as in (3), we have by (2) that $i(M')$ is a summand of $M$, hence there is a projection homomorphism $\pi:M\to i(M')$ that splits the inclusion of the summand into $M$, that is $\pi|_{i(M')}=\id_{i(M')}$. Now $i:M'\to i(M')$ is an isomorphims so we may consider the $R$-module homomorphism $i^{-1}:i(M')\to M'$ and set $s:M\to M'$ to be $s=i^{-1}\circ \pi$. Then 
\[s\circ i=i^{-1}\circ \pi \circ i=i^{-1}\circ \pi_{i(M')} \circ i= i^{-1}\circ i=\id_{M'}.\]

$(3) \Rightarrow (2)$ holds since we can split the inclusion $N\to M$ and thus also the s.e.s.
\[
0\to N\to M\to M/N \to 0.
\]
Therefore the Splitting Theorem yields $M=N\oplus s(M/N)$ where $s$ denotes the splitting of the quotient map $M\to M/N$.

The hard part is proving $(1) \Leftrightarrow (2)$. 
 $(1) \Rightarrow (2)$ Assume (1), so that $M = \oplus_{i \in I} M_i$ for some collection of simple submodules $M_i$,  and let $N \subseteq M$ be any submodule. {\em It is important to note that it does not necessarily follow that $N$ is one of the $M_i$;} see Example \ref{ex:diagonalsimple}. 
  Consider the collection $\cS$ of subsets $J$ of $I$ such that $N \cap M_J = 0$ where we define $M_J := \oplus_{j \in J} M_j$. View $\cS$ as a poset by inclusion.
  It's non-empty since $J = \emptyset$ belongs to $\cS$. If $\{J_\alpha\}$ is a totally ordered sub-collection of $\cS$, let
  $J = \cup_\a J_\a$. I claim $M_J \cap N = 0$. If not, there is a non-zero element $(m_j) \in M_J \cap N$. But since $m_j = 0$ for all but a finite number of $j$'s
  and since the collection of $J_\a$'s was totally ordered, there is some $\a$ such that $(m_j) \in M_{J_\a} \cap N$, a contradiction.
  We may thus apply Zorn's Lemma to get a maximal $J \in \cS$.

  I claim $M$ is the internal direct sum of $N$ and $M_J$.
  We have $N \cap M_J = 0$ by construction and so  it suffices to prove $N + M_J = M$. Since $M = \sum_{i \in I} M_i$, the latter is equivalent to proving
  that $M_i \subseteq N + M_J$ for all $i \in I$. If this fails for some $i$, then since
$M_i \cap (N + M_J)$ is a proper submodule of $M_i$, which is simple, and hence 
$M_i \cap (N + M_J) = 0$. But then $N \cap M_{J'} = 0$ where $J' = J \cup \{i\} \supset J$. Indeed, if $n \in N$ and $n = m_i + \sum_{j \in J} m_j$,
then $m_i = n - \sum_j -m_j \in M_i \cap (N + M_J) = 0$. So, $J'$ is member of $\cS$ that strictly contains $J$, a contradiction.
It must be the $M = N \oplus M_J$.


$(2) \Rightarrow (1)$  Now assume that every submodule of $M$ is a summand. We proceed in three steps:

(i) I claim that every submodule $T$ of $M$  inherits this property; i.e., every submodule of $T$ is a summand of $T$.
For say $U \subseteq T$ is a submodule. By assumption on $M$, we have $M = U \oplus V$ (internal direct sum) for some $V$.  
Since $U \subseteq T$, it follows that  $T = U + (V \cap T)$. (Given $t \in T$, we have $t  =  u +v$ for some $u \in U, v \in V$. Since $U \subseteq T$, $v =
t-u \in V \cap T$.) Since $U \cap (V \cap T) = 0$, this shows $T = U \oplus (V \cap T)$. 


(ii) I claim that every non-zero submodule $T$ of $M$ contains a simple summand. Pick $0 \ne x \in T$ and apply Zorn's Lemma to show 
that there is a maximal submodule $U$ of $T$ with respect to the property that $x \notin U$. We have $T = U \oplus W$ by (i) for some  $W \neq 0$.
If $W$ is not
simple, then $W$ contains a nonzero, proper submodule $W_1$ and hence, by using (i) again, we get that 
$W = W_1 \oplus W_2$ for some proper non-zero submodule $W_2$. 


These properties implies that $(U \oplus W_1) \cap (U \oplus W_2) = U$. One containment is clear. If $v$ belongs to the left side,
then $v = u + w_1 = u' + w_2$. It follows that
$w_1 - w_2 = u - u' \in U \cap W = 0$ and so $w_1 = w_2 \in W_1 \cap W_2 = 0$, and hence $w_1  = w_2 = 0$. 
So, either $x \notin U \oplus W_1$ or $x \notin U \oplus W_2$, and either way we reach a  contradiction to the maximality of $U$. 

(iii) For this part we consider the collection of all  families  $\{S_j\}_{j \in J}$ of submodules of $M$ satisfy the two properties
\begin{itemize}
\item each $S_j$ is a simple submodule of $M$ and
\item  they form an internal direct sum of the submodule that they generate; i.e., $\sum_j S_i = \oplus_j S_j$. 
(This is equivalent to saying that for all $l \in J$ we have $S_l \cap \sum_{j \in J, j \ne l} S_j = 0$.)
\end{itemize}
Define an order relation on the collection of all such families by declaring $\{S_j\}_{j \in J} \leq  \{T_i\}_{i \in I}$ iff $J \subseteq I$
and $S_j = T_j$ for all $j \in J$.
Take my word for it that we may apply Zorn's Lemma to show that there is a member $\{S_j\}_{j \in J}$ of this collection  that is maximal.
Set $U = \oplus_j S_j = \sum_j S_j$. We need to prove $U = M$. By (i) we have $M = U \oplus V$ for some $V$. 
If $V = 0$ we are done. Otherwise by (ii) (and (i) again) we have $V = S \oplus V'$ for some simple submodule $S$. But then $\{S_j\}_{j \in J} \cup \{S\}$ is a
larger member of the collection, a contradiction.
\end{proof}


\begin{cor} \label{cor93}
If $M$  semisimple, so is every submodule and quotient module of $M$.
\end{cor}

\begin{proof} Say $N \subseteq M$ is a submodule. By the claim marked (i) in the proof of Proposition \ref{prop93b} every submodule of $N$ is a summand, and hence $N$ is semisimple by Proposition \ref{prop93b} $(2)\Rightarrow (1)$.
  
  Given a surjection $M \onto P$, it splits by Proposition \ref{prop93b}, so that $P$ is isomorphic to a submodule of $M$, namely the image of $P$ under the splitting map. Hence $P$ is semisimple by the case already proven.
\end{proof}



\begin{comment}


\end{comment}



\printindex































\end{document}







  
 


