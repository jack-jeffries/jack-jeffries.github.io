\documentclass[12pt]{amsart}
\usepackage{graphicx, framed}
\usepackage{comment}
\usepackage{amscd}
\usepackage{amssymb,xcolor}
\usepackage[all, knot]{xy}
%\usepackage[top=1.2in, bottom=1.2in, left=1.2in, right=1.2in]{geometry}
\xyoption{all}
\xyoption{arc}
\usepackage{hyperref}

%%\usepackage[notcite,notref]{showkeys}


\def\floor#1{\lfloor #1 \rfloor}








\def\ku{ku}
\def\bbu{\bf bu}
\def\KR{K{\mathbb R}}

\def\CW{\underline{CW}}
\def\cP{\mathcal P}
\def\cE{\mathcal E}
\def\cL{\mathcal L}
\def\cJ{\mathcal J}
\def\cJmor{\cJ^\mor}
\def\ctJ{\tilde{\mathcal J}}
\def\tPhi{\tilde{\Phi}}
\def\cA{\mathcal A}
\def\cB{\mathcal B}
\def\cC{\mathcal C}
\def\cZ{\mathcal Z}
\def\cD{\mathcal D}
\def\cF{\mathcal F}
\def\cG{\mathcal G}
\def\cO{\mathcal O}
\def\cI{\mathcal I}
\def\cS{\mathcal S}
\def\cT{\mathcal T}
\def\cM{\mathcal M}
\def\cN{\mathcal N}
\def\cMpc{{\mathcal M}_{pc}}
\def\cMpctf{{\mathcal M}_{pctf}}
\def\L{\mathbf{L}}





\def\d{\delta}
\def\td{\tilde{\delta}}

\def\e{\varepsilon}
\def\nsg{\unlhd}




\newcommand{\Q}{\mathbb{Q}}

\newcommand{\bP}{\mathbb{P}}
\newcommand{\bM}{\mathbb{M}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\bH}{{\mathbb{H}}}
\newcommand{\G}{\mathbb{G}}
\newcommand{\bR}{{\mathbb{R}}}
\newcommand{\bL}{{\mathbb{L}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bK}{\mathbb{K}}


\newcommand{\bD}{\mathbb{D}}
\newcommand{\bS}{\mathbb{S}}

\newcommand{\bN}{\mathbb{N}}


\newcommand{\bG}{\mathbb{G}}

\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\M}{\mathcal{M}}
\newcommand{\W}{\mathcal{W}}



\newcommand{\itilde}{\tilde{\imath}}
\newcommand{\jtilde}{\tilde{\jmath}}
\newcommand{\ihat}{\hat{\imath}}
\newcommand{\jhat}{\hat{\jmath}}

\newcommand{\fc}{{\mathfrak c}}
\newcommand{\fp}{{\mathfrak p}}
\newcommand{\fm}{{\mathfrak m}}
\newcommand{\fq}{{\mathfrak q}}
\newcommand{\dual}{\vee}


% The following causes equations to be numbered within sections
\numberwithin{equation}{section}


\theoremstyle{plain} %% This is the default, anyway
\newtheorem{thm}[equation]{Theorem}
\newtheorem{preproof}{Preproof Discussion}
\newtheorem{obs}[equation]{Observation}
\newtheorem{thmdef}[equation]{TheoremDefinition}
\newtheorem{introthm}{Theorem}
\newtheorem{introcor}[introthm]{Corollary}
\newtheorem*{introthm*}{Theorem}
%\newtheorem{question}{Question}
\newtheorem*{question}{Question}
\newtheorem{cor}[equation]{Corollary}
\newtheorem{lem}[equation]{Lemma}
\newtheorem{prop}[equation]{Proposition}
\newtheorem{porism}[equation]{Porism}
\newtheorem{algorithm}[equation]{Algorithm}
\newtheorem{axiom}[equation]{Axiom}
\newtheorem*{axioms*}{Axioms}
\newtheorem*{axiom*}{Axiom}
\newtheorem{conj}[equation]{Conjecture}
\newtheorem{quest}[equation]{Question}

\newcommand{\Aug}[1]{\section{August #1, 2022}}
\newcommand{\Sept}[1]{\section{September #1, 2022}}
\newcommand{\Oct}[1]{\section{October #1, 2022}}
\newcommand{\Nov}[1]{\section{November #1, 2022}}
\newcommand{\Dec}[1]{\section{December #1, 2022}}



\theoremstyle{definition}
\newtheorem{defn}[equation]{Definition}
\newtheorem{chunk}[equation]{}
\newtheorem{ex}[equation]{Example}

\newtheorem{exer}[equation]{Exercise}

\theoremstyle{remark}
\newtheorem{rem}[equation]{Remark}

\newtheorem{notation}[equation]{Notation}
\newtheorem{terminology}[equation]{Terminology}

%%%%%%%%%%%%%
% local definitions
%%%%%%%%%%%%%
\def\TP{\operatorname{TPC}}

\def\HomMF{\operatorname{\uHom_{MF}}}
\def\HomHMF{\operatorname{\Hom_{[MF]}}}

\def\rDsg{D^{\text{rel}}_{\text{sing}}}
\def\Dsg{D_{\text{sing}}}
\def\Dsing{\Dsg}
\def\Db{D^b}


\def\Perf{\operatorname{Perf}}
\def\coh{\operatorname{coh}}

\def\CCh{\check{\cC}}

\newcommand{\Cech}{C}

\newcommand\bs{\boldsymbol}


%\newcommand{\sExt}[4]{\widehat{\operatorname{Ext}}_{#2}^{#1}(#3,#4)}
\newcommand{\cExt}[4]{\uExt_{#2}^{#1}(#3,#4)}

\newcommand\depth{\operatorname{depth}}
\newcommand{\supp}{\operatorname{supp}}

\newcommand{\br}[1]{\lbrace \, #1 \, \rbrace}
\newcommand{\brc}[2]{\lbrace \, #1 \, | \, #2 \rbrace}
\newcommand{\li}{ < \infty}
\newcommand{\quis}{\simeq}
\newcommand{\xra}[1]{\xrightarrow{#1}}
\newcommand{\xla}[1]{\xleftarrow{#1}}
\newcommand{\xroa}[1]{\overset{#1}{\twoheadrightarrow}}
\newcommand{\ps}[1]{\mathbb{P}_{#1}^{\text{c}-1}}

% this is the pull-back of E along l: Z \to X
\newcommand{\pb}[1]{\RQ^*{#1}}
\newcommand{\pbe}{\pb{\cE}}
\newcommand{\YQ}{\gamma}
\newcommand{\RY}{\beta}
\newcommand{\RQ}{\delta}

\newcommand{\mft}[1]{T^{[MF]}_{#1}}
\newcommand{\dst}[1]{T^{\Dsing(R)}_{#1}}

\newcommand{\id}{\operatorname{id}}
\newcommand{\pd}{\operatorname{pd}}
\newcommand{\even}{\operatorname{even}}
\newcommand{\odd}{\operatorname{odd}}

\newcommand{\bl}[1]{{{\color{blue}#1}}}

\def\tmu{\tilde{\mu}}
\def\g{\gamma}
\def\tg{\tilde{\gamma}}
\def\tCliff{\operatorname{\tilde{Cliff}}}
\def\unital{\mathrm{unital}}
\def\D{\Delta}
\def\Tco{\operatorname{T^{\text{co}}}}
\def\Tcob{\operatorname{\ov{T}^{\text{co}}}}

\def\bg{\mathbf g}
\def\ta{\tilde{a}}


\def\z{\zeta}
\def\lf{\operatorname{lf}}
\def\mf{\operatorname{mf}}
\def\lffl{\operatorname{lf}^{\mathrm{fin. len.}}}
\def\mffl{\operatorname{mf}^{\mathrm{fin. len.}}}
\def\len{\operatorname{length}}
\def\Hlf{\operatorname{H}}

\def\codim{\operatorname{codim}}

\def\cPsi{\psi_{\mathrm{cyc}}}
\def\Fold{\operatorname{Fold}}
\def\ssupp{\operatorname{s.supp}}

\def\mult{\operatorname{mult}}
\def\stable{\mathrm{stable}}
\def\and{{ \text{ and } }}
\def\orr{{ \text{ or } }}
\def\oor{\orr}

\def\Perm{\operatorname{Perm}}

\def\Soule{Soul\'e}

\def\Op{\operatorname{Op}}
\def\res{\operatorname{res}}
\def\ind{\operatorname{ind}}

\def\sign{{\mathrm{sign}}}
\def\naive{{\mathrm{naive}}}
\def\l{\lambda}


\def\ov#1{\overline{#1}}

%%%-------------------------------------------------------------------
%%%-------------------------------------------------------------------
% NEW DEFINITIONS BEYOND MARK'S USUAL ONES 
% (OR MODIFIED) 
\newcommand{\chara}{\operatorname{char}}
\newcommand{\Kos}{\operatorname{Kos}}
\newcommand{\opp}{\operatorname{opp}}
\newcommand{\perf}{\operatorname{perf}}

\newcommand{\Fun}{\operatorname{Fun}}
\newcommand{\GL}{\operatorname{GL}}
\def\o{\omega}
\def\oo{\overline{\omega}}

\def\cont{\operatorname{cont}}
\def\te{\tilde{e}}
\def\gcd{\operatorname{gcd}}
%%%-------------------------------------------------------------------
%%%-------------------------------------------------------------------
%%%-------------------------------------------------------------------
%%%-------------------------------------------------------------------
%%%-------------------------------------------------------------------


\def\charpoly{\operatorname{char poly}}
\def\Gal{\operatorname{Gal}}

\makeindex


\begin{document}


\title{Math 325 Lecture Notes}

%\tableofcontents
%\setcounter{tocdepth}{0}


\Aug{23}


What is a number? Certainly the things used to count sheep, money, etc. are numbers:  $1, 2, 3, \dots$.
We will call these the {\em natural numbers}\index{natural number}\index{$\N$} and 
write $\N$ for the set of all natural numbers
$$
\N = \{1, 2, 3, 4, \dots \}.
$$
Since we like to keep track of debts too, we'll allow negatives and $0$, which gives us the {\em integers}\index{integers}\index{$\Z$}:
$$
\Z = \{ \dots, -3, -2, -1, 0, 1, 2, 3, 4, \dots\}.
$$
(The symbol $\Z$ is used since the German word for number is {\em zahlen}.)


Fractions should count as numbers also, so that we can talk about eating one and two-thirds of a pizza last night. We define a {\em rational number}\index{rational number} to be a number expressible
as a quotient of two integers: $\frac{m}{n}$ for $m,n \in \Z$ with $n \ne 0$. For example
$$
\frac{5}{3}, \frac27, \frac{2019}{2020}
$$
are rational numbers. Of course, we often talk about numbers such as
``two and a fourth'', but that the same as $\frac94$. Every integer
is a rational number just by taking $1$ for the denominator; for
example, $7 = \frac{7}{1}$. 
The set of all rational numbers is written as $\Q$ (for ``quotient'').\index{$\Q$} 

You might not have
thought about it before, but an expression of the form $\frac{m}{n}$ is really an ``equivalence class'': 
the two numbers $\frac{m}{n}$ and $\frac{a}{b}$ are deemed equal if $mb =
na$. For example $\frac69 = \frac23$ because $6 \cdot 3 = 9 \cdot 2$.



We'll talk more about decimals later on, but recall for now that a decimal that terminates is just another way of representing a rational number. For example,
$1.9881$ is equal to $\frac{19881}{10000}$. Less obvious is the fact that a decimal that repeats also represents a rational number: For example, 
$1.333\dots$ is rational (it's equal to $\frac43$) and so is 
$23.91278278278\dots$. We'll see why this is true later in the semester. 




Are these all the numbers there are? Maybe no one in this class  would answer ``yes'', but the ancient Greeks believed for a time that every number was rational. Let's convince
ourselves, as the Greeks did eventually, that there must be numbers that are not rational. Imagine a square of side length $1$. By the Pythagorean Theorem, the length of
its diagonal, call this number $c$, must satisfy 
$$
c^2 = 1^2 + 1^2 = 2.
$$
That is, there must be a some  number whose square is $2$ since certainly the length of the diagonal in such a square is representable as a number.  
Now, let's convince ourselves that there is no {\em rational number} with this property. In fact, I'll make this a theorem.

\begin{thm} There is no rational number whose square is $2$. \index{$\sqrt{2}$}
\end{thm}

\begin{preproof} Before launching a formal proof, let's philosophize
  about how one shows something does not exist. To show something does
  not exist, one proves  that its existence is
  not possible. For example, I know that  there must not be large clump of plutonium sewn into the mattress of my bed. I know this since, if such a clump existed, I'd be
  dead by now, and yet here I am, alive and well!

More generally and formally, one way to prove the falsity of a
statement $P$ is to argue that if we assume $P$ to be true then we can deduce from that
assumption something that is known
to be false. If you can do this, then you have proven $P$ is false. In symbols:
If one can prove
$$
P \Longrightarrow \text{Contradiction}
$$
then the statement $P$ must in fact be  false.

In the case at hand, letting  $P$ be the statement ``there is a rational number whose square is $2$'', the Theorem is asserting that $P$ is false. We will prove
this by assuming $P$ is true and deriving an impossibility.

This is known as a proof by contradiction.\index{proof by contradiction}\end{preproof}

\begin{proof} By way of contradiction, assume there were a rational number $q$ such that $q^2 = 2$. By definition of ``rational number'',  
we know that $q$ can be written as  $\frac{m}{n}$ for
some integers $m$ and $n$ such that $n \ne 0$. Moreover, we may assume that we have written $q$ is reduced form so that $m$ and $n$ have no
prime factors in common. In particular, we may assume that not both of $m$ and $n$ are even. (If they
  were both even, then  we could simplify the fraction by factoring out common factors of $2$'s.) Since $q^2 = 2$,
  $\frac{m^2}{n^2} = 2$ and hence $m^2 = 2n^2$. In particular, this shows $m^2$ is even and, since the square of an odd number is odd, it must be that $m$
  itself is even. So, $m = 2 a$ for some integer $a$. But then $(2a)^2 = 2n^2$ and hence $4a^2 = 2n^2$ whence $2a^2 = n^2$. For the same reason as before, this
  implies that $n$
  must be even. But this contradicts the fact that $m$ and $n$ are not both even. 

We have reached a contradiction, and so the assumption that there is a rational number $q$ such that $q^2 = 2$ must be false.
\end{proof}


A version of the previous proof was known even to the ancient Greeks.




Our first major mathematical goal in the class is to make a formal definition of the real numbers. 
Before we do this, let's record some basic properties of the rational numbers. I'll state this as a Proposition (which is something like a minor version of a Theorem), but we won't prove them; instead, we'll take it for granted to be true based on our own past experience with numbers.

For the rational numbers, we can do arithmetic ($+,-,\times,\div$) and we also have a notion of size $(<,>)$. The first seven observations below describe the arithmetic, and the last three describe the notion of size.


\begin{prop}[Arithmetic and order properties of $\Q$]\label{prop1}
  The set of rational numbers form an ``ordered field''.\label{ordered field} This means that the following ten properties hold:
\begin{enumerate}
\item There are operations $+$ and $\cdot$ defined on $\Q$, so that if $p, q$ are in $\Q$, then so are $p + q$ and $p \cdot q$.
\item Each of $+$ and $\cdot$ is a commutative operation (i.e., $p + q = q + p$ and $p \cdot q = q \cdot p$ hold for all rational numbers $p$ and $q$). 
\item Each of $+$ and $\cdot$ is an associative  operation (i.e., $(p + q) + r = p + (q + r)$ and $(p \cdot q) \cdot r = p \cdot (q \cdot r)$ hold for all rational numbers $p$,
  $q$, and $r$). 
\item The number $0$ is an identity element for addition and the number $1$ is an identity element for multiplication. This means that
 $0 + q = q$ and $1 \cdot q = q$ for all $q \in \Q$. 
\item The distributive law holds: $p \cdot (q + r) = p \cdot q + p \cdot r$ for all $p,q,r \in \Q$. 
\item Every number has an additive inverse: For any $p \in \Q$, there is a number $-p$ satisfying $p + (-p) = 0$.
\item Every nonzero number has a multiplicative inverse: For any $p \in \Q$ such that $p \ne 0$, there is a number $p^{-1}$ satisfying ${p \cdot p^{-1}= 1}$.
\item There is a ``total ordering'' $\leq$ on $\Q$. This means that 
\begin{enumerate}
\item For all $p, q \in \Q$, either $p \leq q$ or $q \leq p$.
\item If $p \leq q$ and $q \leq p$, then $p = q$.
\item For all $p, q, r \in \Q$, if $p \leq q$ and $q \leq r$, then $p \leq r$.
\end{enumerate}
\item The total ordering $\leq$ is compatible with addition: If $p \leq q$ then $p + r \leq q + r$.
\item The total ordering $\leq$ is compatible with multiplication by non-negative numbers: 
If $p \leq q$ and $r \geq 0$ then $pr \leq qr$.
\end{enumerate}
\end{prop}



Which of the properties from Proposition~\ref{prop1} does $\N$ satisfy?

The commutativity, associativity, distributive law, multiplicative identity, and all of the ordering properties are true for $\N$.


We expect everything from Proposition~\ref{prop1} to be true for the real numbers. We will build them into our definition.
To define the real numbers $\R$, we take the ten properties listed in the Proposition to be axioms.\index{axiom} 
It turns out the set of real numbers satisfies one key additional
property, called the {\em completeness axiom}, which I cannot state yet. 


\begin{axioms*} The set of all real numbers, written $\R$, satisfies the following eleven properties:\index{real numbers}\index{$\R$}
\begin{enumerate}
\item[(Axiom 1)] There are operations $+$ and $\cdot$ defined on $\R$, so that if $x, y \in \R$, then so are $x + y$ and $x \cdot y$.
\item[(Axiom 2)]  Each of $+$ and $\cdot$ is a commutative operation. 
\item[(Axiom 3)]  Each of $+$ and $\cdot$ is an associative  operation.
\item[(Axiom 4)]  The real number $0$ is an identity element for addition and the real number $1$ is an identity element for multiplication. This means that
 $0 + x = x$ and $1 \cdot x = x$ for all $x \in \R$. 
\item[(Axiom 5)]  The distributive law holds: $x \cdot (y + z) = x \cdot y + x \cdot z$ for all $x,y,z \in \R$.
\item[(Axiom 6)]  Every real number has an additive inverse: For any $x \in \R$, there is a number $-x$ satisfying $x + (-x) = 0$.
\item[(Axiom 7)]  Every nonzero real number has a multiplicative inverse: For any $x \in \R$ such that $x \ne 0$, there is a real number $x^{-1}$ satisfying $x^{-1} \cdot x= 1$.
\item[(Axiom 8)]  There is a ``total ordering'' $\leq$ on $\R$. This means that 
\begin{enumerate}
\item For all $x, y \in \R$, either $x \leq y$ or $y \leq x$.
\item If $x \leq y$ and $y \leq z$, then $x \leq z$.
\item For all $x, y, z \in \R$, if $x \leq y$ and $y \leq z$, then $x \leq z$.
\end{enumerate}
\item[(Axiom 9)]  The total ordering $\leq$ is compatible with addition: If $x \leq y$ then $x + z \leq y + z$ for all $z$.
\item[(Axiom 10)]  The total ordering $\leq$ is compatible with multiplication by nonnegative real numbers: If $x \leq y$ and $z \geq 0$ then $zx \leq zy$.
\item[(Axiom 11)]  The completeness axiom holds. (I will say what this means later.) 
\end{enumerate}
\end{axioms*}



There are many other familiar properties that are consequences of this list of axioms. As an example we can deduce the following property:
\begin{quote}
  ``Cancellation of Addition'': For real numbers, $x,y,z\in \R$,  if $x + y = z + y$ then $x = z$.
\end{quote}
Let's prove this carefully, using just the list of axioms: Assume that $x + y = z + y$. Then we can add $-y$ (which exists by Axiom 6) to both sides to get
$(x + y) + (-y) = (z + y) + (-y)$. This can be rewritten as $x + (y + (-y)) = z + (y + (-y))$ (Axiom 3) and hence as $x + 0 = z + 0$ (Axiom 6), which gives $x = z$ (Axiom 4 and Axiom 2).



For another example, we can deduce the following fact from the axioms:
$$
r \cdot 0 = 0 \, \text{ for any real number $r$}.
$$
Let's prove this carefully: Let $r$ be any real number. We have $0 +  0 = 0$ (Axiom 4) and hence $r \cdot (0 + 0) = r \cdot 0$. But
$r \cdot (0 + 0) = r \cdot 0 + r \cdot 0$ (Axiom~5) and so $r \cdot 0 = r \cdot 0 + r \cdot 0$. We can rewrite this as
$0 + r \cdot 0 = r \cdot 0 + r \cdot 0$ (Axiom 4). Now apply the Cancellation of Addition property (which we previously deduced from the axioms) to obtain
$0 = r \cdot 0$.

As I said, there are many other familiar properties of the real numbers that follow from these axioms, but I will not list them all. The great news is that all of these familiar properties follow from this short list of axioms. We will prove a couple, but for the most part, I'll rely on your innate knowledge that
facts such as $r \cdot 0 = 0$ hold. 







\Aug{25}

\begin{defn} A real number is \emph{irrational}\index{irrational} if it is not rational.
\end{defn}

	
	

\subsection*{Making sense of if then statements and quantifier statements}

\begin{itemize}
\item The \emph{converse}\index{converse} of the statement  ``If $P$ then $Q$'' is the statement  ``If $Q$ then $P$''.
\item The \emph{contrapositive}\index{contrapositive} of the statement  ``If $P$ then $Q$'' is the statement  ``If not $Q$ then not $P$''.
\item Any if then statement is equivalent to its contrapositive, but not necessarily to its converse!
\end{itemize}

\begin{enumerate}
\item For each of the following statements, write its contrapositive and its converse. Is the original/contrapositive/converse true or false for real numbers $a,b$? Explain why (but don't prove). 
\begin{enumerate}
\item If $a$ is irrational, then $1/a$ is irrational.
\item If $a$ and $b$ are irrational, then $ab$ is irrational.
\item If $a>3$, then $a^2>9$.
\end{enumerate}
\end{enumerate}

\begin{framed}
\begin{enumerate}
\item true; contrapositive is ``If $1/a$ is rational, $a$ is rational'' is true; converse is ``if $1/a$ is irrational, then $a$ is irrational'' is true.
\item false; contrapositive is ``If $ab$ is rational, either $a$ or $b$ is rational'' is false; converse is ``if $ab$ is irrational then $a$ and $b$ are irrational'' is false.
\item true; contrapositive is ``if $a^2 \leq 9$, then $a\leq 3$ is true; converse is ``if $a^2>9$ then $a>3$'' is false.
\end{enumerate}
\end{framed}


\begin{itemize}
\item The symbol for ``for all'' is $\forall$\index{$\forall$} and the symbol for there exists is $\exists$.\index{$\exists$}
\item The negation of ``For all $x\in S$, $P$'' is ``There exists $x\in S$ such that $\mathrm{not} \, P$''.
\item The negation of ``There exists $x\in S$ such that $P$'' is ``For all $x\in S$, $\mathrm{not} \, P$''.
\end{itemize}





\begin{enumerate}\setcounter{enumi}{1}
\item Rewrite each statement with symbols in place of quantifiers, and write its negation. Is the original statement true or false? Explain why (but don't prove them).
\begin{enumerate}
\item There exists $x\in \Q$ such that $x^2 = 2$.
\item For all $x\in \R$,  $x^2 >0$.
\item For all $x\in \R$ such that\footnote{In a statement of the form ``For all $x\in S$ such that $Q$, $P$'', the ``such that $Q$'' part is part of the hypothesis: it is restricting the set $S$ that we are ``alling''' over.} $x\neq 0$,  $x^2 >0$.
\item For all $x\in \R$, there exists $y\in \R$ such that $x<y$.
\item There exists $x\in \R$ such that for all $y\in \R$, $x<y$.
%\item There exists $x\in (0,\infty)$ such that\footnote{$(0,\infty)=\{z\in \R \ | \ 0 < z\}$, just like in calculus.} for all $y\in (0,\infty)$, $xy=1$.
%\item For all $x\in (0,\infty)$, there exists $y\in (0,\infty)$ such that $xy=1$.
\end{enumerate}
\end{enumerate}

\begin{framed}
\begin{enumerate}
\item $\exists x\in \Q:x^2=2$ is false. Negation: $\forall x\in \Q, x^2\neq 2$.
\item $\forall x\in \R$, $x^2>0$ is false. Negation: $\exists x\in \R : x^2\leq 0$.
\item $\forall x\in \R: x\neq 0, x^2>0$ is true. Negation: $\exists x\in \R : x\neq 0, x^2\leq 0$.
\item $\forall x\in \R, \exists y\in \R: x<y$ is true. Negation: $\exists x\in \R: \forall y\in \R, x\geq y$.
\item $\exists x\in \R: \forall y\in \R, x< y$ is false. Negation: $\forall x\in \R, \exists y\in \R:  x\geq y$.
\end{enumerate}
\end{framed}

\subsection*{Proving if then statements and quantifier statements}


\

\begin{framed}
\begin{itemize}
\item The general outline of a direct proof of ``If $P$ then $Q$'' goes
\begin{enumerate}
\item Assume $P$.
\item Do some stuff.
\item Conclude $Q$.
\end{enumerate}
\item Often it is easier to prove the contrapositive of an if then statement than the original, especially when the negation of the hypothesis or conclusion is something negative.
\item The general outline of a proof of ``For all $x\in S$, $P$'' goes
\begin{enumerate}
\item Let $x\in S$ be arbitrary.
\item Do some stuff.
\item Conclude that $P$ holds for $x$.
\end{enumerate}
\item To prove a there exists statement, you just need to give an example. To prove ``There exists $x\in S$ such that $P$'' directly:
\begin{enumerate}
\item Consider$^{\textrm{2}}$ $x=$[some specific element of $S$].
\item Do some stuff.
\item Conclude that $P$ holds for $x$.
\end{enumerate}


\end{itemize}

\end{framed}

\
\renewcommand{\thefootnote}{}

\begin{enumerate}\setcounter{enumi}{2}
\item Let\footnote{$^{\textrm{2}}$How you found this $x$ is logically irrelevant to an existence proof, and should not be included.} $x$ and $y$ be real numbers. Use the axioms of $\R$ to prove$^{\textrm{3}}$\footnote{$^{\textrm{3}}$Hint: You may want to add something to both sides.} that $x \geq y$ if and only if $-y\geq -x$.



\item Let $x$ be a real number. Show that if $x^2$ is irrational, then $x$ is irrational.



\item\label{xy=1} Let $x$ be a real number. Use the axioms of $\R$ and facts we have proven in class to show that if there exists a real number $y$ such that $xy=1$, then $x\neq 0$.



\item Prove that$^{\textrm{4}}$\footnote{$^{\textrm{4}}$Hint: Use (\ref{xy=1}).} for all $x\in \R$ such that $x\neq 0$, we have $x^2\neq 0$.


\item Prove that there exists some $x\in \R$ such that for every $y\in \R$, $xy=x$.



\item Prove$^{\textrm{5}}$\footnote{$^{\textrm{5}}$You can ``work out of order here'' and \emph{use} (\ref{1>0}) now.} that (2d) is true and (2e) is false. 



\item Let $S\subseteq \R$ be a set of real numbers. Apply your results above to prove that if for every $x\in S$, $x^2$ is irrational, then for every $y\in S$, $y$ is irrational.



\item\label{1>0} Prove that $1>0$.



\item Let $x,y$ be real numbers. Prove that if $x\leq 0$ and $y\leq 0$, then $xy\geq 0$.

\end{enumerate}

\begin{framed}
\begin{enumerate}
\item[(3)] Let $x\geq y$. Adding $(-x) + (-y)$ to both sides (which exists by Axiom~6), we obtain $-y=x+((-x)+ (-y)) \geq y+((-x)+(-y)) = -x$ (by Axiom 9 and Axiom~5).
	Conversely, let $-x \leq -y$. Adding $x+y$ to both sides, we obtain $y=(x+y)+(-x) \leq (x+y)+(-y) = x$ (by Axiom~9 and Axiom~5).
\item[(6)] Let $x$ and $y$ be nonzero real numbers. By Axiom 7, there are element $x^{-1},y^{-1}\in \R$ such that $x x^{-1}=1$ and $y y^{-1}=1$. Then $xy \cdot (x^{-1}y^{-1}) = (x x^{-1}) (y y^{-1}) =1$, using Axioms 2 and 3 in the first equality and Axiom 5 in the second. By the previous fact (applies to $xy$) we conclude that $xy\neq 0$.
\item[(7)] Consider $x=1$. Let $y\in \R$. By Axiom 4, we have $x y= 1 y =y$. Thus, for all $y\in \R$, we have $xy=x$.
\item[(8)] (2d):  Let $x\in \R$. Consider $y=x+1$. Since $1>0$ we have $y=x+1 > x+0 = x$. Thus, for each $x\in \R$, we have some $y$ such that $x<y$.
(2e): We claim this is false. Suppose, for the sake of contradiction that this was true, and let $x$ be as in the statement. Then for any $y\in \R$, we have $x<y$. But, for $y=x$, the inequality $x<y$ is false. This is a contradiction, so the statement must be false.

\item[(10)] First we establish two lemmas.
\\ \textbf{Lemma:} For real numbers $x\in\R$ we have $-(-x)=x$.
\\ \textit{Proof:} We have \[(-x) + (-(-x))=0\] so \begin{align*}  -(-x) &= 0 + -(-x) = (x+ (-x) )+ (-(-x)) \\&= x+ ((-x) + (-(-x)))=x.\qed\end{align*}

\noindent \textbf{Lemma:} For real numbers $x,y\in\R$ we have $(-x)y= -(xy)$.
\\ \textit{Proof:} We have that \begin{align*} 0 = 0 y= (x+ (-x)) y = xy + (-x)y.\end{align*} Adding $-(xy)$ to both sides we get  \begin{align*} -(xy) &= -(xy) + ( xy + (-x)y ) \\&= (-(xy) + (-x) y) + (-x)y \\&= 0 + (-x)y = (-x)y.\qed\end{align*}
  We proceed with the proof. We either have $1\geq 0$ or $1\leq 0$. Suppose that $1\leq 0$. Then $-1 \geq 0$, so \[(-1)(-1) \geq (-1) 0 = 0.\] But \[(-1)(-1) = - (1 (-1)) = - (-1) = 1,\] so $1 \geq 0$, contradicting the hypothesis.
\end{enumerate}
\end{framed}


%\begin{comment}



\Aug{30}

I owe you a statement of the very important Completeness Axiom. Before we get there, I want to recall an axiom of $\N$ that we haven't discussed yet. It pertains to minimum elements in sets. Let's be precise and define minimum element.

\begin{defn} Let $S$ be a set of real numbers. A \emph{minimum}\index{minimum} element of $S$ is a real number $x$ such that
\begin{enumerate}\item $x\in S$, and
\item for all $y\in S$, $x\leq y$.
\end{enumerate}
In this case, we write $x=\min(S)$.\index{$\min$}
\end{defn}
The definition of \emph{maximum}\index{maximum} is the same except with the opposite inequality.

\begin{axiom}[Well-ordering axiom]\label{Well-ordering axiom}
	Every nonempty subset of $\N$ has a minimum element.
\end{axiom}

\begin{ex} If $S$ is the set of even multiples of $7$, then $S$ has $14$ as its minimum.
\end{ex}

We generally like to say \emph{the} minimum, rather than \emph{a} minimum. To justify this, let's prove the following.

\begin{prop}\label{prop:min-unique} Let $S$ be a set of real numbers. If $S$ has a minimum, then the minimum is unique.
\end{prop}


\begin{preproof} The proposition has the general form  ``If a thing with property P exists, then it is unique''. 
	
  How do we prove a statement such as
  ``If a thing with property P exists, then it is unique''?
  We argue that if two things $x$ and $y$ both have property $P$, then $x$ and $y$ must be the same thing.
\end{preproof}

\begin{proof}[Proof of Proposition~\ref{prop:min-unique}] Let $S$ be a set of real numbers, and let $x$ and $y$ be two minima of $S$. Applying part (1) of the definition of minimum to $y$, we have $y\in S$. Applying part (2) of the definition of minimum to $x$ and the fact that $y\in S$, we get that $x\leq y$. Switching roles, we get that $y\leq x$. Thus $x=y$.

We conclude that if a minimum exists, it is necessarily unique.
\end{proof}


The previous proposition plus the Well-Ordering Axiom together imply that every nonempty subset of $\N$ has exactly one minimum element. A similar proof shows that if a maximum exists, it is necessarily unique. Could a set fail to have a maximum or a minimum? Yes!

\begin{ex}
\begin{enumerate}
\item The empty set $\varnothing$ has no minimum and no maximum element. (There is no $s\in \varnothing$!)
\item The set of natural numbers $\N$ has $1$ as a minimum, but has no maximum. (Suppose there was: if $n=\max(\N)$ was the maximum, then $n<n+1\in \N$ gives a contradiction.)
\item The open interval $(0,1)=\{ x\in \R \ | \ 0 < x < 1\}$ has no minimum and no maximum. (Exercise later.)
\end{enumerate}
\end{ex}




\begin{defn} Let $S$ be any subset of $\R$. A real number $b$ is called an  {\em upper bound}\index{upper bound} of $S$ provided that for every $s \in S$,
we have $s \leq b$. 
\end{defn}


For example, the number $1$ is an upper bound for the interval $(0,1)$. The number $182$ is also an upper bound of this set and so is $\pi$. It is pretty clear
that $1$ is the ``best'' (i.e., smallest) upper bound for this set, in the sense that every other upper bound of $(0,1)$ must be at least as
big as $1$. Let's make this official:

\begin{prop}  If $b$ is an upper bound of the set $(0,1)$, then $b \geq 1$.
\end{prop}

I will prove this claim using just the axioms of the real numbers (in fact, I will only use the first 10 axioms):

\begin{proof} Suppose $b$ is an upper bound of the set $(0,1)$. By way of contradiction, suppose $b < 1$. (Our goal is
  to derive a contradiction from this.)


 Consider the number $y = \frac{b+1}{2}$ (the
  average of $b$ and $1$). I will argue that $b < y$ and $b \geq y$, which is not possible.

Since we are assuming $b < 1$, we have $\frac{b}{2} < \frac12$ and hence
$$
b = \frac{2b}{2} = \frac{b}{2}+ \frac{b}{2} < \frac{b}{2} + \frac{1}{2} = \frac{b+1}{2} = y.
$$
So,  $b < y$. 

Similarly, 
$$
1 = \frac{1 + 1}{2} > \frac{b+1}{2} = y
$$
so that
$$
y < 1.
$$
Since $\frac12 \in S$ and $b$ is an upper bound of $S$, we have $\frac12 \leq b$. Since we already know that $b < y$, it follows that
$\frac12 < y$
and hence $0 < y$. We have proven that $y \in (0,1)$. But, 
remember that $b$ is an upper bound of $(0,1)$, and so we get $y \leq b$ by definition.

To summarize: given an upper bound $b$ of $(0,1)$, starting with the assumption that $b < 1$,
we have deduced the existence of a number $y$ such that  both $b < y$ and $y \leq b$ hold.
As this is not possible, it must be that $b < 1$ is false, and hence $b \geq 1$. 
\end{proof}

This claim proves the (intuitively obvious) fact that $1$ is ``least upper bound'' of the set $(0,1)$. The notion of ``least upper bound''
will be an extremely important one in this class.


\begin{defn} A subset $S$ of $\R$ is called {\em bounded above}\index{bounded above} if there exists at least one upper bound for $S$. That is, $S$ is bounded above provided there
  is a real number $b$ such that for all $s \in S$ we have $s \leq b$.
\end{defn}

For example, $(0,1)$ is bounded above, by for example $50$.  

The subset $\N$ of $\R$ is not bounded above --- there is no real
number that is larger than every natural number. This  fact is surprisingly non-trivial to deduce just using
the axioms; in fact, one needs the Completeness Axiom to show it.  But of course our intuition tells us that it is obviously true. 


Let's give a more interesting example of a subset of $\R$ that is bounded above.  

\begin{ex}
Define $S$ to be those real numbers whose squares are less than $2$:
$$
S = \{x \in \R \mid x^2 < 2\}.
$$
I claim $S$ is bounded above. In fact, I'll prove $2$ is an upper
bound: Suppose $x \in S$. If $x > 2$, then $x \cdot x > x \cdot 2$ and $x \cdot 2 > 2 \cdot 2$, and hence $x^2 > 4 > 2$. This contradicts the fact that $x \in
S$. So, we must have $x \leq 2$.

A nearly identical argument shows that $1.5$ is also an upper bound (since $1.5^2 = 2.25 > 2$) and similarly one can show $1.42$ is an upper bound.  
But $1.41$ is not an upper bound.  For note that $1.411^2 = 1.99091$ and so $1.41 \in S$ but $1.411 > 1.41$. 

Question: What is the smallest (or least) upper bound for this set $S$? Clearly, it ought to be $\sqrt{2}$ (i.e., the positive number whose square is equal
to exactly $2$), but there's a catch: how do we know that such real number exists?
\end{ex}


\begin{defn} Suppose $S$ is subset of $\R$ that is bounded above. A {\em supremum}\index{supremum} (also known as a {\em least upper bound}) of $S$ is a number $\ell$ such that
\begin{enumerate}
\item $\ell$ is an upper bound of $S$ (i.e., $s \leq \ell$ for all $s \in S$) and
\item if $b$ is any upper bound of $S$, then $\ell \leq b$. 
\end{enumerate}
In this case we write $\sup(S)=\ell$.\index{$\sup$}
\end{defn}




\begin{ex} $1$ is a supremum of $(0,1)$. Indeed, it is clearly an upper bound, and 
in the ``Claim'' above, we proved that if $b$ is any upper bound of $(0,1)$ then $b \geq 1$. Note that this example shows that a supremum of $S$ does not necessarily 
belong to $S$.
\end{ex}

\begin{ex} I claim $1$ is  a supremum of \[(0,1] = \{x \in \R \mid 0 <
  x \leq 1\}.\] It is by definition an upper bound. If $b$ is any upper bound of $(0, 1]$ then, since $1 \in (0, 
  1]$, by definition
  we have $1 \leq b$. So $1$ is the supremum of $(0,1]$. 
\end{ex}

\begin{obs} Let $S$ be a set of real numbers. Suppose that $b\in S$ and that $b$ is an upper bound for $S$. Then
\begin{enumerate}
\item $b$ is the maximum of $S$, and 
\item $b$ is a supremum of $S$.
\end{enumerate}
\end{obs}

The subset $\N$ does not have a supremum since, indeed, it does not have any upper bounds at all.



%\begin{question} Does the set $\{x \in \R \mid x^2 < 2\}$ have a supremum? If so, what is it? It ought to be $\sqrt{2}$, but again we do not yet know that
 % this number exists. 
%\end{question}

Can you think of an example of a set that is bounded above but has no supremum? There is only one such example and it is rather silly: the empty set is
bounded above. Indeed, every real number is an upper bound for the empty set. So, there is no least upper bound.

Having explained the meaning of the term ``supremum'', I can finally state the all-important completeness axiom:


\begin{axiom*}[Completeness Axiom]\index{completeness axiom} Every nonempty, bounded-above subset of $\R$ has a supremum.
\end{axiom*}


\Sept{1}

\begin{enumerate}
\item Write, in simplified form, the negation of the statement ``$b$ is an upper bound for $S$''.

\begin{framed}
There exists some $x\in S$ such that $x>b$.
\end{framed}

\item Write, in simplified form, the negation of the statement ``$S$ is bounded above''.

\begin{framed}
For every $b\in \R$, there exists $x\in S$ such that $x>b$.
\end{framed}

\item Let $S$ be a set of real numbers and suppose that $\ell=\sup(S)$. 
\begin{enumerate}
\item If $x > \ell$, what is the most concrete thing you can say about $x$ and $S$?
\item If $x < \ell$, what is the most concrete thing you can say about $x$ and $S$?
\end{enumerate}


\begin{framed}
\begin{enumerate}
\item $x\notin S$.
\item There exists some $y\in S$ such that $y > x$.
\end{enumerate}
\end{framed}

\item Let $S$ be a set of real numbers, and let $T=\{ 2s \ | \ s\in S\}$. Prove that if $S$ is bounded above, then $T$ is bounded above.

\begin{framed}
Assume that $S$ is bounded above. Then there is some upper bound $b$ for $S$, so for every $s\in S$, we have $b\geq s$. We claim that $2b$ is an upper bound for $T$. Indeed, if $t\in T$, then we can write $t=2s$ for some $s\in S$, and $s\leq b$ implies $t=2s\leq 2b$. Thus, $T$ is bounded above.
\end{framed}

\item Let $S$ be a set of real numbers. Show that if $S$ has a supremum, then it is unique.
\begin{framed}
Suppose both $x$ and $y$ are both suprema 
  of the same subset $S$ of $\R$.  Then, since $y$ is an upper bound of $S$ and $x$ is a supremum of $S$, by part (2) of the definition of ``supremum''
  we have $y \geq x$. Likewise, since $x$ is an upper
bound of $S$ and $y$ is a supremum of $S$, we have 
$x \geq y$ by definition.  Since $x \leq y$ and $y \leq x$, we conclude $x = y$.  
\end{framed}

\item Let $S$ be a set of real numbers, and let $\displaystyle T=\left\{ \frac{s}{2} \ | \ s\in S\right\}$. Directly prove that if $S$ is unbounded above, then $T$ is unbounded above.
\begin{framed}
Assume that $S$ is unbounded above. To show that $T$ is unbounded above, let $b$ be a real number. Since $S$ is unbounded above, $2b$ is not an upper bound for $S$, so there is some $s\in S$ with $s>2b$. Then $\frac{s}{2} > b$. By definition of $T$, we have $\frac{s}{2}\in T$, so $b$ is not an upper bound of $T$. We conclude that $T$ is unbounded above.
\end{framed}
\end{enumerate}



\Sept{6}



Let us now explore consequences of the completeness axiom. We know that there is no rational number whose square is $2$; now we show that there is indeed a real number whose square is two.


\begin{prop} \label{prop116} There is a positive real number whose square is $2$.\index{$\sqrt{2}$}
\end{prop}

\begin{proof} 
Define $S$ to be the subset 
$$
S = \{x \in \R \mid x^2 < 2\}.
$$
$S$ is nonempty since, for example, $1 \in S$, and it is bounded above, since, for example, 
$2$ is an upper bound for $S$, as we showed earlier.  So, by the Completeness Axiom, $S$ has a least upper bound, and we know it is unique from
  the proposition above. Let us call it $\ell$. I will prove $\ell^2 = 2$. 


We know one of $\ell^2 > 2$, $\ell^2 < 2$ or $\ell^2 = 2$ must hold. We prove $\ell^2 = 2$ by showing that both
$\ell^2 > 2$ and $\ell^2 < 2$ are impossible.

We start by observing that $1 \leq \ell \leq 2$. The inequality $1 \leq \ell$ holds since $1 \in S$ and $\ell$ is an upper bound of $S$, 
and the inequality $\ell \leq 2$ holds  since $2$ is an upper bound of $S$ and $\ell$ is the least upper bound of~$S$.



Suppose $\ell^2 < 2$. We show this leads to a contradiction by showing that $\ell$ is not an upper bound of $S$ in this case. We will do this by constructing a number
that is ever so slightly bigger than $\ell$ and  belongs to $S$. 
Let $\e = 2 - \ell^2$. Then $0 < \e \leq 1$ (since $\ell^2 < 2$ and $\ell^2 \geq 1$).  
We will now show that $\ell + \e/5$ is in $S$: We have
$$
(\ell + \e/5)^2 = \ell^2 + \frac25 \ell\e + \frac{\e^2}{25} = \ell^2 + \e(\frac{2\ell}{5} + \frac{\e}{25}).
$$
Now, using $\ell \leq 2$ and $0 < \e \leq 1$, we deduce
$$
0 < \frac{2\ell}{5} + \frac{\e}{25} \leq \frac45 + \frac{\e}{25} < 1.
$$
Putting these equations and inequalities  together yields
$$
(\ell + \frac{\e}{5})^2 < \ell^2 + \e = 2.
$$
So, $\ell + \frac{\e}{5} \in S$ and yet $\ell + \frac{\e}{5} > \ell$, contradicting the fact that $l$ is an upper bound of $S$. We conclude $\ell^2 < 2$ is not
possible.

Assume now that $\ell^2 > 2$. Our strategy will be to construct a number ever so slightly smaller than $\ell$, which therefore cannot be an upper bound of $S$, and use this to
arrive at a contradiction. 
Let $\d = \ell^2 - 2$. Then $0 < \d \leq 2$ (since $\ell \leq 2$ and hence $\ell^2 - 2 \leq 2$).
Since $\d > 0$, we have $\ell - \frac{\d}{5} < \ell$. Since $\ell$ is the least upper bound of $S$, $\ell- \frac{\d}{5}$ must not be an upper bound of $S$. By definition, this means that there is $r \in S$ such that 
$\ell - \frac{\d}{5} < r$. Since $\d \leq 2$ and $\ell \geq 1$, it follows that $\ell - \frac{\d}{5}$ is positive and hence so is $r$. We may thus square both sides of 
$\ell - \frac{\d}{5} < r$ to obtain
$$
(\ell - \frac{\d}{5})^2 < r^2.
$$
Now
$$
(\ell - \frac{\d}{5})^2 = \ell^2 - \frac{2\ell\d}{5} + \frac{\d^2}{25} = \d + 2 - \frac{2\ell\d}{5} + \frac{\d^2}{25}
$$
since $\ell^2 = \d + 2$.
Moreover, 
$$
\d + 2 - \frac{2\ell\d}{5} + \frac{\d^2}{25}
= 2 + \d(1 - \frac{2\ell}{5} + \frac{\d}{25})
\geq 2 + \d(1 - \frac{4}{5} + \frac{\d}{25})
$$
since $\ell \leq 2$. We deduce that 
$$
\d + 2 - \frac{2\ell\d}{5} + \frac{\d^2}{25}
\geq 2 + \d(\frac{1}{5}) \geq 2. 
$$

Putting these inequalities together gives $r^2 > 2$, contrary to the fact that $r \in S$. We conclude that $\ell^2 > 2$ is also not possible.

Since $\ell^2 < 2$ and $\ell^2 > 2$ are impossible, we must have $\ell^2 = 2$.
\end{proof}



The collection of rational numbers does not satisfy the completeness axiom and indeed it is precisely the completeness axiom that differentiates $\R$ from
$\Q$. 

\begin{ex} Within the set $\Q$ the subset $S = \{x \in \Q \mid x^2 < 2\}$ does not have a supremum. That is, no matter which rational number you pick that is an
  upper bound for $S$, you may always find an even smaller one that is also an upper bound of $S$. 
\end{ex}

It is precisely the completeness axiom that assures us that everything that ought to be a number (like the length of the diagonal of a square with side length $1$) really is a number.  It gives us that there are ``no holes'' in the real number line --- the real numbers are {\em
  complete}. 


For example, we can use it to prove that $\sqrt[8]{147}$ exists: Let $S = \{ {x \in \R} \mid x^8 < 147\}$. Then $S$ is nonempty (e.g., $0 \in S$) and bounded above
(e.g., $50$ is an upper bound) and so it must have a supremum $\ell$. 
A proof similar to (but even messier than) the proof of Proposition \ref{prop116} above shows
that $\ell$ satisfies $\ell^8 = 147$. 

The completeness axiom is also at the core of the Intermediate Value Theorem and many of the other major theorems we will
cover in this class. 

We also need the completeness axiom to understand the relationship between $\N$, $\Q$, and $\R$.

\begin{thm} \label{thm120}
If $x$ is any real number, then there exists a natural number $n$ such that $n > x$.
\end{thm}

This looks really stupid at first. How could it be false? But consider: there are examples of ordered fields, i.e. situations in which Axioms 1--10 
hold, in which this Theorem is not true! So, its proof must rely on the Completeness Axiom. 


\begin{proof} Let $x$ be any real number. By way of contradiction, suppose there is no natural number $n$ such that $n > x$. That is, suppose that for all $n \in
  \N$, $n \leq x$. Then $\N$ is a bounded above (by $x$). Since it is also clearly nonempty,  by the Completeness
  Axiom, $\N$ has a supremum, call it $\ell$. Consider the number $y := \ell- 1$. Since $y < \ell$ and $\ell$ is the supremum of $\N$,  
$y$ cannot be an upper bound of $\N$. So, there must be some $m \in \N$
  such that such that $\ell-1 < m$. But then by adding $1$ to both sides of this inequality we get $\ell < m+1$ and, 
since $m + 1 \in \N$, this contradicts that assumption that $\ell$ is the supremum of $\N$. 

We conclude that, given any real number $x$,  there must exist a natural number  $n$ such that $n > x$.
\end{proof}

\begin{cor}[Archimedean Principle]\index{Archimedean principle} If $a \in \R$, $a >0$, and $b \in \R$, then for some natural number $n$ we have $na > b$.
\end{cor}


``No matter how small $a$ is and how large $b$ is, if we add $a$ to itself enough times, we can overtake $b$.''

\begin{proof} We apply Theorem \ref{thm120} to the real number $x = \frac{b}{a}$. It gives that 
  there is a natural number $n$ such that $n > x = \frac{b}{a}$. Since $a > 0$, upon multiplying both sides
  by $a$ we get $n \cdot a  > b$.
\end{proof}

\begin{thm}[Density of the Rational Numbers] \index{Density of rational numbers}
Between any two distinct  real numbers there is a rational number; more precisely, if $x, y \in \R$ and $x < y$, then there
  exists $q \in \Q$ such that $x < q < y$.
\end{thm}



\begin{proof} We will prove this by consider two cases: $x \geq 0$ and $x < 0$.

Let us first assume $x \geq 0$.
We apply the Archimedean Principle using $a = y-x$ and $b = 1$. (The Principle applies as
$a > 0$ since $y> x$.) This gives us that there is a natural number $n \in \N$ such that
$$
n \cdot (y-x) > 1
$$
and thus
$$
0 < \frac{1}{n} < y-x.
$$

Consider the set $S=\{p\in \N \ | \ p \, \frac{1}{n} >x \}$.
Since $\frac{1}{n} > 0$, using the Archimedean principle again, there is at least one natural number $p\in S$. By the Well Ordering Axiom, there is a smallest natural number $m\in S$. 

We claim that $\frac{m-1}{n} \leq x$. Indeed, if $m>1$, then $m-1\in \N \smallsetminus S$ (because $m-1$ is less than the minimum), so $\frac{m-1}{n}\leq x$; if $m=1$, then $m-1=0$, so $\frac{m-1}{n} = 0 \leq x$.

So, we have
$$
\frac{m-1}{n} \leq x< \frac{m}{n}
$$
By adding $\frac{1}{n}$ to both sides of $\frac{m-1}{n} \leq x$ and using that $\frac{1}{n} < y- x$, we get
$$
\frac{m}{n} \leq x + \frac{1}{n} < x + (y -x) = y
$$
and hence
$$
x < \frac{m}{n} < y.
$$
Since $\frac{m}{n}$ is clearly a rational number, this proves the result in this case (when $x > 0$). 

We now consider the case $x < 0$. The idea here is to simply ``shift'' up to the case we've already proven. 
By Theorem \ref{thm120}, we can find a natural number $j$ such that $j > -x$ and thus $0 < x +j < y +j$. Using the first case, which we have already proven,
applied 
to the number $x + j$ (which is positive), there is a rational number $q$ such that $x+j < q < y+j$. We deduce that $x < q-j < y$, and, since $q - j$ is also
rational, this proves the theorem in this case.
\end{proof}


\Sept{8}

\begin{enumerate}
\item Let $W$ be the set of real numbers $x$ that satisfy the inequality $x^3+x<10$.
\begin{enumerate}
\item Write $W$ mathematically in set notation.
\item Does $W$ have a supremum? Why or why not?
\item Is $\sup(W)=1$?  Why or why not?
\item Is $\sup(W)=4$?  Why or why not?
\end{enumerate}

\begin{framed}
\begin{enumerate}
\item $W=\{ x\in \R \ | \ x^3 + x < 10\}$.
\item Yes. It is nonempty, since $0\in W$, and bounded above, e.g., by $3$: if $x> 3$, then $x^3+ x > 3^3+3 = 30$, so $x\notin W$.
\item No: $1$ is not an upper bound, because $1.5\in W$.
\item No: $3$ is an upper bound, and $3<4$.
\end{enumerate}
\end{framed}

\item Use the Archimedean Principle to show that for any positive number $\e>0$, there is a natural number $n$ such that $ 0 < \e < \frac{1}{n}$.

\

\item Prove that the supremum of the set $S= \left\{ 1 - \frac1n \ | \ n\in \N\right\}$ is $1$.

\

\item Let $S$ be a set of real numbers, and suppose that $\sup(S)=\ell$. Let $T=\{s+7 \ | \ s\in S\}$. Prove that $\sup(T) = \ell + 7$.

\begin{framed}
First, we show that $\ell+7$ is an upper bound of $T$. Let $t\in T$. Then there is some $s\in S$ such that $t=s+7$. Since $s\leq \ell$, we have $t=s+7 < \ell+7$, so $\ell+7$ is indeed an upper bound. Next, let $b$ be an upper bound for $T$. We claim that $b-7$ is an upper bound for $S$. Indeed, if $s\in S$, then $s+7\in T$ so $s+7 \leq b$, so $s\leq b-7$. Then, by definition of supremum, we have $b-7\geq \ell$, eso $b\geq \ell+7$.
\end{framed}

\item Prove the following:
\begin{cor}[Density of irrational numbers] For any real numbers $x,y$ with $x<y$, there is some irrational number $z$ such that $x<z<y$.
\end{cor}
 
 \begin{framed}
Let $x<y$ be real numbers. Then we have $x-\sqrt{2} < y-\sqrt{2}$. By density of rationals, there is some rational number $q$ such that $x-\sqrt{2} < q < y-\sqrt{2}$. Then $x < q + \sqrt{2} < y$. Since $q$ is rational and $\sqrt{2}$ is irrational, $z=q+\sqrt{2}$ is irrational, and hence the number we seek.
\end{framed}

\item True or false \& justify: There is a rational number $x$ such that $|x^2 - 2| = 0$.

 \begin{framed}
False: this would imply that $x$ is a rational number whose square is $2$.
\end{framed}

\item True or false \& justify: There is a rational number $x$ such that $|x^2 - 2| < \frac{1}{1000000}$.

 \begin{framed}
 True: By density of rational numbers, there is a rational number $q$ such that $\sqrt{2}- \frac{1}{5000000} < q < \sqrt{2}$. Then 
\begin{align*} |x^2- 2| &= |x-\sqrt{2} | \, |x+\sqrt{2}| \\&< \frac{1}{5000000} \left( \frac{1}{5000000} + 4\right) \\&< \frac{1}{5000000} \cdot 5 \\&= \frac{1}{1000000}.\end{align*}
\end{framed}

\end{enumerate}



\Sept{13}


We now turn our attention to the next major topic of this class: sequences of real numbers. We will spend the next few weeks developing their properties carefully and
rigorously. Sequences form the foundation for much of what we will cover for the rest of the semester. 


\begin{defn} A {\em sequence} is an infinite list of real numbers indexed by $\N$:
$$
a_1, a_2, a_3, \dots.
$$
(Equivalently, a sequence is a function from $\N$ to $\R$: the value of the function at $n \in \N$ is written as $a_n$.)

We will usually write $\{a_n\}_{n=1}^\infty$ for a sequence.
\end{defn}

\begin{ex} To describe sequences, we will typically give a formula for the $n$-th term, $a_n$, either an explicit one or a recursive one. On rare occasion we'll
  just list enough terms to make the pattern clear. Here are some examples:
\begin{enumerate}

\item $\{5 + (-1)^n \frac{1}{n}\}_{n=1}^\infty$ is the sequence that starts
$$
4, \frac{11}2,  \frac{14}{3}, \frac{21}4, \frac{24}5, \dots.
$$

\item Let $\{a_n\}_{n=1}^\infty$ be defined by $a_1 = 1, a_2 = 1$ and $a_n = a_{n-1} + a_{n-2}$ for all $n \geq 2$. This gives the sequence
$$
1, 1, 2, 3, 5, 8, 13, 21, 34, \dots
$$
This is an example of a recursively defined sequence. It is the famed {\em Fibonacci sequence}. 

\item Let $\{c_n\}_{n=1}^\infty$ be the sequence whose $n$-th term is the $n$-th smallest positive prime integer:
$$
2, 3, 5, 7, 11, 13, 17, 19, 23, \dots.
$$
Note that here I have not really given an explicit formula for the terms of the sequence, but it is possible to describe an algorithm that lists every term of the sequence in
order.
\end{enumerate}

\end{ex}


You have all probably seen an ``intuitive'' definition of the limit of a sequence before. For example, you probably believe that
$$
5 + (-1)^n \frac{1}{n}
$$
converges to $5$.
Let's give the rigorous definition.

\begin{defn} Let $\{a_n\}_{n=1}^\infty$ be an arbitrary sequence and $L$ a real number. We say $\{a_n\}_{n=1}^\infty$ {\em converges} to $L$ provided the
  following condition is met:
\begin{quote}
For every real number $\e > 0$, there is a real number $N$ such that $|a_n - L| < \e$ for all natural numbers $n$ such that $n > N$.
\end{quote}
\end{defn}

This is an extremely important definition for this class. Learn it by heart!





In symbols, the definition is 
\begin{quote}
A sequence $\{a_n\}_{n=1}^\infty$ converges to $L$ provided\\
$\forall \e >0, {\exists N \in \R} : {\forall n \in \N} \text{ s.t. } n > N, \ {|a_n - L| < \e}$. 
\end{quote}

It's a complicated definition --- three quantifiers!


Here is what the definition is saying somewhat loosely: No matter how small a number $\e$ you pick, so long as it is positive, if you go far enough out in the sequence, all of
the terms from that point on will be within a distance of $\e$ of the limiting value $L$.

\begin{ex} To say that the sequence $\{a_n\}_{n=1}^\infty$ where $a_n =  5 + (-1)^n \frac{1}{n}$ converges to $5$ gives us a different statement for every $\e>0$. For example:
\begin{itemize}
\item Setting $\e=3$, there is a number $N$ such that for every natural number $n>N$, $|a_n-5|<3$. Namely, we can take $N=0$, since for \emph{every} term $a_n$ of the sequence, $|a_n-5|<3$ holds true.
\item Setting $\e=\frac13$, there is a number $N$ such that for every natural number $n>N$, $|a_n-5|<\frac13$. We cannot take $N=0$ anymore, since $1>0$ and $|a_1-5|=1>\frac13$. However, we can take $N=3$, since for $n>3$, $|a_n-5|=\frac1n <\frac13$.
\item Setting $\e=1/1000000$, there is a number $N$ such that for every natural number $n>N$, $|a_n-5|<1/1000000$. We need a bigger $N$; now $N=1000000$ works.
\end{itemize}

In general, our choice of $N$ may depend on $\e$, which is OK since our definition is of the form $\forall \e>0,\exists N\dots$ rather than $\exists N:\forall \e>0\dots$.
\end{ex}



\begin{ex} I claim the sequence $\{a_n\}_{n=1}^\infty$ where $a_n =  5 + (-1)^n \frac{1}{n}$ converges to $5$. I'll give a rigorous proof, along with some commentary and
  ``scratch work'' within the parentheses. 

\begin{proof} Let $\e > 0$ be given. 

(Scratch work: Given this $\e$, our goal is to find $N$ so that if $n > N$, then 
$|5 + (-1)^n \frac{1}{n} - 5| < \e$. The latter simplifies to
$\frac{1}{n} < \e$, which in turn is equivalent to $\frac{1}{\e} < n$ since
$\e$ and $n$ are both positive.  So, it seems we've found the $N$ that
``works''. Back to the formal proof....)


Let $N = \frac{1}{\e}$. Then $\frac{1}{N}
= \e$, since $\e$ is positive.  

(Comment: We next show that this is the $N$ that ``works'' in
the definition. Since this involves proving something about every natural number that is bigger than $N$, 
we start by picking one.) 

Pick any $n \in \N$ such that  $n >
N$. Then $\frac{1}{n} < \frac{1}{N}$ and hence
$$
|a_n - 5| = |5 + (-1)^n \frac{1}{n} - 5| = 
|(-1)^n \frac{1}{n}| = \frac{1}{n} < \frac{1}{N} = \e.
$$
This proves that 
$\{5 + (-1)^n \frac{1}{n}\}_{n=1}^\infty$ converges to $5$.
\end{proof}
\end{ex}


\begin{rem} A direct proof that a certain sequence converges to a certain number follows the general outline:
\begin{itemize}
	\item Let $\e > 0$ be given. (or, if your prefer, ``Pick $\e > 0$.")
	\item Let $N =$ [expression in terms of $\e$ from scratch work].
	\item Let $n \in \N$ be such that $n > N$.
\item $\text{[Argument that $|a_n-L|<\e$.]}$
	\item Thus $\{a_n\}_{n=1}^{\infty}$ converges to $L$.
\end{itemize}
\end{rem}



\begin{ex} I claim that the sequence
$$
\left\{ \frac{2n - 1}{5n + 1} \right\}_{n=1}^\infty
$$
converges to $\frac{2}{5}$.  Again I'll give a proof with commentary and scratch work in parentheses.

\begin{proof} 
Let $\e > 0$ be given. 

(Scratch work: We need $n$ to be large enough so that
$$
\left|\frac{2n - 1}{5n + 1} - \frac{2}{5}\right| < \e.
$$
This simplifies to 
$\left|\frac{-7}{25n + 5}\right| < \e$
and thus to
$\frac{7}{25n + 5} < \e$, which we can rewritten as $\frac{7}{25\e} - \frac15 < n$.)

Let $N = \frac{7}{25\e} - \frac15$. We solve this equation for $\e$: We get
$\frac{7}{25 \e} = \frac{5N +  1}{5}$ and hence 
$\frac{25 \e}{7} = \frac{5}{5N +  1}$, which gives finally
$$
\e = \frac{7}{25N + 5}.
$$

(Next we show this value of $N$ works....)

Now pick any $n \in \N$ is such that
$n > N$. Then
$$
\left|\frac{2n - 1}{5n + 1} - \frac{2}{5}\right| 
= \left|\frac{10n - 5   - 10n -2               }{25n + 5} \right| 
=  \frac{7}{25n + 5}.
$$
Since $n > N$, $25n + 5> 25N + 5$ and hence
$$
\frac{7}{25n + 5} < \frac{7}{25N + 5} = \e.
$$
We have proven that if $n \in \N$ and $n > N$, then
$$
\left|\frac{2n - 1}{5n + 1} - \frac{2}{5}\right| < \e.
$$
This proves $\left\{ \frac{2n - 1}{5n + 1} \right\}_{n=1}^\infty$
converges to $\frac25$.
\end{proof}
\end{ex}


\begin{comment}

\begin{defn} We say a sequence $\{a_n\}_{n=1}^\infty$ {\em converges} or {\em is convergent} 
if there is (at least one) number $L$ such that it converges to $L$. Otherwise, of no such $L$ exists, we say the sequence {\em diverges} or 
  {\em is divergent}.
\end{defn}

(We'll show soon that if a sequence converges to a number $L$, then $L$ is the {\em only} number to which in converges.)


\begin{ex} Let's prove the sequence $\{(-1)^n\}_{n=1}^\infty$ is divergent. This means that there is no $L$ to which it converges. 

\begin{proof}
  We proceed by contradiction: Suppose the sequence did converge to some number $L$. Our strategy will be to derive a contradiction by showing that such
  an $L$ would have to satisfy mutually exclusive conditions. 

By definition, since the sequence converges to $L$, we have that 
for every $\e > 0$ there is a number $N$ such that 
$|(-1)^n - L| < \e$
for all natural numbers $n$ such that $n > N$.
In particular, this statement is true for the particular value $\e = \frac12$. That is,  there is a number $N$ such that  
$|(-1)^n - L| < \frac12$ for all natural numbers $n$ such that  
$n > N$. Let $n$ be any even natural number that is bigger than
$N$. (Certainly one exists: we know there is an integer bigger than
$N$
by Theorem \ref{thm120}. Pick one. If it is even, take that to be
$n$. If it is odd, increase it by one to get an even integer $n$.)  Since $(-1)^n = 1$ for an even integer $n$, we get
$$
|1 - L| < \frac12
$$
and thus $\frac12 < L < \frac32$.

Likewise, let $n$ be an odd natural number bigger than $N$. Since $(-1)^n = -1$ for an odd integer $n$, we get
$$
|-1 - L| < \frac12
$$
and thus $-\frac32 < L < -\frac12$. But it cannot be that both $L > \frac12$ and $L < -\frac12$.

We conclude that no such $L$ exists; that is, this sequence is divergent.
\end{proof}
\end{ex}

\Sept{17}


\begin{prop} If a sequence converges, then there is a unique number to which it converges.
\end{prop}



\begin{proof} Recall that to show something satisfying certain properties is unique, one assumes there are two such things and argues that they must be equal. 
So,
suppose $\{a_n\}_{n=1}^\infty$ is a sequence that converges to $L$ and that also converges to $M$. We will prove $L = M$. 

By way of contradiction, suppose $L \ne M$. Then set $\e =
\frac{|L-M|}{3}$. Since we are assuming $L \ne M$, we have $\e > 0$.  
According to the definition of convergence, since the sequence converges to $L$, there is a real number $N_1$ such that for $n \in \N$ such that  $n > N_1$ we have
$$
|a_n - L| < \e.
$$

Also according to the definition, since the sequence converges to $M$, there is a real number $N_2$ such that for $n \in \N$ and  $n > N_2$ we have
$$
|a_n - M| < \e.
$$
Pick $n$ to be any natural number larger than $\max\{N_1, N_2\}$ (which exists by Theorem \ref{thm120}).   For such an $n$, both
$|a_n - L| < \e$ and $ |a_n - M| < \e$ hold. 
Using the triangle inequality and these two inequalities, we get
$$
|L-M| \leq |L-a_n| + |M-a_n| < \e + \e.
$$
But by the choice of $\e$, we have $\e + \e = \frac23 |L-M|$. That is, we have deduced that $|L-M| < \frac23 |L-M|$ which is impossible.
We conclude that $L = M$. 
\end{proof}




From now on, given a sequence $\{a_n\}_{n=1}^\infty$ and a real number $L$, 
will we use the short-hand notation
$$
\lim_{n \to \infty} a_n = L
$$
to mean that the given sequence converges to the given number. For example, we showed above that 
$$
\lim_{n \to \infty} \frac{2n - 1}{5n + 1}   = \frac25.
$$
But, to be clear, the statement ``$\lim_{n \to \infty} a_n = L$''
signifies nothing more and nothing less than the statement
``$\{a_n\}_{n=1}^\infty$ converges to $L$''.



Here is some terminology we will need:

\begin{defn} Suppose $\{a_n\}_{n=1}^\infty$ is any sequence. 

\begin{enumerate}

\item 
We say $\{a_n\}_{n=1}^\infty$ is {\em bounded above} if there  exists at least one real number $M$ such that $a_n \leq M$ for all $n   \in
\N$; 
we say $\{a_n\}_{n=1}^\infty$ is {\em bounded below} if there  exists at least one real number $m$ such that $a_n \geq m$ for all $n   \in
\N$; and we say 
$\{a_n\}_{n=1}^\infty$ is {\em bounded} if it is both bounded above and bounded below.

\item We say $\{a_n\}_{n=1}^\infty$ is {\em increasing} if for all $n \in \N$, $a_n \leq a_{n+1}$;
we say $\{a_n\}_{n=1}^\infty$ is {\em decreasing} if for all $n \in \N$, $a_n \geq a_{n+1}$; and we say
$\{a_n\}_{n=1}^\infty$ is {\em monotone} if it is either decreasing or increasing.

\item We say $\{a_n\}_{n=1}^\infty$ is {\em strictly increasing} if for all $n \in \N$, $a_n < a_{n+1}$. I leave the definition of
{\em strictly decreasing} and {\em strictly monotone} to your imaginations.
\end{enumerate}
\end{defn}


\begin{rem} Be sure to interpret ``monotone'' correctly. It means
$$
\left(\forall n \in \N, a_n \leq a_{n+1}\right) \text{ or }
\left(\forall n \in \N, a_n \geq a_{n+1}\right);
$$
it does {\em not} mean
$$
\forall n \in \N, \left( a_n \leq a_{n+1}\right) \text{ or } \left(a_n \geq a_{n+1}\right).
$$
Do you see the difference?
\end{rem}


\begin{ex} The sequence $\{\frac{1}{n}\}_{n=1}^\infty$ is strictly increasing and bounded (above by, e.g., $1$ and below by, e.g., $0$).

The Fibonacci sequence $\{f_n\}_{n=1}^{\infty} = 1,1,2,3,5,8,\dots$ is strictly increasing and bounded below, but not bounded above.

The sequence $\{5 + (-1)^n \frac{1}{n}\}_{n=1}^\infty$ is not monotone, but it is bounded (above by, e.g., $6$ and below by, e.g., $4$).

Is the sequence of quotients of Fibonacci numbers $\{\frac{f_{n+1}}{f_n}\}_{n=1}^{\infty} = \frac{1}{1}, \frac{2}{1}, \frac{3}{2}, \frac{5}{3},\frac{8}{5},\dots$
monotone? Is it bounded? Convergent?
\end{ex}


\begin{prop} \label{prop21}
	If a sequence $\{a_n\}_{n=1}^\infty$ converges then it is bounded.
\end{prop}

\begin{proof} Suppose the sequence $\{a_n\}_{n=1}^\infty$ converges to
	the number $L$. Applying the definition of ``converges to $L$'' using the particular value $\e = 1$ gives the following fact:
	There is a real number $N$ such that if $n \in \N$ and $n > N$, then $|a_n -
	L| < 1$. The latter inequality is equivalent to  $L-1 < a_n < L+1$
	for all $n > N$.
	
	Let $m$ be any natural number such that $m > N$,
	and consider the finite list of numbers
	$$
	a_1, a_2, \dots, a_{m-1}, L + 1.
	$$
	Let $b$ be the largest element of this list. I claim the sequence is bounded above by $b$.
	For any $n \in  \N$, if $1 \leq n \leq m-1$, then $a_n \leq b$ since in this case $a_n$ is a member of the above list and $b$ is the largest element of this list.
	If $n \geq m$ then since $m > N$, we have $n > N$ and hence $a_n < L + 1$ from above. We also have $L + 1 \leq b$ (since $L+1$ is in the list) and thus $a_n < b$. 
	This proves $a_n \leq b$ for all $n$ as claimed.
	
	Now take $p$ to be the smallest number in the list
	$$
	a_1, a_2, \dots, a_{m-1}, L - 1.
	$$
	A similar argument shows that $a_n \geq p$ for all $n \in \N$.
\end{proof}


\begin{rem}
The converse of the previous proposition is false; the sequence $\{(-1)^n\}_{n=1}^\infty$ is a counterexample.
\end{rem}



\Sept{20}

\begin{prop}\begin{enumerate}
\item If $c$ is any real number, then the constant sequence $\{ c\}_{n=1}^{\infty}$ converges to $c$.
\item The sequence $\{1/n\}_{n=1}^\infty$ converges to $0$.
\end{enumerate}
\end{prop}

\begin{thm}[Limits and algebra]\label{thm99}
Let $\{a_n\}_{n=1}^\infty$ be a sequence that converges to $L$, and $\{b_n\}_{n=1}^\infty$ be a sequence that converges to $M$.
\begin{enumerate}
\item If $c$ is any real number, then $\{ c a_n\}_{n=1}^\infty$ converges to $cL$.
\item The sequence $\{a_n + b_n\}_{n=1}^\infty$ converges to $L+M$.
\item The sequence $\{a_n b_n\}_{n=1}^\infty$ converges to $LM$.
\item If $L\neq 0$ and $a_n\neq 0$ for all $n\in \N$, then $\displaystyle\left\{\frac{1}{a_n}\right\}_{n=1}^\infty$\!\!\! converges to~$\displaystyle \frac{1}{L}$.
\item If $M\neq 0$ and $b_n\neq 0$ for all $n\in \N$, then $\displaystyle\left\{\frac{a_n}{b_n}\right\}_{n=1}^\infty$\!\!\! converges to~$\displaystyle \frac{L}{M}$.
\end{enumerate}
\end{thm}




\begin{enumerate}
\item Use the two results above to give a short proof\footnote{Hint: Rewrite $\displaystyle\frac{2n-1}{5n+1} = \frac{(2n-1)/n}{(5n+1)/n} = \frac{2-1/n}{5+1/n}$.} that $\displaystyle \left\{ \frac{2n-1}{5n+1} \right\}_{n=1}^\infty$\!\!\! converges~to~$\displaystyle  \frac{2}{5}$.

\begin{framed}
We can rewrite $\displaystyle\frac{2n-1}{5n+1} = \frac{(2n-1)/n}{(5n+1)/n} = \frac{2-1/n}{5+1/n}$. Since $\{1/n\}_{n=1}^\infty$ converges to $0$, $\{-1/n\}_{n=1}^\infty$ converges to $-1\cdot 0 = 0$ by Theorem 12.2(1). Since $\{2\}_{n=1}^\infty$ converges to $2$ and $\{-1/n\}_{n=1}^\infty$ converges to $0$, $\{2-1/n\}_{n=1}^\infty$ converges to $2$ by Theorem 12.2(2). Since $\{5\}_{n=1}^\infty$ converges to $5$ and $\{1/n\}_{n=1}^\infty$ converges to $0$, $\{5+1/n\}_{n=1}^\infty$ converges to $5$ by Theorem 12.2(2). Then, by Theorem 12.2(5), which applies since $5\neq 0$ and every term of $\{5+1/n\}_{n=1}^\infty$ is nonzero, the sequence converges to $2/5$.
\end{framed}

\item Prove part (1) of Proposition~12.1.

\begin{framed}
Let $\varepsilon>0$. Take $N=0$. For any natural number $n>N$, we have $|c-c|=0<\varepsilon$. This shows that the sequence converges to $c$.
\end{framed}

\item Prove part (1) of Theorem~12.2:
\begin{itemize}
\item First, assume that $c=0$. Explain why the result is true in this case.
\item Now, assume that $c\neq 0$. We need to prove that $\{ c a_n\}_{n=1}^\infty$ converges to $cL$. Write the first sentence of the proof of this.
\item We have assumed that $\{a_n\}_{n=1}^\infty$ converges to $L$. Explain what this means when applied to the positive number $\displaystyle \frac{\varepsilon}{|c|}$ (in the place of what we usually call $\varepsilon$).
\item Complete the proof.
\end{itemize}

\begin{framed}
First, if $c=0$, then $\{c a_n\}_{n=1}^\infty$ is the constant sequence $\{0\}_{n=1}^\infty$, which converges to $0 = 0\cdot L$, so the result holds in this case.

Now, let $\e>0$. Applying the definition of $\{a_n\}_{n=1}^\infty$ converges to $L$ with the positive number $\e/|c|$, there is some $N\in \R$ such that for all natural numbers $n>N$, we have $|a_n-L|<\e/|c|$. Then $|ca_n - cL| = |c| |a_n-L| < |c| \e/|c| = \e$. This shows that $\{ca_n\}_{n=1}^\infty$ converges to $cL$.
\end{framed}

\item Prove\footnote{Hint: Given $\varepsilon>0$, can you find $N$ such that $|a_n-L|<\varepsilon/2$ and $|b_n-M|<\varepsilon/2$ for all natural numbers $n>N$?} part (2) of Theorem~12.2.

\begin{framed}
Pick $\e > 0$. 	
	Since $\{a_n\}_{n=1}^\infty$ converges to $L$ and $\frac{\e}{2}$ is positive, 
	there is a number $N_1$ such that for all $n \in \N$ with $n > N_1$ we have
	$$
	|a_n - L| < \frac{\e}{2}.
	$$
	Likewise, 
	since $\{b_n\}_{n=1}^\infty$ converges to $M$,
	there is a number $N_2$ such that for all $n \in \N$ with $n > N_2$ we have
	$$
	|b_n - M| < \frac{\e}{2}.
	$$
	Let $N = \max\{N_1, N_2\}$. If $n \in \N$ and $n > N$, then $n > N_1$ and $n > N_2$ and hence we have
	$$
	|a_n - L| < \frac{\e}{2}
	\and
	|b_n - M| < \frac{\e}{2}.
	$$
	Using these inequalities and the triangle inequality we get
	$$\begin{aligned}
	|(a_n+b_n) - (M+L)| &= |(a_n-M) + (b_n -L)| \\&\leq |(a_n-M)| + |(b_n -L)| < \frac{\e}{2} + \frac{\e}{2}  = \e.\end{aligned}
	$$
	This proves that $\{a_n  + b_n\}_{n=1}^\infty$ converges to $L + M$.
\end{framed}

\item Prove\footnote{Hint: Write $|a_n b_n - L M| = |a_n(b_n -M) + M(a_n - L)|$ and apply the triangle inequality. Since $\{a_n\}_{n=1}^\infty$ converges, it is bounded.}  part (3) of Theorem~12.2.

\begin{framed}
Pick $\e > 0$.
	
	(``Scratch work'': The goal is to make 
	$|a_nb_n - LM|$ small and the trick is to use that
	$$
	\begin{aligned}
	|a_nb_n - LM| & = |a_n(b_n - M) + (a_n - L)M|  \\
	& \leq |a_n(b_n - M)| + |(a_n - L)M| \\
	& = |a_n| |b_n - M| + |a_n - L| |M|.
	\end{aligned}
	$$
	Our goal will be to take $n$ to be large enough so that each of $|a_n| |b_n - M|$ and $|a_n - L| |M|$ is smaller than $\e/2$.
	We can make $|a_n -L|$ as small as we like and $|M|$ is just a fixed number. So, we can ``take care'' of the second term by choosing $n$ big enough so
	that $|a_n - L| < \frac{\e}{2|M|}$. A irritating technicality here is
	that $|M|$ could be $0$, and so we will use $\frac{\e}{2|M|+1}$ instead. 
	The other term   $|a_n| |b_n - M|$ is harder to deal with since each factor varies with $n$. 
	Here we use that convergent sequences are bounded so that we can find a real number $X$ so that $|a_n| \leq X$ for all $n$. Then we choose $n$ large enough
	so that $|b_n - M| < \frac{\e}{2 X}$. Back to the proof.)
	
	Since $\{a_n\}$ converges, it is bounded by Proposition \ref{prop21}, which gives that  there is a strictly positive real number $X$ so that $|a_n| \leq X$ for all $n \in \N$.
	Since $\{b_n\}$
	converges to $M$ and $\frac{\e}{2 X} > 0$, there is a number $N_1$ so that if $n > N_1$ then $|b_n - M| < \frac{\e}{2 X}$. Since $\{a_n\}$ converges to $L$
	and $\frac{\e}{2|M| + 1} > 0$, there is a number
	$N_2$ so that if $n \in \N$ and $n > N_2$, then $|a_n - L| < \frac{\e}{2|M| + 1}$. Let $N = \max\{N_1, N_2\}$. For any $n \in \N$ such that $n > N$, we have
	$$
	\begin{aligned}
	|a_nb_n - LM| & = |a_n(b_n - M) + (a_n - L)M| \\
	& \leq |a_n(b_n - M)| + |(a_n - L)M| \\
	& = |a_n| |b_n - M| + |a_n - L| |M| \\
	& < X \frac{\e}{2 X} + \frac{\e}{2|M| + 1} |M| \\
	& < \e.
	\end{aligned}
	$$
	This proves $\{a_n  \cdot b_n\}_{n=1}^\infty$ converges to $L \cdot M$.
	\end{framed}
\end{enumerate}

Here is the proof of parts (4) and (5) of the theorem:
\begin{proof}
We start with (4).

To prove this claim,  pick $\e > 0$. 
	
	(Scratch work: We want to show $\left|\frac{1}{a_n} -
	\frac{1}{L}\right| < \e$ holds for $n$ sufficiently large.  
	We have
	$$
	\left|\frac{1}{a_n} - \frac{1}{L}\right| = \frac{|L - a_n|}{|a_n||L|}.
	$$ 
	We can make the top of this fraction as small as we like, but the problem is that the
	bottom might be very small too since $a_n$ might get very close to $0$. But since $a_n$ converges to $L$ and $L \ne 0$ if we go far enough out, it will be
	close to $L$. In particular, if $a_n$ is within a distance of
	$\frac{|L|}{2}$ of $M$ then $|a_n|$ will be at least
	$\frac{|L|}{2}$. So for $n$ sufficiently large we have
	$\frac{|a_n-L|}{|a_n||L|} < 2 \frac{|a_n-L|}{|L|^2}$. And then for $n$
	sufficiently large we also get $|a_n - L| < \frac{|L|^2}{2 \e}$.  Back to the formal proof\dots)
	
	
	
	Since $\{a_n\}$ converges to $L$ and $\frac{|L|}{2} > 0$, there is an $N_1$ such that for $n > N_1$ we have $|a_n - L| < \frac{|L|}{2}$  and hence
	$|a_n| > \frac{|L|}{2}$. Again using that $\{a_n\}$ converges to $M$
	and that $\frac{ \e |L|^2}{2} > 0$, there is an $N_2$ so that for $n > N_2$ we have
	$|a_n - L| < \frac{ \e |L|^2}{2}$. Let $N = \max\{N_1, N_2\}$. If $n > N$, then we have
	$$
	\begin{aligned}
	\left|\frac{1}{a_n} - \frac{1}{L}\right| & = \frac{|a_n - L|}{|a_n||L|} \\
	& < \frac{2}{|L|}  \frac{|a_n - L|}{|L|} \\
	& = 2 \frac{|a_n - L|}{|L|^2} \\
	\end{aligned}
	$$
	since $|a_n| > |L|/2$ and hence $\frac{1}{|a_n|} < \frac{2}{|L|}$.
	But then
	$$
	2 \frac{|a_n - L|}{|L|^2}  < 2 \frac{\frac{\e |L|^2}{2}}{|L|^2} = \e
	$$
	since $|a_n - L| < \frac{ \e |L|^2}{2}$. Putting these together gives
	$$
	\left|\frac{1}{a_n} - \frac{1}{L}\right| < \e
	$$
	for all $n > N$. 
	This proves $\{\frac{1}{a_n}\}_{n=1}^\infty$ converges to $\frac{1}{L}$.

Finally, part (5) follows from parts (3) and (4).
\end{proof}


%\begin{comment}
\Sept{22}


The following is another useful technique:

\begin{thm}[The ``squeeze'' principle] \label{thm33}
Suppose 
$\{a_n\}_{n=1}^\infty$, $\{b_n\}_{n=1}^\infty$, and $\{c_n\}_{n=1}^\infty$ are three sequences such that 
\begin{itemize}
\item $\{a_n\}_{n=1}^\infty$ converges to $L$, 
\item $\{c_n\}_{n=1}^\infty$ also converges to $L$ (same value), and
\item there is a real number $M$ such that $a_n \leq b_n \leq c_n$ for all $n \in \N$ such that $n > M$. 
\end{itemize}
Then $\{b_n\}_{n=1}^\infty$ also converges to $L$.
\end{thm}

The heuristic version of this theorem is:
\begin{quote}
If $\lim_{n \to \infty} a_n = L = \lim_{n \to \infty} c_n$ and $b_n$ is ``eventually'' between $a_n$ and $c_n$, then
$\lim_{n \to \infty} b_n = L$ too.
\end{quote}
 



\begin{proof}
Assume $\{a_n\}_{n=1}^\infty$ and  $\{c_n\}_{n=1}^\infty$ both
converge to $L$ and that there is a real number $M$ such that $a_n \leq b_n \leq c_n$ for all $n \in \N$ such that $n > M$. 
We need to prove $\{b_n\}_{n=1}^\infty$ converges to $L$.

Pick $\e> 0$.  Since  $\{a_n\}_{n=1}^\infty$ converges to $L$ there is a number $N_1$ such that if $n \in \N$ and $n > N_1$ then
$|a_n - L| < \e$ and hence $L- \e < a_n < L + \e$. Likewise, 
since $\{c_n\}_{n=1}^\infty$ converges to $L$ there is a number $N_2$ such that if $n \in \N$ and $n > N_2$ then
 $L- \e < c_n < L + \e$. Let 
$$
N = \max\{N_1, N_2, M\}
$$
where $M$ is defined as in the statement of the Theorem. If $n \in \N$ and $n > N$, then
$n > N_1$ and hence $L - \e < a_n$, and $n > N_2$ and hence $c_n < L + \e$, and $n > M$ and hence
$a_n < b_n < c_n$. Combining these facts gives that for $n \in \N$ such that $n > N$,
we have
$$
L - \e < b_n < L + \e
$$
and hence $|b_n - L| < \e$.
This proves $\{b_n\}_{n=1}^\infty$ converges to $L$. 
\end{proof}

%\begin{comment}

\Sept{24}

\begin{ex} We can use the Squeeze Theorem to give a short proof that $\{ 5 + (-1)^n \frac{1}{n} \}_{n=1}^\infty$ converges to $5$. Note that Theorem \ref{thm99} alone cannot be used in this
  example. However, from Theorem~\ref{thm99}, it follows that $\{5-\frac{1}{n}\}_{n=1}^\infty$ and  $\{5+\frac{1}{n}\}_{n=1}^\infty$ both converge to $5$. Then, since \[5-\frac{1}{n} \leq  5 + (-1)^n \frac{1}{n} \leq 5+\frac{1}{n}\] for all $n$, our sequence also converges to $5$.\end{ex}



%Here is a corollary of the Squeeze Theorem that is sometimes handy.

%\begin{cor} If $\{|a_n|\}_{n=1}^\infty$ converges to $0$, then $\{ a_n\}$ converges to~$0$.
%\end{cor}
%\begin{proof}
%By Theorem~\ref{thm99}(1), $\{-|a_n|\}_{n=1}^\infty$ converges to $0$ as well. We have 
%\[ -|a_n| \leq a_n \leq |a_n|\]
%for all $n$, so by the Squeeze Theorem, $\{ a_n\}$ converges to $0$.
%\end{proof}

%\begin{cor}\begin{enumerate}
%		\item 	If  the sequence $\{a_n\}_{n=1}^\infty$ converges to $0$, then the sequence
%		$\{|a_n|\}_{n=1}^\infty$ also converges to $0$.
%		\item If $\{a_n\}_{n=1}^\infty$ converges to $0$ and $\{b_n\}_{n=1}^\infty$ is any bounded sequence, then $\{a_n b_n\}_{n=1}^\infty$ converges to $0$.
%	\end{enumerate}
%\end{cor}
%\begin{proof} 
%	\begin{enumerate}
%		\item Assume $\{a_n\}_{n=1}^\infty$ converges to $0$.  We need to prove $\{|a_n|\}_{n=1}^\infty$ converges to $0$. 
%		Pick $\e > 0$. Since 
%		$\{a_n\}_{n=1}^\infty$ converges to $0$, there is a number $N$ such that if $n \in \N$ and $n > N$, then $|a_n - 0| < \e$. For this same $N$, if $n > N$ then
%		$$
%		||a_n| - 0| = ||a_n|| = |a_n| < \e.
%		$$
%		This proves $\{|a_n|\}_{n=1}^\infty$ converges to $0$. 
%		\item Since $\{b_n\}$ is bounded, there is a positive real number $X$ such that $|b_n| \leq X$ for all $n$. Thus $0 \leq |a_n b_n| \leq X |a_n|$ holds
%		for all $n$ and hence
%		$$
%		- X |a_n| \leq a_n b_n \leq X |a_n|
%		$$
%		holds for all $n$. By the Lemma, since  $\{a_n\}_{n=1}^\infty$ converges to $0$, so does $\{|a_n|\}_{n=1}^\infty$. Using 
%		Theorem \ref{thm99}, we get that 
%		$\{X|a_n|\}_{n=1}^\infty$ and $\{-X|a_n|\}_{n=1}^\infty$ also both converge to $0$. Finally, by the Squeeze Theorem, $\{a_n b_n\}_{n=1}^\infty$ converges to $0$ too. \qedhere
%	\end{enumerate}
%\end{proof}
%
%
%\begin{rem} More generally, if $\{a_n\}_{n=1}^\infty$ converges to $L$, then the sequence
%  $\{|a_n|\}_{n=1}^\infty$ also converges to $|L|$, but I will not take
%  the time to prove this now. The converse of this statement is false
%  however. For example, consider the sequence $\{(-1)^n\}_{n=1}^\infty$.
%  The sequence $\{|(-1)^n|\}_{n=1}^\infty$ is the constant sequence $1$ and hence it converges to $1$, but the original sequence diverges.
%\end{rem}
%
%
%
%\begin{ex} This Corollary gives another way to prove $\{(-1)^n/n\}$ converges to $0$: take $b_n = (-1)^n$ and $a_n = 1/n$.
%  \end{ex}



 
When I introduced the Completeness Axiom, I mentioned that, heuristically, it is what tells us that the real number line doesn't have any
holes. The next result makes this a bit more precise:

\begin{thm}\label{thm216}
Every increasing, bounded above sequence converges.
\end{thm}



\begin{proof} Let $\{a_n\}_{n=1}^\infty$ be any sequence that is both  bounded above and increasing.

(Commentary: In order to prove it converges, we need to find a candidate number $L$ that
  it converges to. Since the set of numbers occurring in this sequence is nonempty and bounded above, this number is provided to us by the Completeness Axiom.) 

Let $S$ be the set of those real numbers that occur in this sequence. (This is technically different that the sequence itself, since sequences are allowed to
have repetitions but sets are not. Also, sequences have an ordering to
them, but sets do not.) 
The set $S$ is clearly
  nonempty, and it is bounded above since we assume the sequence is bounded above. Therefore, by the Completeness Axiom, $S$ has a supremum $L$. We will prove the
  sequence converges to $L$.

Pick $\e > 0$. Then $L - \e < L$ and, since $L$ is the supremum, $L - \e$ is not an upper bound of $S$. This means that 
there is an element of $S$ that is strictly bigger than $L-
\e$. Every element of $S$ is a member of the sequence, and so we get that there is an $N \in \N$ such that $a_N > L - \e$. 

(We will next show that this is the $N$ that ``works''. Note that, in the general definition of convergence of a sequence, $N$ can be any real number, but in this proof it turns
out to be a natural number.)

Let $n$ be any natural number such that $n > N$. Since the sequence is increasing, $a_N \leq a_n$ and hence
$$
L - \e < a_N \leq a_n.
$$
Also, $a_n \leq L$ since $L$ is an upper bound for the sequence, and thus we have
$$
L- \e < a_n \leq L.
$$
It follows that  $|a_n - L| < \e$. We have proven the sequence converges to~$L$.
\end{proof}

\begin{rem} Note that any increasing sequence is bounded below, for example, by its first term. Thus, an increasing sequence is bounded if and only if it is bounded below. Likewise, and decreasing sequence is bounded if and only if it is bounded above.
\end{rem}

\begin{thm}[Monotone Converge Theorem]\label{thm:MCT} Every bounded monotone sequence converges.
\end{thm}
\begin{proof}
If $\{a_n\}_{n=1}^\infty$ is increasing, then this is the content of Theorem~\ref{thm226}. If $\{a_n\}_{n=1}^\infty$ is decreasing and bounded, consider the sequence $\{-a_n\}_{n=1}^\infty$. If $a_n\leq M$ for all $n$, then $-a_n\geq -M$ for all $n$, so $\{-a_n\}_{n=1}^\infty$ is bounded below. Also, since $a_n \geq a_{n+1}$ for all $n$, we have $-a_n \leq -a_{n+1}$ for all $n$, so $\{-a_n\}_{n=1}^\infty$ is increasing. Thus, by Theorem~\ref{thm226}, $\{-a_n\}_{n=1}^\infty$ converges, say to $L$. Then by Theorem~\ref{thm99}(1), $\{a_n\}_{n=1}^\infty=\{-(-a_n)\}_{n=1}^\infty$ converges to $-L$.
\end{proof}

\begin{ex}
	Consider the sequence $\{a_n\}_{n=1}^\infty$ given by the formula
	\[ a_n = 1 + \frac{1}{2^2} + \frac{1}{3^2} + \cdots + \frac{1}{n^2}.\]
	We will use the Monotone Convergence Theorem to prove that this sequence converges.
	
	First, we need to see that the sequence is increasing. Indeed, for every $n$ we have that $a_{n+1} = a_n + \frac{1}{a_{n+1}^2} \geq a_n$.
	
	Next, we need to show that it is bounded above. Observe that
	\begin{align*} 
a_n &= 1 + \frac{1}{2^2} + \frac{1}{3^2} + \cdots + \frac{1}{n^2} \\
&\leq 1 + \frac{1}{1 \cdot 2} + \frac{1}{2 \cdot 3} + \cdots + \frac{1}{(n-1) n}\\
&= 1+ (\frac{1}{1} - \frac{1}{2}) + (\frac{1}{2} - \frac{1}{3})
 + \cdots +  (\frac{1}{n-1} - \frac{1}{n})\\
 &= 1 + 1 - \frac{1}{n},
		\end{align*}
		so we have $a_n \leq 2$ for all $n$. This means that $\{a_n\}_{n=1}^\infty$ is bounded above by $2$. 
		
		Hence, by the Monotone Convergence Theorem, $\{a_n\}_{n=1}^\infty$ converges. Leonhard Euler was particularly interested in this sequence, and was able to prove that it converges to $\frac{\pi^2}{6}$. This requires some other ideas, so we won't do that here.
	\end{ex}
	

%As we have seen, proving sequences converge using just the definition can be tedious and hard, and finding limits can be tricky. The next very long Theorem will make the task easier in some cases.
%
%
%\begin{thm} \label{thm99}
%The following six things hold. 
%\begin{enumerate}
%\item
% For any real number $c$, the constant sequence $\{a_n\}_{n=1}^\infty$ defined by $a_n = c$
%  converges to $c$. 
%
%\item
%The sequence $\{1/n\}_{n=1}^\infty$ converges to $0$.
%\end{enumerate}
%
%
%For the remaining parts, assume
%$\{a_n\}_{n=1}^\infty$ and $\{b_n\}_{n=1}^\infty$ are any two sequences that both converge. 
%\begin{enumerate}
%\item[(3)] 
%The sequence $\{a_n  + b_n\}_{n=1}^\infty$ also converges and
%$$
%\lim_{n \to \infty} (a_n + b_n) = 
%\lim_{n \to \infty} (a_n) + \lim_{n \to \infty} (b_n).
%$$
%
%\item[(4)] For any real number $c$, the sequence $\{c \cdot a_n\}_{n=1}^\infty$ also converges and
%$$
%\lim_{n \to \infty} (c \cdot a_n) =
%c \cdot \lim_{n \to \infty} (a_n).
%$$
%
%\item[(5)] 
%The sequence $\{a_n  \cdot b_n\}_{n=1}^\infty$ also converges and
%$$
%\lim_{n \to \infty} (a_n \cdot b_n) = 
%\lim_{n \to \infty} (a_n) \cdot \lim_{n \to \infty} (b_n).
%$$
%
%\item[(6)] If $b_n \ne 0$ for all $n$ and $\lim_{n \to \infty} (b_n) \ne 0$, then
%the sequence $\{a_n/b_n\}_{n=1}^\infty$ also converges and
%$$
%\lim_{n \to \infty} \left(\frac{a_n}{b_n}\right) = 
%\frac{\lim_{n \to \infty} (a_n)}{\lim_{n \to \infty} (b_n)}.
%$$
%
%\end{enumerate}
%\end{thm}
%
%
%
%Note that in the last item of the Theorem, we have to assume $b_n \ne 0$ for all $n$ (in order that the sequence 
%$\{a_n/b_n\}_{n=1}^\infty$ be well defined), and we also have to assume $\lim_{n \to \infty} (b_n) \ne 0$ (so that the right-hand side makes sense). The latter assumption does not
%follow from the former: for example, if $b_n =
%\frac{1}{n}$ then $b_n \ne 0$ for all $n$ but $\lim_{n \to \infty} b_n = 0$.
%
%
%
%\begin{ex} Before proving (parts of) the theorem, let us illustrate it by redoing our justification that the sequence 
%$\left\{ \frac{2n - 1}{5n + 1} \right\}_{n=1}^\infty$ converges to $\frac{2}{5}$. At first blush, this looks to be impossible since
%$\{2n - 1\}_{n=1}^\infty$ does not converge and so the hypotheses are not met. The trick is to first rewrite the $n$-th term as
%$$
% \frac{2n - 1}{5n + 1} = \frac{2 - 1/n}{5 + 1/n}.
%$$
%
%By the Theorem, Part (2) the sequence $\{1/n\}_{n=1}^\infty$ converges to $0$ and by Part (1) the constant sequence $5$ converges to $5$. So, by applying Part (3) of the theorem 
%we deduce that $\{5 + 1/n\}$ converges to $5$. Similarly, Parts (2) and (3) give that $\{-1/n\}_{n=1}^\infty$ converges to $0$ and so by Parts (1) and (3), $\{2 -
%1/n\}_{n=1}^\infty$
%converges to $2$. Finally, by applying Part (6) of Theorem we conclude that 
%$\left\{ \frac{2n - 1}{5n + 1} \right\}_{n=1}^\infty$ converges to $\frac25$.
%\end{ex}
%
%\section*{Discussion Questions, February 24}
%
%\begin{itemize}
%	\item Let $c$ be a real number, and $\{c\}_{n=1}^\infty$ be the sequence where every term is equal to $c$. Prove that this sequence converges to $c$.
%	
%	\
%	
%	\item Prove that the sequence $\{ 1/n\}_{n=1}^\infty$ converges to $0$.
%	
%	\
%	
%	\item Let $\{a_n\}_{n=1}^\infty$ be a sequence that converges to $L$, and $\{b_n\}_{n=1}^\infty$ be a sequence that converges to $M$. Prove that $\{a_n + b_n\}_{n=1}^\infty$ converges to $L+M$.
%	
%	[Hint: Given $\e >0$, apply the definitions of ``$\{a_n\}_{n=1}^\infty$ converges to $L$'' and ``$\{b_n\}_{n=1}^\infty$ converges to $M$'' with the value $\frac{\e}{2}$.]
%	
%	\
%	
%	\item Let $\{a_n\}_{n=1}^\infty$ be a sequence that converges to $L$, and $c$ be a real number. Prove that $\{c a_n\}_{n=1}^\infty$ converges to $cL$.
%	
%	\
%	
%	\item Let $\{a_n\}_{n=1}^\infty$ be a sequence that converges to $L$. Assume that $a_n\neq 0$ for all $n\in \N$ and that $L\neq 0$. Prove that $\{{1}/{a_n}\}_{n=1}^\infty$ converges to ${1}/{L}$.
%\end{itemize}
%
%
%All of these are parts of~Theorem~\ref{thm99}. Here is a proof of all of the parts of Theorem~\ref{thm99}, with scratch work; the numbering is that of Theorem~\ref{thm99} rather than the discussion questions.
%
%\begin{proof}  To prove (1), pick $\e > 0$. Let $N = 0$
%	(or, really, any number you want). If
%	$n \in \N$ and $n > N$, then $|a_n - c| = |c - c| = 0 < \e$ and hence the
%	constant sequence $\{c\}_{n=1}^\infty$ converges to $c$.
%	
%	To prove (2), we pick $\e > 0$. Let $N = \frac{1}{\e}$. If $n \in \N$
%	and $n > N$, then
%	$$
%	|\frac{1}{n} - 0| = \frac{1}{n} > \frac{1}{N} = \e
%	$$
%	and thus $\{1/n\}_{n=1}^\infty$ converges to $0$.
%	
%	
%	For the rest of this proof, assume $\{a_n\}_{n=1}^\infty$ converges to $L$ and $\{b_n\}_{n=1}^\infty$ converges to $M$. 
%	
%	For Part (3),
%	we need to prove $\{a_n  + b_n\}_{n=1}^\infty$ converges to $L + M$.
%	Pick $\e > 0$. 
%	
%	(``Scratch work'':
%	We need to figure out how big $n$ needs to be in order that
%	$|(a_n+b_n) - (M+L)| < \e$. Note that
%	$|(a_n+b_n) - (M+L)| =  |(a_n-M) + (b_n -L)| \leq |(a_n-M)| + |(b_n -L)|$ by the triangle inequality. 
%	Intuitively, we can make each of $|(a_n-M)|$ and $|(b_n -L)|$ as small as we like by taking $n$ large enough. We need their sum to be smaller than $\e$
%	and so  if we can make each of them be smaller that $\e/2$, we're golden. Back to the proof...)
%	
%	
%	Since $\{a_n\}_{n=1}^\infty$ converges to $L$ and $\frac{\e}{2}$ is positive, 
%	there is a number $N_1$ such that for all $n \in \N$ with $n > N_1$ we have
%	$$
%	|a_n - L| < \frac{\e}{2}.
%	$$
%	Likewise, 
%	since $\{b_n\}_{n=1}^\infty$ converges to $M$,
%	there is a number $N_2$ such that for all $n \in \N$ with $n > N_2$ we have
%	$$
%	|b_n - M| < \frac{\e}{2}.
%	$$
%	Let $N = \max\{N_1, N_2\}$. If $n \in \N$ and $n > N$, then $n > N_1$ and $n > N_2$ and hence we have
%	$$
%	|a_n - L| < \frac{\e}{2}
%	\and
%	|b_n - M| < \frac{\e}{2}.
%	$$
%	Using these inequalities and the triangle inequality we get
%	$$
%	|(a_n+b_n) - (M+L)| = |(a_n-M) + (b_n -L)| \leq |(a_n-M)| + |(b_n -L)| < \frac{\e}{2} + \frac{\e}{2}  = \e.
%	$$
%	This proves that $\{a_n  + b_n\}_{n=1}^\infty$ converges to $L + M$.
%	
%	Part (4) will follow from parts (1) and (5) put together, but we can also prove it on its own. We note first that if $c=0$, then $c a_n= 0$ for all $n$, and hence by part (1), we have $\{c a_n\}_{n=1}^\infty$ converges to $0$ in this case. Now, assume that $c\neq 0$, and let $\e>0$ be given.
%	
%	(``Scratch work'': We need $n$ to be large enough so that $|c a_n - c L|<\e$. We can write $|c a_n - c L| = |c| |a_n - L|$, so if $|a_n - L| < \frac{\e}{|c|}$, we'll be set.)
%	
%	By definition of convergence, there is some $N\in \R$ such that $|a_n - L| < \frac{\e}{|c|}$ for all natural numbers $n>N$. (Note here that it is important that $c\neq 0$; this is why we singled out the case $c=0$ first.) Then, for all natural numbers $n>N$, we have $|ca_n  - cL| = |c| |a_n - L| < |c| \e / |c| = \e$, as required.
%	
%	For (5),  we need to prove $\{a_n  \cdot b_n\}_{n=1}^\infty$ converges to $L \cdot M$.
%	Pick $\e > 0$.
%	
%	(``Scratch work'': The goal is to make 
%	$|a_nb_n - LM|$ small and the trick is to use that
%	$$
%	\begin{aligned}
%	|a_nb_n - LM| & = |a_n(b_n - M) + (a_n - L)M|  \\
%	& \leq |a_n(b_n - M)| + |(a_n - L)M| \\
%	& = |a_n| |b_n - M| + |a_n - L| |M|
%	\end{aligned}
%	$$
%	Our goal will be to take $n$ to be large enough so that each of $|a_n| |b_n - M|$ and $|a_n - L| |M|$ is smaller than $\e/2$.
%	We can make $|a_n -L|$ as small as we like and $|M|$ is just a fixed number. So, we can ``take care'' of the second term by chooseing $n$ big enough so
%	that $|a_n - L| < \frac{\e}{2|M|}$. A irritating technicality here is
%	that $|M|$ could be $0$, and so we will use $\frac{\e}{2|M|+1}$ instead. 
%	The other term   $|a_n| |b_n - M|$ is harder to deal with since each factor varies with $n$. 
%	Here we use that convergent sequence are bounded so that we can find a real number $X$ so that $|a_n| \leq X$ for all $n$. Then we choose $n$ large enough
%	so that $|b_n - M| < \frac{\e}{2 X}$. back to the proof.)
%	
%	Since $\{a_n\}$ converges, it is bounded by Proposition \ref{prop21}, which gives that  there is a strictly positive real number $X$ so that $|a_n| \leq X$ for all $n \in \N$.
%	Since $\{b_n\}$
%	converges to $M$ and $\frac{\e}{2 X} > 0$, there is a number $N_1$ so that if $n > N_1$ then $|b_n - M| < \frac{\e}{2 X}$. Since $\{a_n\}$ converges to $L$
%	and $\frac{\e}{2|M| + 1} > 0$, there is a number
%	$N_2$ so that if $n \in \N$ and $n > N_2$, then $|a_n - L| < \frac{\e}{2|M| + 1}$. Let $N = \max\{N_1, N_2\}$. For any $n \in \N$ such that $n > N$, we have
%	$$
%	\begin{aligned}
%	|a_nb_n - LM| & = |a_n(b_n - M) + (a_n - L)M| \\
%	& \leq |a_n(b_n - M)| + |(a_n - L)M| \\
%	& = |a_n| |b_n - M| + |a_n - L| |M| \\
%	& < X \frac{\e}{2 X} + \frac{\e}{2|M| + 1} |M| \\
%	& < \e.
%	\end{aligned}
%	$$
%	This proves $\{a_n  \cdot b_n\}_{n=1}^\infty$ converges to $L \cdot M$.
%	
%	
%	To prove Part (6), we first prove a slightly weaker statement:
%	
%	{\bf Claim}: If the sequence $\{b_n\}_{n=1}^\infty$ converges to $M$, $b_n \ne 0$ for all $n$ and $M \ne 0$,  then
%	the sequence $\{\frac{1}{b_n}\}_{n=1}^\infty$ converges to $\frac{1}{M}$.
%	
%	To prove this claim,  pick $\e > 0$. 
%	
%	(Scratch work: We want to show $\left|\frac{1}{b_n} -
%	\frac{1}{M}\right| < \e$ holds for $n$ sufficiently large.  
%	We have
%	$$
%	\left|\frac{1}{b_n} - \frac{1}{M}\right| = \frac{|M - b_n|}{|b_n||M|}.
%	$$ 
%	We can make the top of this fraction as small as we like, but the problem is that the
%	bottom might be very small too since $b_n$ might get very close to $0$. But since $b_n$ converges to $M$ and $M \ne 0$ if we go far enough out, it will be
%	close to $M$. In particular, if $b_n$ is within a distance of
%	$\frac{|M|}{2}$ of $M$ then $|b_n|$ will be at least
%	$\frac{|M|}{2}$. So for $n$ sufficiently large we have
%	$\frac{|b_n-M|}{|b_n||M|} < 2 \frac{|b_n-M|}{|M|^2}$. And then for $n$
%	sufficiently large we also get $|b_n - M| < \frac{|M|^2}{2 \e}$.  Back to the formal proof....)
%	
%	
%	
%	Since $\{b_n\}$ converges to $M$ and $\frac{|M|}{2} > 0$, there is an $N_1$ such that for $n > N_1$ we have $|b_n - M| < \frac{|M|}{2}$  and hence
%	$|b_n| > \frac{|M|}{2}$. Again using that $\{b_n\}$ converges to $M$
%	and that $\frac{ \e |M|^2}{2} > 0$, there is an $N_2$ so that for $n > N_2$ we have
%	$|b_n - M| < \frac{ \e |M|^2}{2}$. Let $N = \max\{N_1, N_2\}$. If $n > N$, then we have
%	$$
%	\begin{aligned}
%	\left|\frac{1}{b_n} - \frac{1}{M}\right| & = \frac{|b_n - M|}{|b_n||M|} \\
%	& < \frac{2}{|M|}  \frac{|b_n - M|}{|M|} \\
%	& = 2 \frac{|b_n - M|}{|M|^2} \\
%	\end{aligned}
%	$$
%	since $|b_n| > |M|/2$ and hence $\frac{1}{|b_n|} < \frac{2}{|M|}$.
%	But then
%	$$
%	2 \frac{|b_n - M|}{|M|^2}  < 2 \frac{\frac{\e |M|^2}{2}}{|M|^2} = \e
%	$$
%	since $|b_n - M| < \frac{ \e |M|^2}{2}$. Putting these together gives
%	$$
%	\left|\frac{1}{b_n} - \frac{1}{M}\right| < \e
%	$$
%	for all $n > N$. 
%	This proves $\{\frac{1}{b_n}\}_{n=1}^\infty$ converges to $\frac{1}{M}$.
%	
%	We have proven the claim.
%	To finish the proof of (6), we use the claim and apply (5) to the convergent sequences $\{a_n\}$ and $\{1/b_n\}$. 
%\end{proof}


















We will discuss a bit the notion of ``diverging to infinity'', a concept that you might have seen before in Calculus. 

It is sometimes useful to distinguish between sequences like 
$$
\{(-1)^n\}_{n=1}^\infty
$$
that diverge because they ``oscillate'', and sequences like
$$
\{n\}_{n=1}^\infty
$$
that diverge because they ``head toward infinity''.


\begin{defn} A sequence $\{a_n\}_{n=1}^\infty$ {\em diverges to $\infty$} if for every real number $M$, there is a real number $N$ such that if $n \in \N$ and  $n > N$,
	then  we have $a_n > M$.
	
	
	A sequence $\{a_n\}_{n=1}^\infty$ {\em diverges to $-\infty$} if for every real number $L$, there is a real number $N$ such that if $n \in \N$ and $n > N$,
	then $a_n < L$. 
\end{defn}

Intuitively, a sequence diverges to $\infty$ provided that, no matter how big $M$ is, if you go far enough along the sequence, eventually all of the terms are
bigger than $M$. Similarly for diverges to $-\infty$.


\begin{ex} \begin{enumerate}
\item $\{ n^2+3\}_{n=1}^\infty$ diverges to $\infty$.
\item $\{ -\sqrt{n} \}_{n=1}^\infty$ diverges to $-\infty$.
\item $\{ (-1)^n\}_{n=1}^\infty$ diverges, but not to $\infty$ nor to $-\infty$.
\item $\{(-1)^n n\}_{n=1}^\infty$ diverges, but not to $\infty$ nor to $-\infty$.
\end{enumerate}
\end{ex}



\Sept{27}


\begin{ex} The sequence $\{\sqrt{n}\}_{n=1}^\infty$ diverges to $\infty$. Let us prove this using the definition: Pick $M \in \R$. 
	(Scratch work: I need $\sqrt{n} > M$ which will occur if $n > M^2$. )
	Let $N = M^2$. 
	If $n \in \N$ and $n > N$, then $\sqrt{n} > \sqrt{N} = \sqrt{M^2} = |M| \geq M$. (Note that $M$ could conceivably be negative.)
	This proves $\{\sqrt{n}\}_{n=1}^\infty$ diverges to $\infty$.
\end{ex}

\begin{prop} If a sequence $\{a_n\}_{n=1}^\infty$ diverges to $\infty$ or diverges to $-\infty$, then it diverges.
\end{prop}
\begin{proof}
We prove the contrapositive. (What is the contrapositive? If a sequence converges, then it does not diverge to $\infty$ and it des not diverge to $-\infty$.) Suppose 
$\{a_n\}_{n=1}^\infty$ converges to some number $L$. Then since it converges,  it is  bounded, so that there are real numbers $b$ and $c$ such that $b \leq a_n \leq c$ for all $n$.

In particular, this means that there is no $N\in \R$ such that $a_n > c$ for all natural numbers $n>N$. Thus, taking ``$M=c$'' in the definition of diverges to $\infty$, we see that $\{a_n\}_{n=1}^\infty$ does not diverge to $\infty$.

Similarly, that there is no $N\in \R$ such that $a_n <b$ for all natural numbers $n>N$. Thus, taking ``$M=b$'' in the definition of diverges to $-\infty$, we see that $\{a_n\}_{n=1}^\infty$ does not diverge to $-\infty$.
\end{proof}

As a matter of shorthand, we write  $\lim_{n \to \infty} a_n = \infty$ to indicate that $\{a_n\}_{n=1}^\infty$ diverges to $\infty$.
But note unlike when we wrote things such as $\lim_{n \to \infty} a_n = 17$, when we write
$\lim_{n \to \infty} a_n = \infty$ we are asserting that $\{a_n\}_{n=1}^\infty$ {\em diverges} (in a specific way). 
Similarly, we write $\lim_{n \to \infty} a_n = -\infty$ to indicate that $\{a_n\}_{n=1}^\infty$ diverges to $-\infty$.





\begin{ex}
Take the sequence $\{a_n\}_{n=1}^{\infty}$ given by\[ a_n = 1 + \frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{n}.\] This is known as the ``harmonic series''. We will show that this sequence diverges to $\infty$. 

Observe that
\begin{align*} a_n &= 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5} + \frac{1}{6} + \frac{1}{7} + \frac{1}{8} + \cdots \\
&\geq 1 + \frac{1}{2} + \left(\frac{1}{4} + \frac{1}{4}\right) + \left(\frac{1}{8} + \frac{1}{8} + \frac{1}{8} + \frac{1}{8}\right) + \cdots\\
&=1 + \frac{1}{2} + 2 \cdot \frac{1}{4} + 4 \cdot \frac{1}{8} + \cdots.\end{align*}


For most natural numbers $n$, it may be a little messy to deal with the last terms in the sum. But, if $k\in \N$, and $n=2^k$, we can do this nicely:

\begin{align*} a_n &= 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5} + \frac{1}{6} + \frac{1}{7} + \frac{1}{8} + \cdots + \frac{1}{2^k} \\
&\geq 1 + \frac{1}{2} + \left(\frac{1}{2^2} + \frac{1}{2^2}\right) + \underbrace{\left(\frac{1}{2^3} + \cdots + \frac{1}{2^3}\right)}_{\text{from $2^2+1$ to $2^3$}} + \cdots + \underbrace{\left(\frac{1}{2^k} + \cdots + \frac{1}{2^k}\right)}_{\text{from $2^{k-1}+1$ to $2^{k}$}} \\
&=1 + \frac{1}{2} + 2^1 \cdot \frac{1}{2^2} + 2^2 \cdot \frac{1}{2^3} + \cdots + 2^{k-1} \cdot \frac{1}{2^k} = 1+ \frac{k}{2}.\end{align*}
\end{ex}

Let $M\in \R$ be given. Let $M'$ be the smallest natural number greater than $M$ (why does such a number exist?) and take $N=2^{2M'}$. By the computation above, taking $k=2M'$, we see that $a_N \geq 1 + \frac{2M'}{2}$. Then, for $n>N$, since $\{a_n\}_{n=1}^{\infty}$ is an increasing sequence, we have
\[ a_n \geq a_N \geq 1 + \frac{2M'}{2} = M' + 1 > M' > M,\]
which shows that $\{a_n\}_{n=1}^\infty$ diverges to $\infty$.




  %%%%%%%%%%%%%%%%%

  % \begin{cor} 
% \begin{enumerate}
% \item
% If  the sequence 
% $\{a_n\}_{n=1}^\infty$ converges to $0$, then the sequence
% $\{|a_n|\}_{n=1}^\infty$ also converges to $0$.
% \item
% More generally, if  the sequence 
% $\{a_n\}_{n=1}^\infty$ converges to some number $L$, then the sequence
% $\{|a_n|\}_{n=1}^\infty$ converges to $|L|$.
% \end{enumerate}
% \end{cor}

% \begin{rem} Clearly part (1) is the special case of part (2) given by letting $L = 0$, 
% and so why have we stated these as separate parts? It is because it turns out to be easier to prove
%   (1) first, and then to use that result to then prove (2).
% \end{rem}


% \begin{proof} To prove (1), assume $\{a_n\}_{n=1}^\infty$ converges to $0$.  We need to prove $\{|a_n|\}_{n=1}^\infty$ converges to $0$. 


% Pick $\e > 0$. Since 
% $\{a_n\}_{n=1}^\infty$ converges to $0$, there is a number $N$ such that if $n \in \N$ and $n > N$, then $|a_n - 0| < \e$. For this same $N$, if $n > N$ then
% $$
% ||a_n| - 0| = ||a_n|| = |a_n| < \e.
% $$
% This proves $\{|a_n|\}_{n=1}^\infty$ converges to $0$. 


% To prove (2), assume $\{a_n\}_{n=1}^\infty$ converges to $L$. By Theorem \ref{thm99}, $\{a_n - L\}_{n=1}^\infty$ converges to $0$ and hence, by part
% (1) of this result, $\{|a_n - L|\}_{n=1}^\infty$ converges to $0$ too. Now observe that
% $$
% -|a_n - L| \leq |a_n| - |L| \leq |a_n - L|
% $$
% holds by the Reverse Triangle Inequality. By the Squeeze Theorem we get that
% $\{|a_n| - |L|\}_{n=1}^\infty$ converges to $0$. But then by Theorem \ref{thm99} again
% $\{|a_n| - |L| + |L|\}_{n=1}^\infty$ converges to $|L|$.
% \end{proof}





%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\section*{Discussion Questions, March 3}

{\large TRUE or FALSE. Justify.}

\


\begin{enumerate}
	\item Let $x,y\in \R$. The negation of the statement ``If $x$ and $y$ are rational, then $xy$ is rational'' is ``If $x$ and $y$ are rational, then $xy$ is irrational''. (F)
	
	\
	
	\item Let $x,y\in \R$. The contrapositive of the statement ``If $x$ and $y$ are rational, then $xy$ is rational'' is ``If $xy$ is irrational, then $x$ and $y$ are irrational''. (F)
	
	
	\
	
	\item The associative property/axiom of addition says that $(x+y)+z=x+(y+z)$. (T)
	
	\
	
	\item Every set of real numbers that is bounded above has a supremum. (F)
	
	\
	
	\item There is a set $S$ of real numbers such that $\sup(S)$ exists, but $\sup(S)\notin S$. (T)
	
	\
	
	\item If $a<b$ are real numbers, there is a natural number $n\in \N$ such that $a<n<b$. (F)
	
	\
	
	\item Every nonempty set of real numbers has a smallest element (i.e., a minimum element). (F)
	
	\
	
	\item Every nonempty set of integers that is bounded below has a smallest element (i.e., a minimum element). (T)
	
	\
	
	\item If $S\subseteq \R$ is bounded above, there there is a natural number $b$ such that $b$ is an upper bound for~$S$. (T)
	
	\
	
	\item It is possible to prove that there is a real number $x$ such that $x^2=2$ using just the first 10 axioms (i.e., without using the Completeness Axiom). (F)
	
	\
	
	\item Every set of real numbers satisfies the property that ``for all $x\in S$, there exists a real number $y$ such that $x<y^2$\,''. (T)
	
	\
	
	\item Every set of real numbers satisfies the property that ``for all $x\in S$, there exists a real number $y$ such that $y^2 < x$''. (F)
	
	\
	
	\item The supremum of the set $\{ 1/n \ | \ n\in\N\}$ is $1$. (T)
	
	\
	
	\item The supremum of the set $\{ -1/n \ | \ n\in\N\}$ is $-1$. (F)
	
	\
	
	\item The negation of the statement ``for all $x\in S$, there exists a real number $y$ such that $x<y^2$\,'' is ``for all $x\in S$, there exists a real number $y$ such that $x\geq y^2$\,''. (F)
	
	\
	
	\item If a sequence $\{a_n\}_{n=1}^\infty$ converges to $L$, then there is some $N\in \R$ such that for all natural numbers $n>N$, $a_n=L$. (F)
	
	\
	
	\item For every real number $L$ there is a sequence that converges to~$L$. (T)
	
	\
	
	\item	For every real number $L$ there is a sequence $\{a_n\}_{n=1}^\infty$ such that $a_n\neq L$ for all $n\in \N$ and converges to~$L$. (T)
	
	\
	
	\item A sequence of positive numbers can converge to a negative number. (F)
	
	\
	
	\item A sequence of positive numbers can converge to zero. (T)
	
	\
	
	\item Every increasing sequence is bounded below. (T)
	
	
	\
	
	\item Every increasing sequence is convergent. (F)
	
	\
	
	\item Every convergent sequence is either increasing or decreasing. (F)
	
	\
	
	\item If $\{a_n\}_{n=1}^\infty$ and $\{b_n\}_{n=1}^\infty$ are sequences, $\{a_n\}_{n=1}^\infty$ converges to $L$, and there is some $N\in \R$ such that $a_n=b_n$ for $n>N$, then $\{b_n\}_{n=1}^\infty$ converges to $L$. (T)
	
	\
	
	\item If $\{a_n\}_{n=1}^\infty$ and $\{b_n\}_{n=1}^\infty$ are convergent sequences, then $\{a_n + b_n\}_{n=1}^\infty$ is a convergent sequence. (T)
	
	\
	
	\item If $\{a_n\}_{n=1}^\infty$ and $\{b_n\}_{n=1}^\infty$ are convergent sequences, and $b_n\neq 0$ for all $n\in \N$, then $\{a_n / b_n\}_{n=1}^\infty$ is a convergent sequence. (F)
	
	\
	
	\item The sequence $\displaystyle \left\{ \frac{3 n^2- 4n + 7}{6n^2 +1} \right\}_{n=1}^{\infty}$ converges to $1/2$. (T)
	
	\
	
	\item The negation of ``$\{a_n\}_{n=1}^\infty$ is a monotone sequence'' is ``there exists $n\in \N$ such that $a_n> a_{n+1}$ and $a_n < a_{n+1}$''. (F)
	
	\
	
	\item Every convergent sequence of rational numbers converges to a rational number. (F)
	
	\
	
	\item Every convergent sequence of natural numbers converges to a natural number. (T)
	
	\
	
\end{enumerate}



We will now embark on a bit of detour. I've postponed talking about proofs by induction, but we will need to use that technique on occasion. So let's talk
about that idea now. 

The technique of proof by induction is used to prove that an infinite sequence of statements indexed by $\N$
$$
P_1, P_2, P_3, \dots
$$
are all true. For example, for any real number $x$, the equation
$$
(1-x)(1+ x + \cdots + x^n) = 1 - x^{n+1}
$$
holds for all $n \in \N$. Fixing $x$, we get one statement for each natural number:

\begin{align*}
	&P_1: &(1-x)(1+x)=1-x^2\\
		&P_2: &(1-x)(1+x+x^2)=1-x^3\\
		&P_3: &(1-x)(1+x+x^2+x^3)=1-x^4\\
		&\vdots &\vdots
	\end{align*}

Such a fact (for all $n$) is well-suited to be proven by induction.

Here is the general principle:

\begin{thm}[Principle of Mathematical Induction] Suppose we are given, for each $n \in \N$, a statment $P_n$.  
Assume that $P_1$ is true and that for each $k
  \in \N$, if $P_k$ is true, then $P_{k+1}$ is true. Then $P_n$ is true for all $n \in \N$.
\end{thm}

``The domino analogy'': Think of the statements $P_1, P_2, \dots$ as dominoes lined up in a row. The fact that $P_k \implies P_{k+1}$ is interpreted as meaning
that the dominoes
are arranged well enough so that if one falls, then so does the next one in the line. The fact that $P_1$ is true is interpreted as meaning the first one has been knocked
over. Given these assumptions, for every $n$, the $n$-th domino will (eventually) fall down. 



The Principle of Mathematical Induction (PMI) is indeed a theorem, which we will now prove:

\begin{proof} 
Assume that $P_1$ is true and that for each $k
  \in \N$, if $P_k$ is true, then $P_{k+1}$ is true.
Consider the subset
$$
S = \{n \in \N \mid \text{ $P_n$ is false} \}
$$
of $\N$. Our goal is to show $S$ is the empty set. 

By way of contradiction, suppose $S$ is not empty. Then
by the Well-Ordering Principle, $S$ has a smallest element, call it $\ell$. (In other words, $P_\ell$ is the first statement in the list $P_1, P_2, \dots, $
that is false.) Since $P_1$ is true, we must have $\ell > 1$. But then
$\ell-1 < \ell$ and so $\ell-1$ is not in $S$. Since $\ell > 1$, we have $\ell-1 \in
\N$ and thus we can say that $P_{\ell-1}$ must be true. 
Since $P_k \Rightarrow P_{k+1}$ for any $k$,  letting $k = \ell-1$, we see that, since $P_{\ell-1}$ is true, $P_{\ell}$ must also by true. This contradicts the fact that $\ell \in S$. We
conclude that $S$ must be the empty set.
\end{proof}

The above proof shows that the Principle of Mathematical Induction is a consequence of the Well-Ordering Principle. The converse is also true.

\Sept{29}

(1) Show that if $\{a_n\}_{n=1}^\infty$ is increasing and not bounded above, then $\{a_n\}_{n=1}^\infty$ diverges to $\infty$.
\begin{framed}
Suppose that $\{a_n\}_{n=1}^\infty$ is increasing and not bounded above.
Let $M\in \R$ be arbitrary. Since $\{a_n\}_{n=1}^\infty$ is not bounded above, $M$ is not an upper bound, so there exists some $N\in \N$ such that $a_N > M$. We claim that for all natural numbers $n>N$, we have $a_n>M$ Indeed, if $n>N$, since the sequence is increasing, we have $a_n \geq a_N >M$, as desired. Thus, for every $M\in \R$, there is some $N\in \R$ such that for all natural numbers $n>N$, we have $a_n>M$. This means that $\{a_n\}_{n=1}^\infty$ diverges to $\infty$.

\hrulefill


Alternative argument:


\hrulefill


Suppose that $\{a_n\}_{n=1}^\infty$ is increasing and not bounded above. By way of contradiction, suppose that $\{a_n\}_{n=1}^\infty$ does not diverge to $\infty$. This means that there exists some $M\in \R$ such that for all $N\in \R$, there exists a natural number $n>N$ with $a_n \leq M$. We claim that $M$ is an upper bound for $\{a_n\}_{n=1}^\infty$, which will be the desired contradiction. Let $k\in \N$; we need to show that $a_k \leq M$. Taking $k=N$ in the statement above, there is some natural number $n>k$ with $a_n \leq M$. But since $\{a_n\}_{n=1}^\infty$ is increasing, we have $a_k \leq a_n \leq M$, as desired, so $M$ is an upper bound. We have obtained a contradiction, so we must have that $\{a_n\}_{n=1}^\infty$ does indeed diverge to $\infty$.
\end{framed}


(2) Let $S$ be a nonempty bounded above set and $\ell=\sup(S)$.

$\bullet$ Show that, for every $n\in \N$, there is some $x\in S$ such that $\ell-\frac{1}{n} < x \leq \ell$.

\begin{framed}
Since $\ell-\frac1n < \ell = \sup(S)$, $\ell-\frac1n$ is not an upper bound for $S$, so there exists some $x\in S$ with $\ell-\frac1n < x$. Since $\ell$ is an upper bound of $S$, we also have $x\leq \ell$.
\end{framed}

$\bullet$ Show that there is a sequence $\{a_n\}_{n=1}^\infty$ with $a_n\in S$ for all $n\in \N$ that converges to $\ell$.

\begin{framed}
For each $n$, pick some $a_n\in S$ such that $\ell-\frac1n < a_n \leq \ell$. We claim that this sequence converges to $\ell$. Note that the sequence  $\{\ell \}_{n=1}^\infty$ converges to $\ell$ since it is constant, and $\{ \ell-\frac1n \}_{n=1}^\infty$ converges to $\ell$ also by Theorem 12.2. Thus, by the Squeeze Theorem, $\{\ell \}_{n=1}^\infty$ converges to $\ell$ too.


\hrulefill


Alternative argument:


\hrulefill

For each $n$, pick some $a_n\in S$ such that $\ell-\frac1n < a_n \leq \ell$. We claim that this sequence converges to $\ell$. Let $\e>0$. Take $N=1/\e$. Then for a natural number $n>N$, using the inequalities above, we have that $|a_n-\ell|<\frac1n <\frac1N =\e$.

\end{framed}



\Oct{1}





\begin{ex} Let's show that for every $x\in \R$, the equality
\[ (1-x)(1+ x + \cdots + x^n) = 1-x^{n+1}\]
holds for all $n\in \N$.
Fix a real number $x\in \R$. We will show by induction on $n$ that the equality \[ (1-x)(1+ x + \cdots + x^n) = 1-x^{n+1}\]
holds this real number $x$ and for all $n\in \N$. For $n=1$, we have \[(1-x)(1+x) = 1-x^2,\]
so the equality holds in this case. Assume that the equality holds for some natural number $n=k$. Then 
\begin{align*}
(1-x)(1+ x + \cdots + x^{k+1}) &= (1-x)((1+x+\cdots + x^k)+x^{k+1})\\&=(1-x)(1+ x + \cdots + x^k) + x^{k+1} - x^{k+2} \\&= 1-x^{k+1} + x^{k+1} - x^{k+2} \\&= 1-x^{k+2}\end{align*}
so the equality holds for $n=k+1$. Thus, by induction, the equality holds for all $n\in \N$.\qed
\end{ex} 

\begin{ex} Let's show that every finite set has a minimum element. It's not obvious that induction makes any sense here, but we can rephrase the statement as saying that for every $n\in \N$, a set of real numbers with $n$ elements has a minimum. So, let us prove by induction on $n$ that for every set of $n$ real numbers, the set has a minimum.
 When $n=1$, any set with one element clearly has its only element as a minimum, so the statement is true in this case.
Assume that for some natural number $n=k$, every set of $k$ elements has a minimum element; we need to show that every set of $k+1$ elements has a minimum. Let $S$ be a set of $k+1$ elements, and fix some $x\in S$. Then $T=S\smallsetminus\{x\}$ has $k$ elements, so it has a minimum, which we will call $y$. Either $x<y$ or $x\geq y$. If $x<y$, and $z\in S$, either $z \in T$ so $z\geq y > x$, or else $z=x$ so $z\geq x$; thus, $x=\min (S)$ in this case. If $x\geq y$, and $z\in S$, if $z\in T$ then $z\geq y$, and otherwise $z=x\geq y$; thus $y=\min (S)$ in this case. We have shown that $S$ has a minimum element. This completes the inductive step, and hence completes the proof.\qed
\end{ex}

Now, I want to apply what we've done so far to decimal expansions. Let us say that $d_1,d_2,d_3,\dots$ is a ``digit sequence'' if $d_i\in\{0,1,\dots,9\}$ for each $i\in\N$. We will say that an integer $k$ and a digit sequence $d_1,d_2,d_3,\dots$ is a ``decimal expansion'' and write $k. d_1 d_2 d_3 \dots$ to denote a decimal expansion. We say that the decimal expansion $k. d_1 d_2 d_3 \dots$  ``corresponds to a real number $r$'' if the sequence
\begin{align*}
a_1 &= k + \frac{d_1}{10^1} \\
a_2 &= k + \frac{d_1}{10^1} + \frac{d_2}{10^2} \\
a_3 &= k + \frac{d_1}{10^1} + \frac{d_2}{10^2} + \frac{d_3}{10^3}\\
&\vdots
\end{align*}
converges to $r$.

\begin{thm}
Every decimal expansion corresponds to a real number.
\end{thm}
\begin{proof}
Let $k$ be an integer, and $d_1,d_2,d_3,\dots$ be a sequence such that $d_i\in \{0,1,\dots,9\}$ for each $i\in \N$. We need to show that the sequence 
\begin{align*}
a_1 &= k + \frac{d_1}{10^1} \\
a_2 &= k + \frac{d_1}{10^1} + \frac{d_2}{10^2} \\
a_3 &= k + \frac{d_1}{10^1} + \frac{d_2}{10^2} + \frac{d_3}{10^3}\\
&\vdots
\end{align*}
converges. Observe that $\{a_n\}_{n=1}^\infty$ is increasing. Note now that for any $n\in \N$, we have
\begin{align*}
a_n &=k + \frac{d_1}{10^1} + \frac{d_2}{10^2} + \cdots + \frac{d_n}{10^n}\\
&=k + \frac{9}{10^1} + \frac{9}{10^2} + \cdots + \frac{9}{10^n}\\
&=k + \frac{9}{10} \left( 1 + \frac{1}{10^1} + \cdots + \frac{1}{10^{n-1}} \right) \\
&= k+ \frac{9}{10} \frac{ 1 - \frac{1}{10^n} }{1- \frac{1}{10}}\\
&= k+ \frac{9}{10} \frac{ 1 - \frac{1}{10^n} }{\frac{9}{10}}\\
&= k+ 1-\frac{1}{10} \leq k+1.\end{align*}
Since this holds for all $n$, $\{a_n\}_{n=1}^\infty$ is bounded above by $k+1$. By the Monotone Convergence Theorem, $\{a_n\}_{n=1}^\infty$ converges.
\end{proof}

\begin{ex} The decimal expansion $0.25$ corresponds to $\frac{1}{4}$. The decimal expansion $0.99999\dots$ corresponds to $1$.\end{ex}











% \begin{ex} Let us prove that the sum of the first $n$ natural numbers is $\frac{n(n+1)}{2}$; that is, $1 + \cdots + n = \frac{n(n+1)}{2}$. Let $P_n$ be the statement
% \begin{quote}
% The equation $1 + \cdots + n = \frac{n(n+1)}{2}$ holds.
% \end{quote}
% Then $P_1$ is true since $1 = \frac{1 \cdot 2}{2}$. Assume $P_k$ is true for some $k \in \N$, so that
% $1 + \cdots + k = \frac{k(k+1)}{2}$. Add $k+1$ to both sides to get
% $$
% \begin{aligned}
%  1 + \cdots + k + k+1 & = \frac{k(k+1)}{2} + k+1 \\
% & =  \frac{k(k+1)}{2} + k+1 \\
% & = \frac{k(k+1) + 2(k+1)}{2} \\
% & =  \frac{(k+1)(k+2)}{2} \\
% \end{aligned}
% $$
% and thus $P_{k+1}$ is true. By PMI, $P_n$ is true for all $n$.
% \end{ex}




\Oct{8}

The next example of a proof by induction will establish a fact that is perhaps
intuitively obvious.  Since it will play an important role in later proofs, we state it as a Lemma here:

\begin{lem} \label{lem211}
	Let $b_1, b_2, \dots$ be any strictly increasing sequence of natural numbers; that is, assume $b_k \in \N$ for all $k \in \N$ and that $b_k < b_{k+1}$ for 
	all $k \in \N$. Then $b_k \geq k$ for all $k$.
\end{lem}

\begin{proof} Suppose $b_1, b_2, \dots$ is a strictly increasing
	sequence of natural numbers. We prove $b_n \geq n$ for all $n$ by
	induction on $n$. 
	That is, for each $n  \in \N$, let $P_n$ be
	the statement that $b_n \geq n$. 
	
	$P_1$ is true since $b_1 \in \N$ and so $b_1 \geq 1$. Given $k \in \N$, assume $P_k$ is true; that is, assume $b_k \geq k$. Since $b_{k+1} >
	b_k$ and both are natural numbers, we have $b_{k+1} \geq b_k + 1 \geq k+1$; that is, $P_{k+1}$ is true too. By induction, $P_n$ is true for all $n \in \N$. 
\end{proof}


% Let's prove the Binomial Theorem by induction. Recall that $n! = n(n-1) \cdots 2 \cdot 1$ (and $0! = 1$) and that
% $$
% {n \choose i} := \frac{n!}{(n-i)! i!}.
% $$
%
%
%
% \begin{prop} For all real numbers $x$ and $y$ and all natural numbers $n$, 
% $$
% (x+y)^n = \sum_{i=0}^n {n \choose i} x^{n-i} y^i.
% $$
% \end{prop}
%
% \begin{proof} Let $x$ and $y$ be arbitrary real number. We prove 
%$ (x+y)^n = \sum_{i=0}^n {n \choose i} x^{n-i} y^i$
%holds for each $n \in \N$ by induction on $n$. 
%
%The case $n = 1$ is immediate since $\sum_{i=0}^1 {1 \choose i} x^{1-i} y^i = x + y = (x+y)^n$.
%
% Assume the equation 
% $$
% (x+y)^n = \sum_{i=0}^n {n \choose i} x^{n-i} y^i
% $$
% holds. Multiplying both sides by $x+y$ gives
% $$
% (x+y)^{n+1} = x \sum_{i=0}^n {n \choose i} x^{n-i} y^i
%+ y \sum_{i=0}^n {n \choose i} x^{n-i} y^i
%= \sum_{i=0}^n {n \choose i} x^{n-i+1} y^i + \sum_{i=0}^n {n \choose i} x^{n-i} y^{i+1}
% $$
% We have
% $$
% \sum_{i=0}^n {n \choose i} x^{n-i+1} y^i = x^{n+1} + \sum_{i=1}^n {n \choose i} x^{n-i+1} y^i
% $$
% and
% $$
% \begin{aligned}
% \sum_{i=0}^n {n \choose i} x^{n-i} y^{i+1} & = 
% \left(\sum_{i=0}^{n-1} {n \choose i} x^{n-i} y^{i+1}  \right) + y^{n+1} \\
% & = \left(\sum_{j=1}^{n} {n \choose i-1} x^{n-j+1} y^{j}  \right) + y^{n+1} \\
%& = \left(\sum_{i=1}^{n} {n \choose i-1} x^{n-i+1} y^{i}  \right) + y^{n+1} 
% \end{aligned}
% $$
% It follows that
% $$
% (x+y)^{n+1} = x^{n+1} + \left(\sum_{i=1}^{n} ({n \choose i} + {n \choose i-1}) x^{n-i+1} y^{i}  \right) + y^{n+1}. 
% $$
% Now observe that
% $$
% \begin{aligned}
% {n \choose i} + {n \choose i-1} & = \frac{n!}{(n-i)! i!} + \frac{n!}{(n-i+1)! (i-1)!} \\
% & = \frac{n!(n-i+1)}{(n-i+1)! i!} + \frac{n!i}{(n-i+1)! i!} \\
% & = \frac{n!(n-i+1) + n!i}{(n-i+1)! i!} \\
% & = \frac{(n+1)!}{(n-i+1)! i!} \\
% & = {n+1 \choose i}.
% \end{aligned}
% $$
% We get
% $$
% (x+y)^{n+1} = x^{n+1} + \left(\sum_{i=1}^{n} {n +1 \choose i}  x^{n-i+1} y^{i}  \right) + y^{n+1}
% = \sum_{i=0}^{n+1} {n +1 \choose i}  x^{n-i+1} y^{i}.
% $$
%
% By the principle of mathematical induction, the equation holds for all $n$.
% \end{proof}


%  \begin{ex} Suppose in a certain country, for every pair of cities there is a unique one-way road joining them (going in one direction or the other, but not both). 
%  Let us prove that it is possible to visit every city in the country by flying into some city, visiting each city exactly once by car, and then flying away from a different
%     city. 

%   Here is a more mathematical version. Imagine a graph with $n$ vertices such that for any pair of vertices there is a unique directed edge joining them. 
%  I claim there is a path that visits each vertex exactly once
%  (starting and stopping at different vertices). 

%   We proceed by induction on $n$. The statement $P_n$ is ``Given any graph with $n$ vertices such that between any two vertices there is a unique directed edge,
%   there is a path that visits every vertex exactly once.''

%  The case $n = 1$ is true, since if the country has just one city, then you can fly into it, do no driving, and then fly away. Or in terms of graph theory,
% the path of length $0$ counts. (But if this makes you nervous
% you can have $n = 2$ be the base case: if a country has two cities with a one-way street joining them, fly into the city for which the road is leaving that city,
% drive to the other city along that road, and then fly home.)


% Suppose $P_n$ is true for some $n \in \N$, 
%  and now consider such a graph with $n+1$ vertices. Let $w$ be any one of the vertices of this graph.  
%  If we remove $w$ from consideration, we are left with a graph with $n$ vertices that satisfies the same condition. Since we assume $P_n$ is true, 
%  there is at least one path among these other $n$ vertices that visits each of
%  them exactly once. Let us label these vertices in the order in which this path visits them, as $v_1, \dots, v_n$, so that the path looks like
%  $$
%  v_1 \to v_2 \to v_3 \to \cdots \to v_n.
%  $$
% We will extend this to a suitable path involving $w$ too.

% {\bf Case I}: Assume the edge  joining $v_i$ and $w$ goes from $v_i$ to $w$, for all $1 \leq i \leq n$. 
% In this case, in particular we have that the edge between $v_n$ and $w$ goes from $v_n$ to $w$. Thus we may form the path 
%  $$
%  v_1 \to v_2 \to v_3 \to \cdots \to v_n \to w.
%  $$

% {\bf Case II}: 
% Assume there is at least one value of $i$ such that edge joining $w$ and $b_i$ goes from $w$ to  $v_i$. 
% In this case, let $j$ be the least integer such that the edge joining $w$ and $v_j$ goes from $w$ to $v_j$. 
% If $j = 1$, then we mauy form the path
%  $$
%  w \to v_1 \to v_2 \to v_3 \to \cdots \to v_n.
%  $$
% Otherwise if $j > 1$, then the edge between $w$ and $v_{j-1}$ goes from $v_{j-1}$ to $w$ and thus we may form the path
% $$
%  v_1 \to v_2 \to  \cdots \to v_{j-1} \to w \to v_j \to v_{j+1} \to \cdots \to v_n.
% $$

%  In all cases, we've shown there is a path of the desired type. That is, we've shown $P_n \Rightarrow P_{n+1}$. By PMI, $P_n$ is true for all $n$.
%  \end{ex}




%In the next homework assignment, I will ask you to prove:
%
%\begin{exer} Let $\{a_n\}_{n=1}^\infty$ be a sequence such that $a_n > 0$ for all $n$. Prove $\{a_n\}_{n=1}^\infty$ diverges to $\infty$ if and only if the sequence
%  $\left\{\frac{1}{a_n}\right\}_{n=1}^\infty$ converges to $0$.
%\end{exer}

%\begin{proof} Suppose $\{a_n\}_{n=1}^\infty$ diverges to $\infty$.  Pick $\epsilon > 0$. Letting $M = \frac{1}{\epsilon}$ by
 % definition of ``diverges to $\infty$'' we have that there is an $N$ such that for all $n \in \N$ with $n > N$, $a_n > \frac{1}{\epsilon}$. Since $a_n > 0$ and $\epsilon > 0$,
  %this implies $|\frac{1}{a_n} - 0| = \frac{1}{a_n} < \epsilon$. This proves   $\left\{\frac{1}{a_n}\right\}_{n=1}^\infty$ converges to $0$.

%  Suppose   $\left\{\frac{1}{a_n}\right\}_{n=1}^\infty$ converges to $0$. Pick any real number $M$ and let $\epsilon = \frac{1}{\max\{M, 1\}}$. Note that this implies $1/\e \geq M$. 
 % By definition of ``converges to $0$'', there is an $N$ such that if $n \in \N$ and $n > N$, then $1/a_n = |1/a_n - 0| < \epsilon$. (Recall $a_n > 0$ for all $n$.) It follows that
 % $a_n > 1/\e \geq M$ for all $n > N$.
 % This proves that $\{a_n\}_{n=1}^\infty$ diverges to $\infty$.
 % \end{proof}
  



We next discuss the important concept of a ``subsequence''. 

Informally speaking, a subsequence of a given sequence is a sequence one forms by skipping some of the 
terms of the original sequence. In other words, it is a sequence formed by taking just some of the terms of the original
sequence, but still infinitely many of them,   
without repetition. 

We'll cover the formal definition soon, but let's give a few examples first, based on this informal definiton. 

\begin{ex}
Consider the sequence
$$
a_n = 
\begin{cases}
7 &  \text{if $n$ is divisible by $3$ and} \\
\frac{1}{n} &  \text{if $n$ is not divisible by $3$}. \\
\end{cases}
$$ If we pick off every third term starting with the term $a_3$ we get the subsequence
$$
a_3, a_6, a_9, \dots
$$
which is the constant sequence
$$
7,7,7, \dots.
$$
If we pick off the other terms we form the subsequence
$$
a_1, a_2, a_4, a_5, a_7, a_8, a_{10}, \dots
$$
which gives the sequence
$$
1, \frac12, \frac14, \frac15, \frac17, \frac19, \frac1{10}, \dots.
$$
Note that it is a little tricky to find an explicit formula for this sequence.
\end{ex}

\begin{ex} For another, simpler, example, consider the sequence $\{(-1)^n \frac{1}{n}\}_{n=1}^\infty$. Taking just the odd-indexed terms gives the sequence
$$
-1, - \frac13, - \frac15, - \frac17, - \frac19, \dots
$$
and taking the even-indexed terms gives the sequence
$$
\frac12, \frac14, \frac16, \frac18, \dots
$$
This time we can easily give a formula for each of these sequences: the first is
$$
\{- \frac{1}{2n-1} \}_{n=1}^\infty
$$
and the second is
$$
\{\frac{1}{2n} \}_{n=1}^\infty.
$$
\end{ex}

Here is the formal definiton:

\begin{defn} A {\em subsequence} of a given sequence
  $\{a_n\}_{n=1}^\infty$ is any sequence of the form
$$
\{a_{n_k}\}_{k=1}^\infty 
$$
where 
$$
n_1, n_2, n_3, \dots
$$
is any strictly increasing sequence of natural numbers --- that is
$n_k \in \N$ and 
$n_{k+1} > n_k$ for all $k \in \N$, so that 
$$
n_1 < n_2 < n_3 < \cdots.
$$
\end{defn}

\begin{ex} Let $\{a_n\}_{n=1}^\infty$ be any sequence.

Setting $n_k = 2k-1$ for all $k \in \N$ gives the subsequence of just the odd-indexed terms of the
  original sequence. 

Setting $n_k = 2k$ for all $k \in \N$ gives the subsequence of just the even-indexed terms of the
  original sequence. 

Setting $n_k = 3k-2$ for all $k \in \N$ gives the subsequence of consising of every third term of the
  original sequence, starting with the first.

Setting $n_k = 100 + k$ gives the subsequence that is that ``tail
end'' of the original, obtained by skipping the first 100 terms:
$$
a_{101}, a_{102}, a_{103}, a_{104}, \dots.
$$
Of course, there is nothing special about $100$ in this example.
\end{ex}


The following result is important:



\begin{thm} If a sequence $\{a_n\}_{n=1}^\infty$ converges to $L$, then every subsequence of this sequence also converges to $L$.
\end{thm}


\begin{proof} Assume $\{a_n\}_{n=1}^\infty$ converges to $L$
and let $n_1 < n_2 < \cdots$ be any strictly increasing sequence of natural numbers. We need to prove $\{a_{n_k}\}_{k=1}^\infty$ converges to $L$. 

Pick $\e > 0$. Since $\{a_n\}_{n=1}^\infty$ converges to $L$, there is an $N$ such that if $n \in \N$ and $n > N$, then $|a_n - L| < \e$. (We will show that the
same $N$ also ``works'' for the subsequence.) 

If $k \in \N$ and $k > N$, then $n_k \geq k$ by Lemma \ref{lem211},
and hence $n_k > N$. It follows that $|a_{n_k} - L| < \e$. This proves 
$\{a_{n_k}\}_{k=1}^\infty$ converges to $L$. 
\end{proof}


\begin{cor}  Let $\{a_n\}_{n=1}^\infty$ be any sequence.

\begin{enumerate}

\item If there is a subsequence of this sequence that diverges, then the sequence itself diverges.

\item If there are two subsequences of this sequence that converge to different values, then  the sequence itself diverges.

\end{enumerate}

\end{cor}


\begin{proof} These are both immediate consequences of the theorem.
\end{proof}


\begin{ex}
Consider the sequence
$$
a_n = 
\begin{cases}
7 &  \text{if $n$ is divisible by $3$ and} \\
\frac{1}{n} &  \text{if $n$ is not divisible by $3$}. \\
\end{cases}
$$

Let $n_k = 3k$. 
Then the subsequence $\{a_{n_k}\}_{k=1}^\infty$ is 
$$
a_3, a_6, a_9, \dots
$$
which is the constant sequence
$$
7,7,7, \dots.
$$
It converges to $7$.

Now let $n_k = 3k-2$. 
Then the subsequence $\{a_{n_k}\}_{k=1}^\infty$ is 
$$
a_1, a_4, a_7, \dots
$$
which is the sequence $\{\frac{1}{3k-2}\}_{k=1}^\infty$. It converges to $0$. 

Since the original sequence admits two  subsequences that converge to different values, by the Corollary, the original sequence diverges.
\end{ex}



I want to give a {\em crazy} example of a sequence:  



\begin{lem} \label{lem222}
	There exists a sequence of strictly positive rational numbers
	$\{a_n\}_{n=1}^\infty$ such that every strictly positive rational
	number occurs in it  infinitely many times.
\end{lem}

\begin{proof} Consider the points in the first quadrant whose Cartesian coordinates are positive integers: $(m,n)$ for some $m,n \in \N$. Starting at $(1,1)$
	travel back and forth along diagonal lines of slope $-1$ as shown:
	
\[{\xymatrix@C=1em@R=1em{ 
\vdots & \vdots & \vdots & \vdots & \reflectbox{$\ddots$} \\
(1,4) \ar@{.}@[blue][u] & (2,4) \ar@{->}@[blue][dr] \ar@{.}@[blue][ul]& (3,4)  \ar@{.}@[blue][ul] & (4,4) \ar@{.}@[blue][ul] \ar@{.}@[blue][dr] & \cdots \\
(1,3) \ar@{->}@[blue][dr] & (2,3) \ar@{->}@[blue][ul] & (3,3) \ar@{->}@[blue][dr] & (4,3) \ar@{->}@[blue][ul] \ar@{.}@[blue][dr]& \cdots \\
(1,2) \ar@{->}@[blue][u] & (2,2)\ar@{->}@[blue][dr]  & (3,2)\ar@{->}@[blue][ul]  & (4,2) \ar@{.}@[blue][dr] & \cdots \\
(1,1)\ar@{->}@[blue][r]  & (2,1)\ar@{->}@[blue][ul]   & (3,1)\ar@{->}@[blue][r]   & (4,1)\ar@{->}@[blue][ul]  & \cdots }}\]

\
	
	This gives the list of points
	$$
	(1,1),  (2,1), (1,2),(1,3), (2,2), (3,1), (4,1), (3,2), (2,3), (1,4), \dots
	$$
	Now convert these to a list of rational numbers by changing $(m,n)$ to $\frac{m}{n}$ to get the sequence
	$$
	\frac11, \frac21, \frac12, \frac13, \frac22, \frac31, \frac41, \frac32, \frac23, \frac14, \dots
	$$
	of positive rational numbers.  
	
	I claim every strictly positive rational number occurs infinitely many times in this sequence: 
	Let $q$ be any strictly positive rational number. Then $q =\frac{m}{n}$ for some $m,n \in \N$. Moreover, $q = \frac{jm}{jn}$ for all $j \in \N$, and since  
	$\frac{jm}{jn}$ occurs in the sequence  for all $j \in \N$, the number $q$ appears infinitely many times.
\end{proof}



We can improve this a bit:

\begin{cor} \label{cor222}
	There exists a sequence $\{q_n\}_{n=1}^\infty$ of rational numbers such that every rational number occurs infinitely many times.
\end{cor}


\begin{proof} Starting with a sequence  $\{a_n\}_{n=1}^\infty$ as in
	Lemma \ref{lem222}, 
	such that every strictly positive rational number occurs infinitely many times, define a new sequence by
	$$
	a_1, -a_1, 0, a_2, -a_2, 0, a_3, -a_3, 0, a_3, -a_3, 0, \dots
	$$
	More formally, let 
	$$
	q_n = \begin{cases}
	a_{(n-1)/3}, & \text{ if $n$ is congruent to $1$ modulo $3$,} \\
	-a_{(n-2)/3}, & \text{ if $n$ is congruent to $2$ modulo $3$, and} \\
	0, & \text{ if $n$ is congruent to $0$ modulo $3$.} \\
	\end{cases}
	$$
	It is clear that every rational number occurs infinitely many times in
	this new sequence. 
\end{proof}


In particular, the sequence $\{q_n\}_{n=1}^\infty$ in this Corollary has the following property: For each rational number $q$, there is a subsequence of it that converges to
$q$. Namely, for any $q \in \Q$,  form the constant subsequence $q,q,q, \dots$ of the sequence, which is possible since $q$ occurs an infinite number of times. 

\Oct{11}

In fact, we can do even better: I claim that every {\em real} number occurs as a limit of the sequence of the Corollary!

First a Lemma.

\begin{lem} \label{lem221}
	For any $x \in \R$ there is a sequence of rational numbers that converges to $x$.
\end{lem}

\begin{proof} For each $n \in \N$ we have  $x < x+  \frac{1}{n}$, and hence by the Density of the Rationals,
	there is a rational number $a_n \in \Q$ so that $x < \a_n < x +\frac{1}{n}$. Since both the constant sequence $\{x\}_{n=1}^\infty$ and the sequence
	$\{x + \frac{1}{n}\}_{n=1}^\infty$ converge to $x$. 
	By the Squeeze Theorem, $\{a_n\}_{n=1}^\infty$ also converges to~$x$.
\end{proof}





\begin{thm} There exists a sequence of rational numbers having the property that every real number is the limit of some subsequence of~it.
\end{thm}

\begin{proof} 
	Let $\{q_n\}_{n=1}^\infty$ be a sequence of rational nubmers as in
	Corollary \ref{cor222}, so that that every rational number occurs infinitely many times.
	Let $x$ be any real number. I will construct a subsequence that converges to $x$.
	
	By Lemma~\ref{lem221} there is a sequence of rational numbers $\{a_n\}_{n=1}^\infty$ that converges to $x$. 
	Since $a_1 \in \Q$, $a_1$ occurs (infinitely many times) in $\{q_n\}_{n=1}^\infty$ and hence there is $n_1 \in \N$ such that $a_1 = q_{n_1}$.
	
	Given $n_1<\cdots<n_k$ such that $q_{n_k}=a_k$, we claim that we can find some $n_{k+1}\in \N$ such that $n_{k+1} > n_k$ and $q_{n_{k+1}}=a_{k+1}$. Indeed, there are infinitely many natural numbers $m$ such that $q_m=a_{k+1}$, and only finitely many of them can be less than or equal to $n_k$, so there must be such a number that is greater than $n_k$. 
	
	Thus, we can recursively define an increasing sequence of natural numbers $n_1 < n_2 < n_3 < \cdots$ such that $q_{n_k}=a_k$ for all $k$, and hence $\{a_k\}_{k=1}^\infty$ is a subsequence of $\{q_n\}_{n=1}^\infty$.
\end{proof}

On the other hand, there is no sequence that actually contains every real number. To prove this, we will use decimal expansions, as discussed on the homework.

Recall that if $d_1,d_2,d_3,\dots$ is a sequence of ``digits'', where $d_i\in\{0,1,2,3,4,5,6,7,8,9\}$ for every $i$, then the sequence $\{q_n\}_{n=1}^\infty$, where 
\[ q_n = \frac{d_1}{10^1} + \frac{d_2}{10^2} + \cdots + \frac{d_n}{10^n}\]
converges, and we say that $.d_1d_2d_3\cdots$ is a \textit{decimal expansion} for the real number $r=\lim_{n\to \infty} q_n$.




\begin{thm}[Cantor's Theorem] There is no sequence that contains every real number.
\end{thm}

\begin{proof} By way of contradiction, suppose $\{a_n\}_{n=1}^\infty$ is a sequence in which every real number appears at least once.
	Write each member of this sequence in
	its decimal form, so that
	$$
	\begin{aligned}
	a_1 & = (\text{whole part}). d_{1,1} d_{1,2} d_{1,3} \cdots \\
	a_2 & = (\text{whole part}). d_{2,1} d_{2,2} d_{2,3} \cdots \\
	a_3 & = (\text{whole part}). d_{3,1} d_{3,2} d_{3,3} \cdots \\
	\vdots & \\
	\end{aligned}
	$$
	where each $d_{i,j}\in \{0,1,2,3,4,5,6,7,8,9\}$ is a digit. Now form a real number $x$ as $0.e_1 e_2 e_3 \cdots$ where the $e_i$'s are digits chosen as follows: 
	Let 
	\[e_i=\begin{cases} 9 & \text{if} \ d_{i,i}\leq 5\\
	0 & \text{if}\ d_{i,i}> 5.\end{cases}\]
	 In particular, $e_i\neq d_{i,i}$ for every $i$.
	 Then 
	$x \ne a_1$ since these two numbers have different  first digits, 
	$x \ne a_2$ since these two numbers have different  second digits, etc.; in general, for any $n$, $x \ne a_n$ since these two numbers have different digits in
	the $n$-th position.
	
	Thus $x$ is not a member of this sequence, contrary to what we assumed. 
\end{proof}

It is worth noting that the proof given above needs a little more justification because two different decimal expansions can converge to the same number. I'll leave it as a challenge to you to prove that 


Our next big theorem has a very short statement, but is surprisingly hard to prove.


\begin{thm}[Bolzano-Weierstrass Theorem]\label{thm214}
Every sequence has a monotone subsequence.
\end{thm}



The proof of this theorem requires two preliminary lemmas.



\begin{lem} \label{lem214a}
	If a sequence is not bounded above, then it has a subsequence that is
	increasing. 
	If a sequence is not bounded below, then it has a subsequence that is
	decreasing.
\end{lem}

\begin{proof} Suppose $\{a_n\}_{n=1}^\infty$ is not bounded above. This means that for each real number $M$, there is a natural number such that $a_n > M$. 
Using this assumption, we will build a strictly  increasing sequence of natural numbers $n_1 < n_2 < \cdots$ so that the subsequence $\{a_{n_k}\}_{k=1}^\infty$ is increasing. We will define this sequence recursively.

We start by just letting $n_1 = 1$. 

If we have chosen $n_k$, then let $b=\max\{a_1,\dots,a_{n_k}\}$. Since $b$ is not an upper bound of the sequence $\{a_n\}_{n=1}^\infty$, there exists some $m\in \N$ such that $a_m>b$. We must have $m> n_k$, since otherwise $a_m$ is on the list $a_1,\dots,a_{n_k}$ so that $a_m \leq b$. Thus, we can take $n_{k+1} = m$, and we have $n_{k+1} > n_k$ and $a_{n_{k+1}}>a_{n_k}$. Thus, we have defined the desired subsequence recursively.

The proof for the case of sequences that are not bounded below is similar.
\end{proof}

\begin{lem} \label{lem214b}
Let $\{a_n\}_{n=1}^\infty$ be a sequence.
 Assume $\{a_n\}_{n=1}^\infty$ is bounded above and let $\b$ be the supremum of the numbers appearing in the sequence. 
If $\b$ does not occur in the sequence, then the sequence contains  an increasing subsequence.
\end{lem}

\Oct{13}







\begin{proof}
Assume $\{a_n\}_{n=1}^\infty$ is bounded above, $\b$ is  the supremum of the numbers appearing in the sequence, and $\beta$ does not occur in the sequence.
Notice that these conditions mean: (1) $a_n < \b$ for all $n$ and (2) if $y$ is any real number such that $y < \b$, then there exists a natural number $n$ such that $a_n > y$.
Moreover, from (1) it follows that for all $n$, we have $\max\{a_1, a_2, \dots, a_n\} < \beta$. 

We again define our subsequence recursively.
We start by setting $n_1 = 1$.

If we have chosen $n_k$, then let $b=\max\{a_1,\dots,a_{n_k}\}$. By (1) above, $b<\beta$, and by (2) above, there is some $m\in \N$ for which $b < a_m$.  We must have $m> n_k$, since otherwise $a_m$ is on the list $a_1,\dots,a_{n_k}$ so that $a_m \leq b$. Thus, we can take $n_{k+1} = m$, and we have $n_{k+1} > n_k$ and $a_{n_{k+1}}>a_{n_k}$. Thus, we have defined the desired subsequence recursively.
\end{proof}


\begin{rem} We will actually use the contrapositive of the  Lemma in the proof of the Theorem:
\begin{quote}
If  $\{a_n\}_{n=1}^\infty$ is a bounded above sequence that does not contain any increasing subsequences, 
then the supremum of the terms of the sequence must occur somewhere in the sequence.
\end{quote}
\end{rem}




\begin{proof}[Proof of Bolzano-Weierstrass Theorem \ref{thm214}]
Let $\{a_n\}_{n=1}^\infty$ be any sequence. Recall that our goal is to prove it either has an increasing subsequence or it has a decreasing subsequence. We
consider two cases.

{\bf Case I}: The sequence is not bounded.

Then it is not bounded above or it is not bounded below, and in either situation, Lemma \ref{lem214a} gives the result. 

{\bf Case II}: The sequence  is bounded. 

We need to prove that it either has an increasing subsequence or a decreasing subsequence. This is equivalent to showing that if it has no increasing
subsequences, then it does have at least one  decreasing subsequence.
So, let us assume it has no increasing subsequences. 


We will prove it has at least one
decreasing subsequence by constructing the indices $n_1<n_2< \cdots$ of such a subsequence one at a time. 

Let $\beta_1$ be the supremum of all the terms of the sequence. By Lemma~\ref{lem214b} (see the remark following it), since $\{a_n\}_{n=1}^\infty$ is bounded
above and does not contain any increasing subsequences, we know that  
$\beta_1$ must be {\em in the sequence}. That is,
there exists a natural number $n_1$ such that $\beta_1 = a_{n_1}$. Note that it follows that $a_{n_1}
\geq a_m$ for all $m \geq 1$. 


For any $k$, given $n_k$, the subsequence $a_{n_k +1} , a_{n_k+2},a_{n_k+3},\ldots$ is also bounded above and has no increasing subsequence. Thus, it must contain its supremum $\beta_{k+1}$ by Lemma~\ref{lem214b}. So, $\beta_{k+1} = a_{m}$ for some $m>n_k$. Choose $n_{k+1} =m$ for such a value $m$. This gives a recursive definition for $n_k$.

By construction, we have $n_{k+1} > n_k$ for all $k$. Note that $a_{n_{k}} = \beta_{k}$ is the supremum of a set that contains $a_{n_{k+1}}=\beta_{k+1}$. It follows that $a_{n_k} \geq a_{n_{k+1}}$. That is, we have constructed a decreasing subsequence of the original sequence.
\end{proof}

\Oct{15}



\begin{cor}[Main Corollary of Bolzano-Weierstrass Theorem] Every bounded sequence has a convergent subsequence.
\end{cor}

\begin{proof} Suppose $\{a_n\}_{n=1}^\infty$ is a bounded sequence. 
By the Bolzano-Weierstrass Theorem \ref{thm214} it admits a monotone subsequence
$\{a_{n_k}\}_{k=1}^\infty$, and it too is bounded (since 
any subsequence of a bounded sequence is also bounded.) 
The result follows since every monotone bounded sequence converges by the Monotone Convergence Theorem \ref{thm:MCT}.
\end{proof}



\begin{defn} A sequence $\{a_n\}_{n=1}^\infty$ is called a Cauchy sequence if for every $\e>0$, there is some $N\in \R$ such that for all $m,n\in \N$ such that $m>n>N$, we have $|a_m - a_n|<\e$.
\end{defn}

 Loosely speaking sequence is Cauchy if eventually all the terms are very close together. The most important fact about Cauchy sequences is the following:



\begin{thm} A sequence is a Cauchy sequence if and only if it converges.
\end{thm}

\begin{enumerate}
	\item Prove that the sequence $\{ \frac{1}{n} \}_{n=1}^\infty$ is a Cauchy sequence using the definition.%\footnote{Hint: Take $N=\frac{1}{\e}$.}
	\begin{framed}
	 Pick $\e > 0$. (Scratch work: we need to figure out
			how big $m$ and $n$ need to be in order that $|\frac{1}{m} - \frac{1}{n}| < \e$.
			Note that if both $\frac{1}{n}$ and $\frac{1}{m}$ are between $0$ and $\e$, then the distance between them will be at most $\e$,
			and this occurs so long as $n,m > \frac{1}{\e}$. So we will set $N = \frac{1}{\e}$. Back to the proof.) 
			Let $N = \frac{1}{\e}$. Let  $m,n \in \N$ be such that $m > n > N$. Then $0 < \frac{1}{n} < \frac{1}{N} = \e$
			and     $0 < \frac{1}{m} < \frac{1}{N} = \e$. It follows that 
			$$
			|a_m - a_n| = |\frac{1}{m} -  \frac{1}{n}| < \e
			$$
			and this proves the sequence is Cauchy.
		
	\end{framed}
	
	
%	\item Write, in simplified form, precisely what it means for a sequence to \textit{not} be a Cauchy sequence.
	
%\begin{framed}
%	There exists some $\e>0$ such that for all $N\in \R$, there are natural numbers $m,n$ with $m>n>N$ such %that ${|a_m - a_n|\geq \e}$.
%	\end{framed}
	
	\item Prove that every convergent sequence is a Cauchy sequence.%\footnote{Hint: Given $\e>0$, apply the definition of ``converges to $L$'' with the positive number $\frac{\e}{2}$ (where we usually write $\e$).}
	
	\begin{framed}
		Assume $\{a_n\}_{n=1}^\infty$ converges to $L$.  Pick $\e > 0$. We
		apply the definition of ``converges'' to the sequence $\{a_n\}_{n=1}^\infty$, which converges to $L$, using the positive number  $\frac{\e}{2}$.
		We get that there is a real number  $N$ such that if $n \in \N$ and $n > N$, then $|a_n - L| < \frac{\e}{2}$. I claim
		that this same number $N$ ``works'' to prove the sequence is Cauchy:
		Assume $m$ and $n$ are natural numbers such that  $m > N$ and  $n > N$. By the triangle inequality
		$$
		|a_m - a_n| \leq |a_m - L| + |L - a_n| < \frac{\e}{2} + \frac{\e}{2} = \e.
		$$
		This proves $\{a_n\}_{n=1}^\infty$ is a Cauchy sequence.
		\end{framed}
	
	\item Prove that every Cauchy sequence is bounded.%\footnote{Hint: Take $\e=1$ (or any other positive number. Consider $a_n$ for some $n>N$ where $N$ is the number we get from the definition. Focus first on bounding all of the values $a_m$ with $m>n$.}
	
	
	\begin{framed}
		 Using $\e = 1$ in the definition of ``Cauchy'', 
		we get that there is an $N$ such that if $m,n \in \N$ are such that  $m > N$ and $n > N$, then we have $|a_m - a_n| < 1$. 
		Let $k$ be the smallest natural number that is bigger than $N$, and let
		$M$ be the  maximum of the numbers
		$$
		a_1, \dots, a_{k-1}, a_k + 1.
		$$
		I claim $M$ is an upper bound of this sequence. 
		Given $n \in \N$, if $n < k$, then $a_n \leq M$ since in this case $a_n$ occurs in the above list. 
		If $n \geq k$ then we have $|a_n - a_k| < 1$ and thus  $a_n < a_k + 1 \leq M$. So, $M$ is indeed an upper bound of the sequence. 
		
		A similar argument shows that the sequence is bounded below by the minimum number in the list $a_1, \dots, a_{k-1}, a_k - 1$.
		\end{framed}
	
	\item Prove that every Cauchy sequence has a convergent subsequence.
	
	\begin{framed} Since a Cauchy sequence is bounded, it has a convergent subsequence by the Main Corollary to Bolzano-Weierstrass.
		\end{framed}
	
	\item Prove that every Cauchy sequence converges.%\footnote{Hint: By the previous part, we can take a subsequence $\{a_{n_k}\}_{k=1}^\infty$ that converges to some real number $L$. Taking $\frac{\e}{2}$ in the definition of $\{a_n\}_{n=1}^\infty$ is Cauchy gives us a ``magic number'' $N_1$, and taking $\frac{\e}{2}$ in the definition of $\{a_{n_k}\}_{k=1}^\infty$ converges to  $L$ gives us a ``magic number'' $N_2$. Use the triangle inequality with $|a_{n_k} - L|$ and $|a_n - a_{n_k}|$ for some $k>\max\{N_1,N_2\}$.}
	
	\begin{framed}
		Given a Cauchy sequence, it
		has a convergent subsequence $\{a_{n_k}\}_{k=1}^\infty$; let's say that this subsequence converges to $L$. We will prove
		$\{a_n\}_{n=1}^\infty$ itself converges to $L$, using that it is Cauchy.   
		
		Pick $\e > 0$. Since the sequence is Cauchy and $\frac\e2 > 0$, by definition
		there is a number $N$ such that if $m > n > N$ then $|a_m - a_n| <
		\frac\e2$. We will prove that this $N$ ``works'' to show $\{a_n\}_{n=1}^\infty$ converges to $L$; that is, I claim that if $n > N$, then $|a_n -L| < \e$. 
		
		So, let $n>N$. Since the subsequence $\{a_{n_k}\}_{k=1}^\infty$ converges to  $L$ and
		$\frac\e2$ is positive, there is a real number $K$ such that if $k > K$ then $|a_{n_k} - L| < \e/2$. 
		Let $k$ be any natural number such that $k > \max\{K,n\}$. We have $n_k > n >N$ and, since $n_k \geq k$, $n_k > K$.
		
Thus,  by the bounds above and the triangle inequality,
		$$
		|a_n - L| \leq |a_n - a_{n_k}| + |a_{n_k} - L| < \frac\e2 + \frac\e2 = \e.
		$$
		This shows that $\{a_n\}_{n=1}^\infty$ converges to $L$.
				\end{framed}
	
\end{enumerate}



\Oct{20}




We are now going to start talking about functions, limits of functions, continuity of functions, etc.

In general, if $S$ and $T$ are any two sets, a {\em function} from $S$ to $T$, written
$$
f: S \to T
$$
is any ``rule'' that assigns to each element $s \in S$ and unique element $t \in T$. The element of $T$ that is assigned to $s$ by this rule is written $f(s)$.

This is not really a very good definition since ``rule'' itself is not defined. A more careful definiton is: Given sets $S$ and $T$, let $S \times T$ denote the
set consisting of all ordered pairs $(s,t)$ where $s \in S$ and $t \in T$. Then a {\em function} $f$ from $S$ to $T$ is a subset $G$ of $S \times T$ having
the following property: For each $s \in S$ there is a unique $t \in T$ such that $(s,t) \in G$. In other words, a function is by definition given by its graph.




In this class, we will almost always consider functions of the form
$$
f: S \to \R
$$
where $S$ is a subset of $\R$. Indeed, henceforth, let us agree that if I say ``function'' I mean a function of the form $f: S \to \R$ for some subset $S$ of $\R$. Recall that the {\em
  domain} of a function refers to the subset $S$ of $\R$ on which it is defined. 


Often, but certainly not always, $f$ will indeed by given by a formula, such as $f(x) = \frac{x^2-1}{x-1}$. In such cases, 
we will usually be a bit sloppy in specifying its domain $S$. For example, if I say
``consider the  function $f(x) = \frac{x^2-1}{x-1}$'', it is understood that its domain is every real number on which this formula is well-defined. In this
example, that would be $S = \R \setminus \{1\} = \{ x \in \R \mid x
\ne 1 \}$. Following this convention, $f(x) = \frac{x^2-1}{x-1}$ and $g(x)=x+1$ are two different functions, since their domains are different.

It is also worth noting that while most of the functions we consider will be given by formulas, there are many functions that cannot be expressed in terms of formulas. Imagine for every real number $x$ flipping a coin and setting $f(x)=1$ if coin $x$ turns up heads and $f(x)=0$ if coin $x$ turns up tails. The result will certainly be a function, albeit an unimaginably wild one.

Many times, the domain of the functions we talk about will be intervals:
$$
(a,b), (a,b], [a,b), [a,b], (a,\infty), (-\infty,b), [a,\infty), (-\infty,b], (-\infty, \infty) 
$$


% The following formal definition will be convenient:

% \begin{defn} Let $f: S \to \R$ be a function (where $S$ is a subset of
%   $\R$). For any real number $a$ (not necessarily in $S$), we say that
%   $f$ is {\em defined near, but not    necessarily at,  $a$} (I'll write ``$f$ is dnbnna $a$'' for short)
%    if there exists a positive real number $\d > 0$ such that the domain of $f$ contains  $(a-\delta, a+\delta) \setminus \{a\}$; that is,
% $f$ is defined for all $x$ such that either  $a - \delta < x < a$ or
% $a < x < a + \delta$.
% \end{defn}

% \begin{ex}  Let $f(x) = \frac{5x^2-5}{x-1}$ and note that, by our convention, since this formula is defined for all $x \ne 1$, the domain of $f$ is $S = \R \setminus \{1\}$.
%   For any $a \ne 1$, $f$ is defined near, but not necessarily at,  $a$: To see this, take $\d = |a-1|$ and note that for this choice of $\d$, $f$ is defined at every point of
%   $(a-\delta, a+ \delta) \setminus \{a\}$ since $1$ is not in this set. (If $a > 1$,   $(a-\delta, a+ \delta) \setminus \{a\} = (1,a) \cup (a,2a-1)$, and if $a < 1$ then
%   $(a-\delta, a+ \delta) \setminus \{a\} = (2a-1,a) \cup (a,1)$.) Note that for $a \ne 1$, $f$ is of course defined at $a$ too.

  
%   When $a = 1$, $f$ is also defined near, but not necessarily at, $1$. To see this we may take $\d = 1$ (or any positive number), since $f$ is defined at $(0,2) \setminus \{1\}$.
%   end{ex}

%   \begin{ex} Let $f(x) = \sqrt{x}$, and note that the domain of $f$ is $[0, \infty)$. For any $a > 0$, $f$ is defined near, but not necessarily at,  $a$. To see this, take $\delta
%     = a$, so that $(a- \delta, a + \delta) \setminus \{a\} = (0,2a) \setminus (a) = (0,a) \cup (a,2a)$. (Letting $\delta$ be any strictly positive number less than $a$ would also work.)
% But note that $f$ is {\em not} dnbnna $0$, since there is $\d > 0$, such that $f$ is defined on $(-\delta, 0)$. 
% \end{ex}

%% With these notions in place, we can finally define what a limit of a function is:

\begin{defn} Let $f: S \to \R$ be a function (where $S$ is a subset of $\R$) and let $a \in \R$ be any real number. 
  For a real number $L$, we say {\em the limit of $f(x)$ as $x$ approaches $a$ is $L$} provided the following condition is met:
\begin{quote}
  For every $\e > 0$, there is $\d > 0$ such that if $x$ is any real number such that $0 < |x-a| < \d$, then $f$ is defined at $x$ and $|f(x) - L| < \e$. 
\end{quote}

\end{defn}



\begin{ex} Let $f$ be the function given by the formula
$$
 f(x) = \frac{5x^2-5}{x-1}.
$$
Recall our convention that we interpret the domain of $f$ to be all real numbers where this rule is defined. So, $f: S \to \R$ where $S = \R \setminus \{1\}$.

I claim that the limit of $f(x)$  as $x$ approaches $1$ is $10$.
Pick $\e > 0$.

%(Scratch work: Since $f$ is defined at all points other that $1$, the condition about $f$ being defined for all $x$ such that $0 < |x-a| < \d$ will be  met for any choice of $\delta$. 
%We need $|f(x) - 10| < \e$ to
%hold. Manipulating this a bit, we see that it is equivalent to $|x-1|  < \frac{\e}{5}$. Thus setting $\d = \frac{\e}{5}$ is the way to go. Back to the proof....)

Let $\d = \frac{\e}{5}$. Pick $x$ such that $0 < |x-1| < \d$. Then $x \ne 1$ and hence $f$ is defined at $x$. We have
$$
\begin{aligned}
|f(x) &- 10|  =  \left|\frac{5x^2-5}{x-1} - 10\right|=  \left|\frac{5x^2-5-10x+10}{x-1}\right|  =  \left|\frac{5x^2-10x + 5}{x-1}\right| \\ 
& =  \left|\frac{5(x^2-2x +1)}{x-1}\right| =  \left|\frac{5(x-1)^2 }{x-1}\right|  =  |5x-5|  =  5|x-1| < 5 \d  = \e \\
\end{aligned}
$$
We have shown that for any $\e>0$ there is a $\d>0$ such that 
if $0 < |x-1|< \d$, then $f$ is defined at $x$ and $|f(x) - 10| < \e$. This proves $\lim_{x \to 1} f(x) = 10$.  
\end{ex}

\Oct{23}

Here is an equivalent formulation of the definition of limit: 
\begin{quote}
  For every $\e > 0$, there is $\d > 0$ such that if $x$ is any real number such that either $a-\d < x < a$ or $a < x < a + \d$, then $f$ is defined at $x$ and $|f(x) - L| < \e$. 
\end{quote}

As a matter of shorthand, we write $\lim_{x \to a} f(x) = L$ to mean
the limit of $f(x)$ as $x$ approaches $a$ is $L$.

Note that in order for the limit of $f$ at $a$ to exist,
we need in particular that there is a $\d > 0$ such that $f$ is defined at every point on $(a-\d, a)$ and $(a, a+ \d)$. Loosely, $f$ needs to be defined
at all points near, but not necessarily equal to, $a$. If the domain of $f$ is $\R$ or  $\R \setminus \{a\}$, this condition is automatic.

The more important condition is that if $0 < |x-a| < \d$, then $|f(x) - L| < \e$. Intuitively, this is saying that no matter how small of a positive number $\e$ you pick,
if you only look at  inputs very close to (but not equal to) $a$,
the function values at these inputs are within a distance of $\e$ of the limiting value $L$. 


\begin{ex} Let's do a more complicated example: Let $f(x) = x^2$ with  domain all of $\R$. I claim that $\lim_{x \to 2} x^2 = 4$. This is intuitively
  obvious but we need to prove it using just the definition.

\begin{proof}
  Pick $\e > 0$.

  (Scratch work: The domain of $f$ is all of $\R$ and so we don't need to worry at all about whether $f$ is defined at all.
  We need to figure out how small to make $\d$ so that if $0 < |x-2| < \d$ then $|x^2 - 4| < \e$. The latter
is equivalent to $|x-2||x+2| < \e$. We can make $|x-2|$ arbitrarily small by making $\d$
aribitrarily small, but how can we handle $|x+2|$? The trick is to bound it appropriately. This can be done in many ways. Certainly we can choose $\d$ to
be at most $1$, so that if $|x-2| < \d$ then $|x-2| < 1$ and hence $1 < x < 3$, so that $|x+2| < 5$. So, we will be allowed to assume $|x+2| < 5$. Then
$|x-2||x+2| < 5 |x-2|$ and $5 |x-2| < \e$ provided $|x-2| < \frac{\e}{5}$. Back to the formal proof...)

Let $\d = \min\{\frac{\e}{5}, 1\}$. Let $x$ be any real number such that  ${0 < |x-2| < \d}$. Then certainly $f$ is defined at $x$. 
Since $\d \leq 1$ we get $|x-2| < 1$ and hence $|x+2| \leq
5$. Since $\d \leq \frac{\e}{5}$ we have  $|x-2|  < \frac{\e}{5}$. Putting these together gives
$$
|f(x) - 4| = |x^2 - 4| = |x-2||x+2| < |x-2| 5 < \frac{\e}{5} 5 = \e.
$$
This proves $\lim_{x \to 2} x^2 = 4$. 
\end{proof}
\end{ex}


Let's give an example of a function that does {\em not} have a limiting value as $x$ approaches some number $a$. 

\begin{ex} Let  $f(x)   = \frac{1}{x-3}$ with domain $\R \smallsetminus \{3\}$. I claim that the limit of $f(x)$ as $x$ approaches $3$ does not exist. To prove this, by way of
contradiction, suppose the limit of $f(x)$ as $x$ approaches $3$ does exist and is equal to $L$. Taking $\e = 1$ in the definition, there is a $\d > 0$ so that if
$0 < |x-3| < \d$, then $\left|\frac{1}{x-3} - L\right| < 1$. We can find a real number  $x$ so that both $3 < x < 3.05$ and $0 < |x-3| < \d$ hold. For such
an  $x$ we have 
$\left|\frac{1}{x-3} - L\right| < 1$ and so 
$$
\frac{1}{x-3} - 1 <  L < \frac{1}{x-3} + 1,
$$
and we also have 
$0 < x-3 < .05$ and so 
$\frac{1}{x-3} > 20$. It follows that
$$
L > 19.
$$
Now pick $x$ such that $2.95 < x < 3$ and $0 < |x-3| < \d$.  We get
$$
\frac{1}{x-3} - 1 <  L < \frac{1}{x-3} + 1,
$$
and $\frac{1}{x-3} < -20$ and hence
$$
L < -19.
$$
This is not possible; so the limit of $f(x)$ as $x$ approaches $3$ does not exist. 
\end{ex}


\Oct{25}

\begin{enumerate}
	\item Explain why $f(x)=\sqrt{x}$ does not have a limit as $x$ approaches~$0$.
		
	\begin{framed}
	The domain of $f(x)$ consists of all nonnegative numbers. For any $\delta>0$, there is some negative number $x$ such that $0<|x|<\delta$, so there is no $\delta>0$ such that $f(x)$ is defined for all numbers satisfying that inequality.
	\end{framed}
	
	\item  Let $f(x)$ be any linear function: $f(x) = mx +b$ where $m$ and $b$ are fixed real numbers. Let $a$ be any real number.
	\begin{itemize}
	\item Prove that $\lim_{x\to a} f(x) = f(a)$ in the case $m=0$.
	\begin{framed}
	Suppose $m = 0$. Then set $\d = 10^{100}$ (or any positive number you like).  If $0 < |x - a| < \d$, then $f$ is defined at $x$ and $|f(x) - f(a)| = |b - b| = 0 < \e$. This proves
$\lim_{x \to a} f(x) = f(a)$.
\end{framed}
	\item Prove that $\lim_{x\to a} f(x) = f(a)$ in the case $m\neq 0$.
	\begin{framed}
	Suppose $m \ne 0$. Set $\d = \frac{\e}{|m|}$. If \\
$0 < |x - a| < \d$, then $f$ is defined at $x$ and 
$\begin{aligned}
|f(x) - f(a)| &= |mx + b - (ma +b)|
\\& = |m(x-a)| = |m| |x-a| < |m| \d = \e.
\end{aligned}$
This proves $\lim_{x \to a} f(x) = f(a)$.
\end{framed}
	\end{itemize}
	
	
	
	\item Let $f$ be a function. Prove that the limit as of $f$ as $x$ approaches $a$, if it exists, is unique.
	
	\begin{framed} Suppose that the limit of $f$ as approaches $a$ is both $L$ and $M$. By way of contradiction, suppose that $L\neq M$. Take $\e=\frac{|L-M|}{2}$. By definition, there exist $\delta_1,\delta_2>0$ such that if $0<|x-a|<\delta_1$ then $|f(x)-L|<\e$, and if $0<|x-a|<\delta_2$ then $|f(x)-M|<\e$. Let $x$ be some real number such that $0<|x-a|<\min\{\delta_1,\delta_2\}$. Then
	\[ |L-M| \leq |f(x) -L| + |f(x) - M| < \e + \e = |L-M|.\]
	This is a contradiction, so we must in fact have $L=M$.
	\end{framed}
	
	\item Using any basic facts from trig, compute \,$\lim_{x\to 0} \sin(\frac{1}{x})$.
	\begin{framed}
	We will show that there is no limit of $\sin(\frac{1}{x})$ as $x$ approaches $0$. By way of contradiction, suppose that there is such a limit $L$. Take $\e = 1$. By definition, there is some $\delta>0$ such that $|\sin(\frac{1}{x}) - L| <\e$ for all $x$ such that $0<|x|<\delta$. 
	
	We consider two cases.
 First, assume that $L\geq 0$. There is some natural number $N$ greater than $1/\delta$. Then \[2\pi N + \frac{3\pi}{2} > \frac{1}{\delta}\] as well, so \[x=\frac{1}{2\pi N + \frac{3\pi}{2}}< \delta,\] and \[\sin(1/x)=\sin(2\pi N + \frac{3\pi}{2}) = -1\] so $|\sin(1/x) - L| \geq 1=\e$. Thus $L\geq 0$ is impossible.
 
 Now, assume that $L< 0$. There is some natural number $N$ greater than $1/\delta$. Then \[2\pi N + \frac{\pi}{2} > \frac{1}{\delta}\] as well, so \[x=\frac{1}{2\pi N + \frac{\pi}{2}}< \delta,\] and \[\sin(1/x)=\sin(2\pi N + \frac{\pi}{2}) = 1,\] so $|\sin(1/x) - L| \geq 1=\e$. Thus $L< 0$ is impossible too.

We conclude that no such $L$ exists.
	\end{framed}	
	\item Come up with a definition of $\lim_{x\to \infty} f(x) = L$ and prove, using your definition, that
	\[ \lim_{x\to \infty} \frac{2x-1}{x+3} = 2.\]
	
	
\end{enumerate}



%\begin{ex} Let $f(x)$ be any linear function: $f(x) = mx +b$ where $m$ and $b$ are fixed real numbers. (The domain of $f$ is all of $\R$. )
%Let $a$ be any real number. I claim 
%$$
%\lim_{x \to a} f(x) = f(a).
%$$
%(As we will see, this is equivalent to the property that $f$ is ``continuous'' at $x= a$.)
%Pick $\e > 0$.
%
%(Scratch work: We need $|f(x) - f(a)| < \e$ and this is equivalent to $|m(x-a)| < \e$ which in turn (provided $m \ne 0$) is equivalent to $|x-a|
%< \frac{\e}{|m|}$.  If $m = 0$, then $|f(x) - f(a)| < \e$ is automatic.)
%
%We proceed in cases.
%
%{\bf Case I}: Suppose $m = 0$. Then set $\d = 10^{100}$ (or any
%positive number you like).  If $0 < |x - a| < \d$, then $f$ is defined at $x$ and $|f(x) - f(a)| = |b - b| = 0 < \e$. This proves
%$\lim_{x \to a} f(x) = f(a)$.
%
%{\bf Case II}: Suppose $m \ne 0$. Set $\d = \frac{\e}{|m|}$. If 
%$0 < |x - a| < \d$, then $f$ is defined at $x$ and 
%$$
%|f(x) - f(a)| = |mx + b - (ma +b)| = |m(x-a)| = |m| |x-a| < |m| \d = \e.
%$$
%This proves $\lim_{x \to a} f(x) = f(a)$.
%\end{ex}






%\begin{ex} Let $f(x) = \sqrt{x}$. Does $\lim_{x \to 0} f(x) = 0$? No. Since the domain of $f$ is $[0, \infty)$, there is no $\d > 0$ such that $f$ is defined
%  at all $x$ satisfying $0 < |x-0| < \d$ (e.g., for any $\d > 0$, $f$ is not defined at $-\d/2$).
  
%But, later, we will talk about one-sided limits, and we will see that  it is true that $\lim_{x \to 0^+} \sqrt{x} = 0$.
%\end{ex}


\Oct{27}



The following result gives an important connection between limits of functions and limits of sequences. This Lemma will allow us to translate
statements we have proven about limits of sequences to limits of functions.



\begin{thm} \label{lem228}
  Let $f(x)$ be a function and let $a$ be a real number.
  Let $r > 0$ be a positive real number such that
  $f$ is defined
  at every point of $\{x \in \R \mid 0 < |x-a| < r\}$.
    Let $L$ be any real number. 

$\lim_{x \to a} f(x) = L$ if and only if for every sequence 
$\{x_n\}_{n=1}^\infty$ that converges to $a$ and satisfies $0 < |x_n - a| < r$ for all $n$, we have that the sequence $\{f(x_n)\}_{n=1}^\infty$ converges to $L$. 
\end{thm}

 Loosely, the condition that there is an $r > 0$ such that 
$f$ is defined at every point of $\{x \in \R \mid 0 < |x-a| < r\}$ says that ``$f$ is defined near, but not necessarily at, $a$''. 




%Before proving the Theorem, let us illustrate it:

%\begin{ex} Let us prove that 
%$$
%\lim_{x \to 2} \frac{3x^2 - x + 2}{x + 3} = \frac{12}{5}.
%$$ 
%Using the definition alone would be messy, but thanks to the Lemma and
%what we already know about sequences, it is easy.

%Note that $f$ is defined at every point of $(1,2) \cup (2,3)$ and so the Theorem applies with $r = 1$. 
%Pick any sequence $\{x_n\}_{n=1}^\infty$ that converges to $2$ such that $1 < x_n < 3$ and $x_n \ne 2$. By the
%Theorem concerning  sums, products, and quotients of limits of %sequence we have
%$$
%\lim_{n \to \infty} \frac{3x_n^2 - x_n + 2}{x_n + 3} = \frac{2 \cdot 2^2 - 2 + 2}{2 + 5} = \frac{12}{5}.
%$$
%So, by the Theorem, 
%$$
%\lim_{x \to 2} \frac{3x^2 - x + 2}{x + 3} = \frac{12}{5}.
%$$ 
%\end{ex}




\begin{proof}Let $f$ be a function, $a \in \R$, and $r > 0$ a
  positive real number such that $f$ is    defined on $\{x \in \R \mid 0 < |x-a| < r \}$. 

  ($\Rightarrow$) Assume 
$\lim_{x \to a} f(x) = L$. Let 
$\{x_n\}_{n=1}^\infty$ be any sequence that converges to $a$ and is such that $0 < |x_n-a| < r$ for all $n$.
We need to prove that the sequence $\{f(x_n)\}_{n=1}^\infty$ converges to $L$. 

Pick $\e > 0$. By definition of the limit of a function, there is a $\d > 0$ such that if
$0 < |x-a| < \d$, then $f$ is defined at $x$ and ${|f(x) - L| < \e}$.
Since $\d > 0$ and $\{x_n\}_{n=1}^\infty$ converges to $a$, by the definition of convergence, there is an $N$ such that if
$n \in \N$ and $n > N$ then $|x_n - a| < \d$. 
I claim that this $N$ ``works'' to prove $\{f(x_n)\}_{n=1}^\infty$ converges to $L$ too:
If $n \in \N$ and $n > N$, then $|x_n - a| < \d$ and, since $x_n \ne a$ for all $n$, we have  
$0 < |x_n - a| < \d$. It follows that $|f(x_n) - L| < \e$. This shows that $\{f(x_n)\}_{n=1}^\infty$ converges to $L$. 


($\Leftarrow$) We prove the contrapositive. That is, assume 
$\lim_{x \to a} f(x)$ is not $L$ (including the case where the limit does not exist).  We need to prove that
there is at least one sequence 
$\{x_n\}_{n=1}^\infty$ such that (a) it converges to $a$, (b)  $0 < |x_n -a| < r$ for all $n$ and yet
(c) the sequence $\{f(x_n)\}_{n=1}^\infty$ does not converge to $L$. 

The fact that $\lim_{x \to a} f(x)$ is not $L$ means:
\begin{quote}
  There is an $\e > 0$ such that for every $\d > 0$ there exists an $x \in \R$ such that
$0 < |x- a| < \d$, but   either $f$ is not  defined at $x$  or $|f(x) -L| \geq \e$.
\end{quote}
For this $\e$, for any natural number $n$, set $\d_n = \min\{\frac{1}{n}, r\}$. We get that there is a $x_n \in \R$ such
that $0 < |x_n-a| < \d_n$ and $|f(x_n) - L| \geq \e$. (Note that $f$ is necessarily defined at $x_n$ since $\d_n \leq r$.)
I claim that the sequence $\{x_n\}_{n=1}^\infty$ satisfies the needed three conditions. 
(a) Since $\delta_n \leq \frac{1}{n}$, we have $a - \frac{1}{n} < x_n < a + \frac{1}{n}$ for all $n$, and hence by the Squeeze Lemma,
the sequence $\{x_n\}_{n=1}^\infty$ converges to $a$. (b) This holds by construction, since $\d_n \leq r$. 
(c) Since, for the positive number $\e$ above, we have $|f(x_n) -L| \geq \e$ for all $n$, the sequence $\{f(x_n)\}_{n=1}^\infty$ does not converge to $L$. 
\end{proof}


\Oct{29}

\begin{cor}\label{cor:261} Let $f$ be a function and $a$ and $L$ be real numbers. Suppose that the domain of $f$ is all of $\R$ or $\R \smallsetminus \{ a\}$. Then $\lim_{x \to a} f(x) = L$ if and only if for every sequence 
	$\{x_n\}_{n=1}^\infty$ that converges to $a$ such that $x_n\neq a$ for all $n$, we have that the sequence $\{f(x_n)\}_{n=1}^\infty$ converges to $L$. 
\end{cor}
\begin{proof}
	($\Rightarrow$) Assume $\lim_{x \to a} f(x) = L$, and let $\{x_n\}_{n=1}^\infty$ be a sequence that converges to $a$ such that $x_n\neq a$ for all $n$. Since $\{x_n\}_{n=1}^\infty$ is convergent, it is bounded, so there is some $M>0$ such that $|x_n|<M$ for all $n$. Then $|x_n - a|<M+|a|$ by the Triangle Inequality. Thus, $0<|x_n - a|<M+|a|$ for all $n$, so we can apply Theorem~\ref{lem228} (with ``$r$''$=M+|a|$), so $\{f(x_n)\}_{n=1}^\infty$ converges to $L$.
	
		($\Leftarrow$) The point is that if the ``right hand side'' condition holds in this statement, then for any $r>0$, the ``right hand side'' condition of Theorem~\ref{lem228} holds.  Thus, by Theorem~\ref{lem228}, $\lim_{x \to a} f(x) = L$.
		\end{proof}

We can use this Corollary to compute limits.

\begin{ex} Let $f(x) = \sin(1/x)$. We claim that $\lim_{x\to 0} f(x)$ does not exist. (We proved this using the definition on a worksheet earlier, but we give a second proof here.) Since the domain of $f$ is $\R \smallsetminus \{0\}$, Corollary~\ref{cor:261} applies. Consider the sequence given by $\displaystyle x_n = \frac{1}{\pi n + \frac{\pi}{2}}$. We have $x_n\neq 0$ for all $x$, and $\{x_n\}_{n=1}^\infty$ converges to $0$. Thus, by Corollary~\ref{cor:261}, if $\lim_{x\to 0} f(x) = L$, then $\{f(x_n)\}_{n=1}^\infty$ converges to $L$, and in particular, is convergent. But \[ f(x_n) = \begin{cases}  -1 & \textrm{if} \ n \ \textrm{is odd}\\
1 & \textrm{if} \ n \ \textrm{is even},\end{cases}\]
so $\{f(x_n)\}_{n=1}^\infty$  is divergent. It follows that $\lim_{x\to 0} f(x) = L$ does not exist.
\end{ex} 

\begin{ex} 
Consider the function $f:\R\to\R$ given by the rule
	\[f(x) = \begin{cases} 1 &\text{if } x\in \Q \\ 0 &\text{if } x\notin \Q.\end{cases}.\]
	 We will show that, for every real number $a$, $\displaystyle \lim_{x\to a} f(x)$ does not exist.
	
			Fix $a$ and, by way of contradiction, suppose $\lim_{x \to a} f(x)$ exists and is equal to $L$. 
		Let $\{q_n\}_{n=1}^\infty$ be any sequence of rational numbers that converges to $a$ such that $q_n \ne a$ for all $n$. 
		(Such a sequence exists by Lemma \ref{lem221} above; technically this Lemma does not include the statement that $q_n \ne a$ for all $n$,
		but the proof makes it clear that there is a sequence that also has this property.) Then by  Corollary~\ref{cor:261}, $L = \lim_{n \to \infty} f(q_n)$. But $f(q_n) = 1$ for all $n$ by definition, and hence $L = 1$.
		
		On the other hand, as you proved on the homework, there also exists a sequence $\{y_n\}_{n=1}^\infty$ that converges to $a$ such that $y_n$ is {\em irrational} for each $n$ and $y_n \ne a$
		for all $n$. (Likewise, the homework problem did not include the fact that $y_n \ne a$ for all $n$, but your proof should give that too.)
		By Corollary~\ref{cor:261}, 
		$L = \lim_{n \to \infty} f(y_n)$. But $f(y_n) = 1$ for all $n$ by definition, and hence $L = 0$. This is impossible.
		\end{ex}

We can also use Theorem~\ref{lem228} to deduce some results about limits that are analogous to earlier results about sequences.

\begin{thm} \label{thm33b}
	Suppose $f$ and $g$ are two functions and that $a$ is a real number, and
	assume  that 
	$$
	\lim_{x \to a} f(x) = L \and \lim_{x \to a} g(x) = M
	$$
	for some real numbers $L$ and $M$. Then
	\begin{enumerate}
		\item $\lim_{x \to a} (f(x) + g(x)) = L  + M$.
		\item For any real number $c$, $\lim_{x \to a} (c \cdot f(x)) = c \cdot L$.
		\item $\lim_{x \to a} (f(x) \cdot g(x)) = L \cdot M$.
		\item If, in addition, we have that $M \ne 0$,
		then $\lim_{x \to a} (f(x)/g(x)) = L/M$.
	\end{enumerate}
\end{thm}


\begin{proof} Each part follows  from  Theorem \ref{lem228} and the corresponding theorem about sums, products, and quotients of sequences (Theorem~\ref{thm99}). We give the details just for one of them, part (3):
	
	First, as a technical matter, we note that since we assume
	${\lim_{x \to a} f(x) = L}$ there is a positive real number $r_1$ such that $f(x)$ is defined for all $x$ satisfying $0 < |x-a| < r_1$,
	and likewise since
	$\lim_{x \to a} g(x) = L$ there is a positive real number $r_2$ such that $g(x)$ is defined for all $x$ satisfying $0 < |x-a| < r_1$. Letting $r = \min\{r_1, r_2\}$,
	we have that $r > 0$ and $f(x)$ and $g(x)$ and hence $f(x) \cdot g(x)$ are defined for all $x$ satisfying $0 < |x-a| < r$.  (We needed to prove this in order to apply Theorem~\ref{lem228}.)
	
	
	Let $\{x_n\}_{n=1}^\infty$ be any sequence converging to $a$ such that  ${0 < |x_n -a| < r}$ for all $n$. 
	By  Theorem~\ref{lem228} in the ``forward direction'', we have that  $\lim_{n \to \infty} f(x_n) = L$ and $\lim_{n \to \infty} g(x_n) = M$. By Theorem \ref{thm99}, \\
	${\lim_{n \to \infty} f(x_n) g(x_n) = L \cdot M}$. 
	So, by  Theorem~\ref{lem228} again (this time applying it to $f(x) g(x)$ and using the ``backward
	implication''), it follows that
	$\lim_{x \to a} (f(x) \cdot g(x)) = L \cdot M$.
\end{proof}

\begin{ex} We can use this theorem to compute $\displaystyle \lim_{x\to 2} \frac{3x^2-x+2}{x+3}$. Indeed, since $\lim_{x\to 2} x = 2$, and  $\lim_{x\to 2} 1 = 1$, we have
$\lim_{x\to 2} 3 x^2 = 3\lim_{x\to 2} x^2 = 3 (\lim_{x\to 2} x)(\lim_{x\to 2} x ) = 12$, and from earlier computations, $\lim_{x\to 2} (-x+2) = (-2) + 2 = 0$ and $\lim_{x\to 2} x+3 = 2+3 = 5$, so  
$\lim_{x\to 2} 3 x^2 -x+2 = 12$, and $\displaystyle \lim_{x\to 2} \frac{3x^2-x+2}{x+3} = \frac{12}{5}$.
\end{ex}

Here is another example of the same idea.

\begin{thm} Suppose $f$, $g$, and $h$ are three functions and $a$ is a real number. Suppose there is a positive real number $r > 0$
	such that \begin{enumerate}
		\item each of $f,g,h$ is defined on $\{x \in \R \mid 0 < |x-a| < r\}$,
		\item $f(x) \leq g(x) \leq h(x)$ for all
		$x$ such that $0 < |x-a| < r$,
		\item	$\lim_{x \to a} f(x) = L = \lim_{x \to    a} h(x)$ for some number $L$.
	\end{enumerate}  
	Then $\lim_{x \to a} g(x) = L$.
	\end{thm}
	\begin{proof}
		Let $f,g,h,a,r,L$ be as in the statement. Let $\{x_n\}_{n=1}^\infty$ be a sequence that converges to $a$ and such that $0<|x_n-a|<r$ for all $n$. By Theorem~\ref{lem228}, it suffices to show that $\lim_{n \to \infty} g(x_n) = L$. By Theorem~\ref{lem228}, we know that $\lim_{n \to \infty} f(x_n) = L=\lim_{n \to \infty} h(x_n)$. Since $f(x_n)\leq g(x_n) \leq h(x_n)$ for all $n$, we have $\lim_{n \to \infty} g(x_n) = L$ by the Squeeze Theorem (for sequences).
		\end{proof}
		


%\begin{ex} We claim that $\lim_{x\to 0} x\sin(1/x) = 0$. Indeed, for all $x\neq 0$, since $|\sin(1/x)| \leq 1$ we have \[ |x \sin(1/x)| = |x| |\sin(1/x) | \leq |x|, \ \text{so}\]
%\[ -|x| \leq x\sin(1/x) \leq |x|.\]
%By the quiz, we have $\lim_{x\to 0} |x| = 0$, and using Theorem~\ref{thm33b}, we have 
%$\lim_{x\to 0} - |x| = -0 = 0$. Thus, by the Squeeze Theorem, the result holds.
%\end{ex}

\Nov{1}


We come to the formal definition of continuity. We first define what it means for a function to be continuous {\em at a single point}, but ultimately we will be
interested in functions that are continuous on entire intervals.

\begin{defn} Suppose $f$ is a function and $a$ is a real number.
  We say {\em $f$ is
    continuous at $a$} provided the following condition is met:
\begin{quote}
For every $\e > 0$ there is a $\d > 0$ such that if $x$ is a real
number such that $|x - a| < \d$ then $f$ is defined at $x$ and $|f(x) - f(a)| < \e$.
\end{quote}
\end{defn}

\begin{rem} If $f$ is continuous at $a$, then by applying the definition using any postive number $\e > 0$ you like (e.g., $\e = 1$) we get that  
there exists a $\d> 0$ such that $f$ is defined for all $x$ such that $a-\d < x < a+
\d$. That is, in order for $f$ to be continuous at $a$ it is necessary (but not sufficient) that $f$ is defined at all points near $a$ {\em including at $a$ itself}.
In particular, unlike in the
  definition of ``limit'', $f$ must be defined at $a$ in order for it to possibly be continuous at $a$.
\end{rem}

\begin{ex} I claim $f(x) = 3x$ is continuous at $a$ for every value of $a$. Pick $\e > 0$. Let $\d = \frac{\e}{3}$. If $|x - a| < \d$ then $f$ is defined at $x$ (since the domain
  of $f$ is all of $\R$) and
  $$
  |f(x) - f(a)| =
  |3x-3a| = 3|x-a| < 3 \d = \e.
  $$
\end{ex}

\begin{ex} The function $f(x)$ with domain $\R$ defined by
$$
f(x) = \begin{cases}
2x - 7 & \text{ if $x \geq 3$ and } \\
-x  & \text{ if $x < 3$} \\
\end{cases}
$$
is not continuous at $3$. Since the domain of $f$ is all of $\R$, the negation of the defintion of ``continuous at $3$'' is:
\begin{quote}
there is an $\e > 0$ such that for every $\d > 0$ there is a real number $x$ such that $|x-3| < \d$ and $|f(x) - f(3)| \geq \e$. 
\end{quote}

Set $\e = 1$. For any $\d > 0$, we may choose a real number $x$ so that $3 - \d < x < 3$ and $2.9 < x < 3$. For such an $x$, we have
$$
|f(x) - f(3)| = |-x+1| = x-1 > 1.9 > \e.
$$
The proves $f$ is not continuous at $3$.
\end{ex}



The definition of continuous looks a lot like the definition of limit, with $L$ replaced by $f(a)$. This is not just superficial:

\begin{thm} \label{thm36}
Suppose $f$ is a function and $a$ is a real number and assume that $f$ is defined at $a$.  $f$ is continous at   $a$ if and only if $\lim_{x \to a} f(x) = f(a)$.
\end{thm}



\begin{proof} ($\Rightarrow$) This is immediate from the definitions.

($\Leftarrow$) This is almost immediate from the definitions: 
Suppose $\lim_{x \to a} f(x) = f(a)$. Pick $\e>0$. Then there is a $\d$ such that if $0 < |x - a| < \d$, then $f$ is defined at $x$ and $|f(x) - f(a)| < \e$. This nearly gives
that $f$ is continuous at $a$ by definition, except that we need to know that if $|x - a| < \d$, then $f$ is defined at $x$ and $|f(x) - f(a)| < \e$. The only ``extra''
case is the case $x = a$. But if $x = a$, then $f$ is defined at $a$ by assumption and we have $|f(x) -f(a)| = 0 < \e$.
\end{proof}


\begin{rem} Remember, when we write $\lim_{x \to a} f(x) = f(a)$ we
  mean that the limit exists and is equal to the number $f(a)$. So, by
  this Lemma, if $\lim_{x \to a} f(x)$ does not exist, then $f$ is not
  continuous at $a$.
\end{rem}





\begin{ex} Define a function $f$ whose domain is all of $\R$  by
$$
f(x) = 
\begin{cases}
1 & \text{if $x \in \Q$ and } \\
0 & \text{if $x \notin \Q$.} \\
\end{cases}
$$
As I proved, $\lim_{x \to a} f(x)$ does not exist for any $a$. So, this function is continuous nowhere.
\end{ex}

  
\begin{ex} 
Recall the function $f$ whose domain is all of $\R$ defined by
$$
f(x) = 
\begin{cases}
x & \text{if $x \in \Q$ and } \\
0 & \text{if $x \notin \Q$.} \\
\end{cases}
$$
As you showed, $\lim_{x \to 0} f(x) = 0$ and $\lim_{x \to a} f(x)$
does not exist for all $a \ne 0$. Since $f(0) = 0$ this shows that
$f(x)$ is continuous at $x = 0$, but not continuous at all other
points. So, somewhat counterintuitively, it is possible for a function defined on all of $\R$ to be continuous at one and only one spot! 
\end{ex}

\begin{ex}  The function $f(x)=\sqrt{x}$ is  continuous at $a$ for every $a>0$. This holds since for any $a > 0$, as you proved on the homework we have
  $$
  \lim_{x \to a} \sqrt{x} = \sqrt{a}.
  $$
\end{ex}


\Nov{3}



\begin{thm} Let $a\in \R$ and suppose $f$ and $g$ are two functions that are both
  continuous at $a$. Then so are
\begin{enumerate}
\item $f(x) + g(x)$,
\item $c \cdot f(x)$, for any constant $c$,
\item $f(x) \cdot g(x)$, and
\item $\frac{f(x)}{g(x)}$ provided $g(a) \ne 0$.
\end{enumerate}
\end{thm}
\begin{proof}
Follows from Theorems~\ref{thm36} and \ref{thm33b}.\end{proof}

\begin{ex} Polynomials are continuous everywhere. The function $x$ is continuous everywhere (since $\lim_{x\to a} \, x = a$). By part (3) above and a simple induction, $x^n$ is continuous everywhere for every $n$. Then by parts (1) and (2), it follows that every polynomial is continuous everywhere.
\end{ex}

Recall that for functions $f$ and $g$, $f \circ g$ is the {\em composition}: it is the function that
sends $x$ to $f(g(x))$. The domain of $f \circ g$ is
$$
\{x \in \R \mid \text{ $x$ is the domain of $g$ and $g(x)$ is in the domain of $f$} \}.
$$


\begin{thm} Suppose $g$ is continuous at a point $a$ and $f$ is continuous at $g(a)$. Then $f \circ
  g$ is continuous at $a$.
\end{thm}
\begin{proof}
  Let $a \in \R$ be such that that $g$ is continuous at $a$ and $f$ is continuous at $g(a)$.
  I prove $f \circ g$ is continuous at $a$ using the definition.
  
 Pick $\e > 0$. Since $f$ is continuous at $g(a)$, there is a $\gamma > 0$ such that if $|y
  - g(a)| < \gamma$ then $f$ is defined at $y$ and $|f(y) - f(g(a))| < \e$. (I am using $y$ in place of the usual $x$ for clarity below, and 
I am calling this number $\gamma$, and not $\d$, since it is
  not the $\d$ I am seeking.) Since $\gamma > 0$ and $g$ is continuous at $a$, there is a $\d > 0$ such
  that if $|x - a| < \d$ then $g$ is defined at $x$ and $|g(x) - a| < \gamma$. 

  This $\d$ ``works'' to prove $f \circ g$ is continuous at $a$: Let $x$ be any real number such that  $|x-a| < \d$. Then $g$ is defined at $x$ and {${|g(x) - g(a)| < \gamma}$}.
  Taking $y = g(x)$ above, this gives that $f$ is defined at $g(x)$ and $|f(g(x)) -
f(g(a))| < \e$. This proves $f \circ g$ is continuous at~$a$.
\end{proof}

\begin{ex} 
 The function $\sqrt{x^2 + 5}$ is continuous at every real number: let $g(x) = x^2 + 5$ and $f(x) = \sqrt{x}$. Then $g$ is continuous at $a$ for every $a\in \R$
  since it is a polynomial. For each $x \in \R$, $g(x) > 0$ and hence $f$ is continuous at $g(x)$. So $\sqrt{x^2 + 5}$ is continuous at every $a\in\R$ by the Theorem.

\end{ex}

\begin{ex} You can apply the Theorem to the compositions of more than two functions too. For example $\sqrt{|x^3| + 1}$ is continuous at $a$ for and $a\in\R$.
\end{ex}



We are probably getting a bit tired of saying ``continuous at $a$ for every $a\in \R$''. The following definition will then be convenient.



\begin{defn} Let $S$ be an open interval of $\R$ of the form $S=(a,b)$, $S=(a,\infty)$, $S=(-\infty,a)$, or $S=(-\infty,\infty)=\R$. We say $f$ is {\em continuous on $S$} if $f$ is continuous
  at $a$ for all $a\in S$. 
\end{defn}



\begin{ex} 
\begin{itemize}
\item Every polynomial is continuous on $\R$.
\item Every polynomial is continuous on $(-13,5)$.
\item The function $\sqrt{x}$ is continuous on $(0,\infty)$.
\item The function $1/x$ is continuous on $(0,\infty)$. It is also continuous on $(-\infty,0)$.
\end{itemize}
\end{ex}


Since the function $\sqrt{x}$ does not jump at $x=0$, we would like to say the function is continuous on all of $[0,\infty)$. However, the definition for $f$ to be continuous at a point $a$ requires that $f$ is defined on $(a-\d,a+\d)$ for some $\d>0$, so we have to change our definition a bit.

\Nov{5}

\begin{defn} 
 Given a function $f(x)$ and real numbers $a < b$,
we say $f$ is {\em continous on the closed interval $[a,b]$} provided 
\begin{enumerate}
\item for every $r \in (a,b)$, $f$ is  continuous at $r$ in the sense defined already,
\item for every $\e > 0$ there is a $\d > 0$ such that if $x \in [a,b]$ and $a \leq x < a+\d$, then ${|f(x)
  - f(a)| < \e}$.
\item for every $\e > 0$ there is a $\d > 0$ such that if $x \in [a,b]$ and $b -\d < x \leq b$, then ${|f(x)
  - f(b)| < \e}$.
\end{enumerate}
\end{defn}

\

In short, condition (1) says that $f$ is continuous on the open interval $(a,b)$, condition (2) says that $f$ is ``continuous from the right'' at $a$, and condition (3) says $f$ is ``continuous from the left'' at $b$.


\

\begin{thm}[Intermediate Value Theorem] Suppose $f$ is a function, that $a < b$ are real numbers, and that $f$ is continuous on the closed interval
  $[a,b]$. If $y$ is any number between $f(a)$ and $f(b)$ (i.e., $f(a) \leq y \leq f(b)$ or
$f(a) \geq y \geq f(b)$), then there is a $c \in [a,b]$ such that $f(c) = y$.\end{thm}

\begin{enumerate}

\item Explain why if $f$ is continuous at $x$ for every $x\in[a,b]$, then $f$ is continuous on the closed interval $[a,b]$. Conclude that every polynomial is continuous on every closed interval.

\begin{framed}
Suppose that $f$ is continuous at $r$ for all $r\in [a,b]$. Then, in particular $f$ is continuous at $r$ for all $r\in (a,b)$ so (1) holds. Since $f$ is continuous at $a$, then condition (2) holds (with the same $\d$ for any given $\e$); since $f$ is continuous at $b$, then condition (3) holds (with the same $\d$ for any given~$\e$).
\end{framed}



\item Show that the function $f(x) = \sqrt{1-x^2}$ is continuous on the closed interval $[-1,1]$:
\begin{itemize}
\item For showing condition (1), I recommend using a Theorem from last class. 
\item For condition (2), it may help to write $\sqrt{1-x^2}=\sqrt{1-x}\sqrt{1+x}$. \footnote{You can ask me for a $\delta$ if you get stuck.}
\item Condition (3) is similar to condition (2) so you can just say ``Similar to (2)'' for that.
\end{itemize}
\begin{framed}
  It is continuous at $a$ for all $a$ such that $-1 < a < 1$ since $1 - x^2$ is continuous on all of $\R$,
  $\sqrt{y}$ is continuous on $(0, \infty)$, and $1 - x^2 > 0$ for all $-1 < x < 1$. 

To prove the condition at the endpoint $-1$, pick $\e > 0$. (Scratch work:
$\sqrt{1-x^2} < \e$ if and only if $|1-x||1+x| < \e^2$. So long as $-1
\leq x \leq 1$ we have $|1-x| \leq 2$.) Let $\d =\e^2/2$.
Assume $x$ is any real number such that 
$-1 \leq x < -1 + \d$ and $x\in [-1,1]$. We have that $|1+x| \leq 2$
and hence $\sqrt{|1+x|} \leq \sqrt{2}$. 
We have $\sqrt{|1-x|} \leq \sqrt{\e^2/2} = \e/\sqrt2$.
Thus we obtain
$$
\sqrt{1-x^2} = \sqrt{|1-x|}\sqrt{|1+x|} < \sqrt{2} \e/\sqrt{2} = \e.
$$


The other endpoint $1$ is dealt with in a similar way.
\end{framed}

\item Explain why if $f$ is continuous on the closed interval $[a,b]$, then it does not follow in general that $f$ is continuous at $x$ for every $x\in[a,b]$.

\begin{framed} The function $f(x)=\sqrt{1-x^2}$ is continuous on the closed interval $[-1,1]$, but not continuous at $-1$.
\end{framed}

\item Use the Intermediate Value Theorem to give a quick proof that there is a positive real number $x$ such that $x^2=2$.

\begin{framed} Let $f(x)=x^2$. This is continuous on $[1,2]$. We have $f(1) = 1 < 2 < 4=f(2)$, so there is some $x$ (between 1 and 2) such that $f(x)=2$.
\end{framed}

\item Use the Intermediate Value Theorem to give a quick proof that every real number has a cube root.

\begin{framed}
 Let  $f(x) = x^3$. 

  I claim that there are numbers $a$ and $b$ such that $f(a) \leq r \leq f(b)$: 
If $r \geq 1$, then $a = 0$ and $b = r$ work; 
if $r \leq -1$, then $a = r$ and $b  = 0$ work; and
if $-1 \leq r < 1$, then $a = -1$ and $b = 1$ work.

Since $f$ is a polynomial, it is continuous on all of $\R$ and hence on the closed interval $[a,b]$.
Thus, since $f(a) \leq r \leq f(b)$  by the Intermediate Value Theorem, there is a real number  $s$
such that $a \leq s \leq b$ and $f(s) = r$; that is, $s^3 =r$. 
\end{framed}

\item Explain why the statement of the Intermediate Value Theorem would be false if we did not assume that $f$ is continuous on $[a,b]$.

\begin{framed}
Suppose $f(x) = x$ for $x  < 0$ and $f(x) = x+1$ for $x \geq 0$.
  Then $f$ is defined on $[-1,1]$, $f(-1) = -1$ and $f(1) = 2$. So $f(-1) < 1/2 < f(1)$, there there is clearly no $x$ such that $f(x) = 1/2$. \end{framed}

\item True or False: The polynomial $f(x) = x^3 - 4x +1$ has two roots in the interval $[0,2]$.

\begin{framed}
Note that $f(0)=1$ and $f(1)=-2$. Since $f$ is continuous on $[0,1]$, there is some $c_1\in [0,1]$ such that $f(c_1)=0$. Now note that $f(2)=1$. Since $f$ is continuous on $[1,2]$, there is some $c_2\in [1,2]$ such that $f(c_2) = 0$. We must have $c_1<1$ since $f(c_1)\neq 0$, so $c_1\neq c_2$; these are two distinct roots in $[0,2]$.
\end{framed}

%\item True or False: If $f$ is a function, $a<b$ are real numbers, and for every number $y$ between $f(a)$ and $f(b)$, there is some $c\in [a,b]$ such that $f(y)=c$, then $f$ is continuous on $[a,b]$.

%\

%\item True or False: If $f$ is a function with domain $\R$, and for every pair $a<b$ of real numbers and for every number $y$ between $f(a)$ and $f(b)$, there is some $c\in [a,b]$ such that $f(y)=c$, then $f$ is continuous on $\R$.


\end{enumerate}

\begin{comment}


\section{Wednesday, April 7}

\begin{rem}
If $[a,b]$ is a closed interval, and $f$ is continuous at $r$ for every $r\in [a,b]$, then $f$ is continuous on the closed interval $[a,b]$; this follows directly from the definition. The converse of this is false, however, as we will see in examples below.
\end{rem}

The notion of continuity on a closed interval can be characterized in terms of one-sided limits. 


\begin{prop}\label{prop:closedintervallimits} Assume $f$ is defined at every point of  $[a,b]$. Then $f$ is continuous on $[a,b]$  if and only if
\begin{enumerate}
\item for every $r \in (a,b)$, $\lim_{x \to r} f(x) = f(r)$,
\item $\lim_{x \to a^+} f(x) = f(a)$, and
\item $\lim_{x \to b^-} f(x) = f(b)$.
\end{enumerate}
\end{prop}
\begin{proof}
Similar to proof of Theorem~\ref{thm36}.
\end{proof}


\begin{ex}
  I claim that the function $\sqrt{1 - x^2}$ is continuous on $[-1, 1]$.

  It is continuous at $a$ for all $a$ such that $-1 < a < 1$ since $1 - x^2$ is continuous on all of $\R$,
  $\sqrt{y}$ is continuous on $(0, \infty)$, and $1 - x^2 > 0$ for all $-1 < x < 1$. 


To prove the condition at the endpoint $-1$, pick $\e > 0$. (Scratch work:
$\sqrt{1-x^2} < \e$ if and only if $|1-x||1+x| < \e^2$. So long as $-1
\leq x < 1$ we have $|1-x| \leq 2$.) Let $\d = \min\{\e^2/2, 2\}$.
Assume $x$ is any real number such that 
$-1 \leq x < -1 + \d$. Since $\d \leq 2$, we get that $-1 \leq x \leq 1$ and
hence  $\sqrt{1-x^2}$ is defined. It is also true that $|1+x| \leq 2$
and hence $\sqrt{|1+x|} \leq \sqrt{2}$. 
Since $\d \leq \e^2/2$ we have $\sqrt{|1-x|} \leq \sqrt{\e^2/2} = \e/\sqrt2$.
Thus we obtain
$$
\sqrt{1-x^2} = \sqrt{|1-x|}\sqrt{|1+x|} < \sqrt{2} \e/\sqrt{2} = \e.
$$


The other endpoint $1$ is dealt with in a similar way.

Notice that $f$ is not continuous at $-1$ , since $f$ is not defined on $(-1-\d,-1+\d)$ for any $\d>0$.\end{ex}



We arrive at a batch of well-known and pleasing theorems pertaining to continuous functions.


\begin{thm}[Intermediate Value Theorem]
  Suppose $f$ is a function and $a < b$ and that $f$ is continuous on the closed interval
  $[a,b]$. If $y$ is any number between $f(a)$ and $f(b)$ (i.e., $f(a) \leq y \leq f(b)$ or
$f(a) \geq y \geq f(b)$), then there is an $c \in [a,b]$ such that $f(c) = y$.
\end{thm}



\Nov{8}

\begin{proof}[Proof of Intermediate Value Theorem] Assume $f$ is continuous on $[a,b]$ and $y$ is a real
  number such that  $f(a) \leq y \leq f(b)$ or $f(b) \leq y \leq f(a)$. We need to prove there is a $c \in [a,b]$ such that $f(c) = y$.

  Let us assume $f(a) \leq y \leq f(b)$ --- the other case may be proved in a very similar manner, or by appealing to this case using the function $-f(x)$ instead.
  
 If $f(a) = y$ then we may take $c = a$ and if $f(b) = y$ then we may take $c = b$. So, we may assume $f(a) < y < f(b)$.

Consider the set 
$$
S = \{z \in \R \mid a \leq z \leq b \text{ and $f(x) < y$ for all $x \in [a, z]$}\}
$$
This set is nonempty, since $a \in S$, and it is bounded above, by $b$. It therefore has a supremum,
which we will call $c$. I claim $f(c) = y$.





Let us first show that $c > a$. By way of contradiction, suppose $c \leq a$. Since $c \geq a$, we must have $c = a$. 
Since $f$ is continuous on $[a,b]$, taking $\e = y
- f(a) > 0$ in the definition, we get that there is a $\d>0$ such that if $a \leq x < a +
\d$, then $f(a) - \e < f(x) < f(a) + \e$. In particular, if $a \leq x \leq a + \d/2$, then $f(x) < f(a) + \e = y$.
This proves that $a+ \d/2 \in S$. But $a + \d/2 > a = c$, contrary to the fact that $c$ is the supremum of $S$. We conclude that $c > a$. 


Similarly, one may show that $c < b$ --- I leave this to you as an exercise.

We now know that $a < c < b$, and 
we next prove that $f(c) = y$ by showing that $f(c) > y$ and $f(c) < y$ are each impossible.

Suppose $f(c) > y$. 
Setting $\e = f(c) - y$ and applying the definition of continuous at $c$, there is a $\d >0$ such that if $x$ is any number such that $c
- \d < x < c + \d$ then $f(c) - \e < f(x) < f(c) + \e$. In particular, for any $z$ such that $c - \d < z \leq c$, 
we have
$$
f(z) > f(c) - \e = y.
$$
In particular, $z$ is not in the set $S$. It follows that $c - \d$ is an upper bound of $S$, contrary to the fact that $c$ is the least upper bound of $S$. 



Suppose $f(c) < y$. 
Setting $\e = y - f(c)$ and applying the definition of continuous at $c$, there is a $\d >0$ such that if $c - \d < x < c + \d$, 
then $f(c) - \e < f(x) < f(c) + \e$. In particular, 
if $x$ is any real number such that $c \leq x \leq c + \d/2$, then $f(x) < f(c) + \e = y$. Moreover, if $x < c$, then $x$ is not an upper bound of $S$, and hence there is a $z \in
S$ such that $x < z$. If follows that $f(x) \leq y$.
So, we have shown that if $x \leq c + \d/2$, then $f(x) < y$. This shows that
 $c + \d/2 \in S$, contrary to $c$ being an upper bound of $S$. 
\end{proof}


%\section{Friday, April 9}
%%%%%

%\begin{ex} As a ``real world'' example, of the Intermediate Value Theorem, if the temperature at midnight last night was 45 degrees, and it is now 54 degrees, then there was an instant where it was exactly 51.3 degrees.\end{ex}



%\begin{ex} Let $f(x) = x^2$. Then $f$ is continuous on $[0,2]$ (and, for that matter, on any closed interval) since it is a polynomial.
%  We have $f(0) \leq 2 \leq f(2)$ and hence by the IVT there is an $x \in [0,2]$ such that $x^2 = 2$. That is, the square root of two exists as a real number.
%\end{ex}



%\begin{ex} We can also prove that cube roots exist using the IVT.

%\begin{proof} Let  $f(x) = x^3$. 

%  I claim that there are numbers $a$ and $b$ such that $f(a) \leq r \leq f(b)$: 
%If $r \geq 1$, then $a = 0$ and $b = r$ work; 
%if $r \leq -1$, then $a = r$ and $b  = 0$ work; and
%if $-1 \leq r < 1$, then $a = -1$ and $b = 1$ work.
%
%Since $f$ is a polynomial, it is continuous on all of $\R$ and hence on the closed interval $[a,b]$.
%Thus, since $f(a) \leq r \leq f(b)$  by the Intermediate Value Theorem, there is a real number  $s$
%such that $a \leq s \leq b$ and $f(s) = r$; that is, $s^3 =r$. 
%\end{proof}
%\end{ex}
%
%
%\begin{ex} The Intermediate Value Theorem would become false if we omitted the continuity assumption. For instance, suppose $f(x) = x$ for $x  < 0$ and $f(x) = x+1$ for $x \geq 0$.
%  Then $f$ is defined on $[-1,1]$, $f(-1) = -1$ and $f(1) = 2$. So $f(-1) < 1/2 < f(1)$, there there is clearly no $x$ such that $f(x) = 1/2$. 
%\end{ex}



Our next goal is to prove the Boundedness Theorem and the Extreme Value Theorem. You might recall the latter from Calculus class. Here are the statements:


\begin{thm}[Boundedness Theorem] 
Suppose $f$ is continuous on the closed interval $[a,b]$ for some real numbers $a,b$ with $a < b$. Then $f$ is bounded on $[a,b]$ --- that is,
  there are real numbers $m$ and $M$ so that $m \leq f(x) \leq M$ for all $x \in [a,b]$.
\end{thm}


\begin{rem} The statement of this theorem would become false if either  we omit the continuous
  assumption or if we changed the closed interval $[a,b]$ to an open one.

For example, consider
$$
f(x) = 
\begin{cases}
 1/x & \text{if $x \ne 0$ and} \\
 5 & \text{if $x = 0$.} \\
\end{cases}
$$

Simialarly, $f(x) = \frac{1}{x}$ is continuous on $(0,1)$ but not bounded on it.
\end{rem}

\begin{thm}[Extreme Value Theorem]
  Assume $f$ is continuous on the closed interval $[a,b]$ for some real numbers $a$ and $b$ with $a < b$.
  Then $f$ attains a minimum and a maximum value on $[a,b]$ ---
that is, there exists a number $r \in [a,b]$ such that $f(x) \leq f(r)$ for all $x \in [a,b]$ and
there exists a number $s \in [a,b]$ such that $f(x) \geq f(s)$ for all $x \in [a,b]$.
\end{thm}

\begin{rem} The statement of the  Extreme Value Theorem would become false if we either omitted the continous assumption or replaced $[a,b]$ with, for example,  $(a,b)$.
\end{rem}

\begin{rem} The Boundedness Theorem is an immediate consequence of the Extreme Value Theorem. The reason we state it first as a separate theorem is that we need the Boundedness
  Theorem in order to prove the Extreme Value Theorem.
\end{rem}

%\begin{center}{\large{End of material for exam 2}}\end{center}






For the proofs of both of these theorems, we will need the following Lemma.


\begin{lem} \label{lem315}
Assume $f$ is continuous on $[a,b]$ and that $\{x_n\}_{n=1}^\infty$ is any sequence such that $a \leq x_n  \leq b$ for
  all $n$. If $\{x_n\}_{n=1}^\infty$ converges to some number $r$, then
\begin{enumerate}
\item $r \in [a,b]$ and
\item $\lim_{n \to \infty} f(x_n) = f(r)$.
\end{enumerate}
\end{lem}

We will return to the proof soon.

\begin{comment}

\begin{proof} For part (1), to show that $r\leq b$, by way of contradiction suppose that $r>b$. Taking $\e=r-b$ in the definition of ``converges to $r$'' we see that there is some $N\in \R$ such that if $n>N$ and $n\in \N$ then $a_n> r-\e =b$, which contradicts that $a_n\leq b$ for all $n$. The inequality $r\geq a$ is similar.

To prove (2) we proceed in cases:

{\bf Case $1$:} $a < r <  b$.  Let $\e >0$. Since $f$ is continuous at $r$, there is some $\d>0$ such that if $|x-r|<\d$, then $|f(x)-f(r)|<\e$. Using this positive number $\d$ in the definition of the sequence $\{x_n\}$ to converge to $r$, there is some $N\in \R$ such that if $n>N$, then $|x_n-r|<\d$. Consequently, for this $N$, if $n>N$, then $|f(x_n)-f(r)|<\e$. This means that $\{f(x_n)\}$ converges to $f(r)$.

{\bf Case $2$:} $r = a$. Similarly to above, let $\e>0$. By definition, there is some $\d>0$ such that if $a\leq x < a+\delta$ then $|f(x)-f(a)|<\e$. Again using $\d$ in the definition of the sequence $\{x_n\}$ to converge to $r$, there is some $N\in \R$ such that if $n>N$, then $|x_n-a|<\d$. By our hypotheses on $\{x_n\}$ we must have $a\leq x_n < a+\d$ for $n>N$. Consequently, for this $N$, if $n>N$, then $|f(x_n)-f(r)|<\e$. Again, this means that $\{f(x_n)\}$ converges to $f(r)$.

{\bf Case $3$:} $r = b$. Similar to Case 2.
\end{proof}




\Nov{10}


\begin{proof}[Proof of the Boundedness Theorem]
  Assume $f(x)$ is continuous on the closed interval $[a,b]$. By way of contradiction, suppose $f(x)$ is not bounded above.
  Then for each natural number $n$, the function $f(x)$ is bigger than $n$ somewhere on the interval. So, for each $n \in
\N$, there is
  a real number $x_n$ such that $a \leq x_n \leq b$ and $f(x_n) >
  n$. Consider the sequence $\{x_n\}_{n=1}^\infty$ formed by these chosen numbers.  It need not converge, but it is bounded (above by $b$ and below
  by $a$) and so the Main Corollary to the Bolzano-Weierstrass Thereom ensures that there is a subsequence $\{x_{n_k}\}_{k=1}^\infty$ that converges, say to the number $r$. 
By the Lemma, $r \in [a,b]$ and 
$\lim_{k \to \infty} f(x_{n_k}) = f(r)$. In particular, $\{f(x_{n_k})\}_{k=1}^\infty$ converges.
But by construction
$f(x_{n_k}) > n_k$
for all $k$, and so it cannot converge. We have reached a contradiction. So $f$ must be bounded above.

To show $f(x)$ is bounded below,
using what we have already proven, we have  that $-f(x)$ is also bounded above by some number $N$, and it follows that $f(x)$ is
bounded below by $-N$.
\end{proof}






\begin{proof}[Proof of the Extreme Value Theorem] 
We first prove $f$ attains a maximum value on $[a,b]$.

Let $R$ be the range of $f$; that is,
$$
R = \{y \in \R \mid y = f(x) \text{ for some $x \in [a,b]$}\}.
$$
By the Boundedness Theorem, $R$ is bounded above, and  it is clearly
nonempty. So,  by the
Completeness Axiom it has a supremum, call it  $M$. We can find a sequence $\{y_n\}$ with $y_n \in R$ for all
$n$ that converges to $M$. (This follows from the definition of supremum: for each $n \in \N$ since
$M - \frac{1}{n}$ is not an upper bound of $R$, there exists a $y_n \in R$ with $M- \frac{1}{n} <
y_n \leq M$. By the Squeeze Theorem, $\{y_n\}$ converges to $M$.)

Since $y_n \in R$, for each $n$, we may pick $x_n \in [a,b]$ such that $f(x_n) = y_n$. The
sequence $\{x_n\}$ might not converge, but it is bounded, and so thanks to the Main Corollary of the Bolzano-Weierstrass Theorem it has a subsequence
$\{x_{n_k}\}$ that does converge. Say this subsequence converges to
$r$. By the Lemma, we have $r \in [a,b]$ and
$$
f(r) = \lim_{k \to \infty} f(x_{n_k}) = \lim_{k \to \infty} y_{n_k}.
$$
Since $\{y_{n_k}\}$ is a subsequence of $\{y_n\}$ and the latter converges to $M$, so does the former. So, $f(r) = M$. 
By definition of $M$, we have $f(x) \leq M = f(r)$ for all $x \in [a,b]$.
 
To prove $f$ attains a minimum value on $[a,b]$, apply what we just proven to the function $-f(x)$. This gives us that $-f(x)$ attains its maximum value $N$
at some point $s$. It follows that $f(x)$ attains its minimum value (which is $-N$) at $s$. 
\end{proof}



\begin{cor} Let $f:[a,b]\to \R$ be continuous. The range of $f$ is either a single point or a closed interval.
\end{cor}
\begin{proof} By the Extreme Value Theorem, the range of $f$ contains a minimum value $m$ and a maximum value $M$. If $m=M$, then the range is the single point $m=M$. Otherwise, we have $m<M$. We claim that the range of $f$ is equal to $[m,M]$. Clearly the range of $f$ is contained in $[m,M]$ by definition of minimum and maximum. For the other containment, take $y\in [m,M]$. Take $r,s\in [a,b]$ such that $f(r)=m$ and $f(s)=M$.
 
Case 1: $r<s$. In this case, $f$ is continuous on $[r,s]$, and by the Intermediate Value Theorem, there is some $c\in [r,s] \subseteq [a,b]$ such that $f(c) = y$, so $y$ is in the range of $f$.

Case 2: $s<r$. In this case, $f$ is continuous on $[s,r]$, and by the Intermediate Value Theorem, there is some $c\in [s,r] \subseteq [a,b]$ such that $f(c) = y$, so $y$ is in the range of $f$.

We conclude that the range is the closed interval $[m,M]$.
\end{proof}


\begin{center}{\large{End of material for exam 2}}\end{center}




\Nov{19}


\begin{defn} Suppose $f$ is a function and $r$ is a real number.
  We say $f$ is {\em differentiable at $r$} if $f$ is defined at $r$ and
the limit
$$
\lim_{x \to r} \frac{f(x) - f(r)}{x-r}
$$
exists. In this case, the value of this limit is the {\em derivative  of $f$ at $r$} and is written as $f'(r)$ for short.
\end{defn}

\begin{rem} Notice that $\frac{f(x) - f(r)}{x-r}$ is undefined when $x
  = r$. But this is OK since in the definition of a limit, the function is not necessarily defined
  at the limiting point. 
\end{rem}

\begin{rem} For the limit  $\lim_{x \to r} \frac{f(x) - f(r)}{x-r}$
  there must be a positive real number $\d  > 0$ such that $f$ is defined for all $x$ satisfying
  $0 < |x-r| < \d$. Since we assume $f$ is defined at $r$ too, it
  follows that if $f$ is differentiable at $r$, then it is defined on $(r-\d, r+\d)$ for some $\d > 0$.
\end{rem}


\begin{ex} Let $f(x) = x^3$ and let $r$ be arbitrary. Then using that $x^3 - r^3 = (x-r)(x^2 + xr + r^2)$ we get
$$
\lim_{x \to r} \frac{f(x) - f(r)}{x-r} = 
\lim_{x \to r} \frac{x^3 - r^3}{x-r} = 
\lim_{x \to r} x^2 + xr + r^2 = 3r^2.
$$
This proves that $f'(r) = 3r^2$ for any real number $r$. 
\end{ex}

\begin{ex} Let $f(x) = \sqrt{x}$. For any $r > 0$ we have
$$
\lim_{x \to r} \frac{f(x) - f(r)}{x-r} = \lim_{x \to r} \frac{\sqrt{x} - \sqrt{(r)}}   {x-r}
  = \lim_{x \to r} \frac{x-r}{   (x-r) (\sqrt{x} + \sqrt{r})   }
  = \lim_{x \to r} \frac{1}{\sqrt{x} + \sqrt{r}} = \frac{1}{2 \sqrt{r}}
$$
where the last step uses that $\lim_{x \to r} \sqrt{x} = \sqrt{r}$ and
other properties of limits we have established.

Note that $\sqrt{x}$ is not differentiable at $0$. 
\end{ex}



 As you well know, the derivative of a function may  again be regarded as another function. 
In detail, if $f$ is any function then its derivative is the function $f'$ whose value at $x$ is
$f'(x)$. The domain of $f'(x)$ is
$$
\{x \in \R \mid \text{ $f$ is differentiable at $x$} \}
$$
In our fist example above, we have shown that the derivative of $f(x) = x^3$ is $f'(x) = 3x^2$, and the domain of $f'(x)$ is all of $\R$. In the second we showed that $g(x) =
\sqrt{x}$ is differentiable for all $x < 0$ and its derivative is $\frac{1}{2 \sqrt{x}}$ for $x > 0$. 


A somewhat technical but nevertheless useful result is: 

\begin{prop} If $f$ is differentiable at $r$, then $f$ is continuous at~$r$.
\end{prop}

\begin{proof}
Using the Theorem about limits of sums, products etc. we get
$$
\begin{aligned}
\lim_{x \to r} f(x) & = \lim_{x \to r} (f(r) + f(x) - f(r))    \\
 & = f(r) + \lim_{x \to r} \frac{f(x) - f(r)}{x-r} \cdot (x-r)  \\
& =  f(r) + \lim_{x \to r} \frac{f(x) - f(r)}{x-r} \cdot \lim_{x \to
  r}(x-r) \\
& = f(r) + f'(r) \cdot 0 \\
& = f(r). \\
\end{aligned} 
$$
This proves $f$ is continuous at $r$.
\end{proof}


Let us give an example of a function that is not differentiable at a point:

\begin{ex} Let $f(x) = |x|$. It is pretty clear at an intuitive level that $f$ is not differentiable at the point $x = 0$. To prove this carefully, we need to show that 
$\lim_{x \to 0} \frac{f(x) - f(0)}{x-0}$ does not exist. Note 
$$
\frac{f(x) - f(0)}{x-0} = \frac{|x|}{x} = 
\begin{cases} 
1, & \text{if $x > 0$ and} \\ -1, 
& \text{if $x < 0$}. \end{cases}
$$
Letting $x_n = \frac{1}{n}$ we have $\lim_{n \to \infty} x_n = 0$ and $\lim_{n \to \infty} f(x_n) = 1$ and 
letting $y_n = \frac{-1}{n}$ we have $\lim_{n \to \infty} y_n = 0$
and $\lim_{n \to \infty} f(y_n) = -1$. This proves the limit does not exist.
\end{ex}

\begin{ex} 
 The derivative of a constant function is $0$ at all points. Also, if $h(x) = x$ then $h'(x) = 1$. These both follow immediately from the definition.
 \end{ex}
 
 \begin{thm} Suppose $f$ and $g$ are two functions that are both differentiable at a number $r$.  Then:

    \begin{enumerate}
\item $f + g$ is differentiable at $r$ and $(f + g)'(r)  = f'(r) + g'(r)$.
\item For any constant $c$, $cf$ is differentiable at $r$ and
  $(cf)'(r) = c f'(r)$.
\item (``The Product Rule'') $f \cdot g$ is differentiable at $r$ and $(f \cdot g)'(r)  = f'(r) g(r) + f(r) g'(r)$.
\item If $g(r) \ne 0$, then $\frac{1}{g}$ is differentiable at $r$ and
$$
\left(\frac{1}{g}\right)'(r) = - \frac{1}{g^2(r)} g'(r).
$$
\end{enumerate}
\end{thm}
\begin{proof}
For part (1), we note that \[ \frac{(f+g)(x) - (f+g)(r)}{x-r} = \frac{f(x) + g(x) - f(r) - g(r)}{x-r} = \frac{f(x)-f(r)}{x-r} + \frac{g(x) - g(r)}{x-r}.\] When we take the limit as $x$ approaches $r$, this is $f'(r) + g'(r)$, using the definition of $f'(r)$ and $g'(r)$ and the fact that the limit of a sum of two functions is the sum of the limits (when they both exist).

For (2),   we note that \[ \frac{(cf)(x) - (cf)(r)}{x-r} = c \frac{f(x) - f(r)}{x-r},\] and it follows from our limit theorems that the limit as $x$ approaches $r$ is $cf'(r)$.


\Nov{22}




  For (3), using what we know about limits we get 
$$
\begin{aligned}
& \lim_{x \to r} \frac{f(x)g(x) - f(r)g(r)}{x-r} 
=  \lim_{x \to r} \left(\frac{f(x)g(x) - f(r)g(x)}{x-r}  + \frac{f(r)g(x) -     f(r)g(r)}{x-r} \right) \\
& =  \lim_{x \to r} g(x) \cdot \lim_{x \to r}\left(\frac{f(x) - f(r)}{x-r}\right)  
+ f(r) \cdot \lim_{x \to r} \left(\frac{g(x) -     g(r)}{x-r} \right)  \\
&= g(r) f'(r) + f(r) g'(r),
\end{aligned}
$$
where for the last step we use that $\lim_{x \to r} g(x) = g(r)$ since $g$ is continuous at $r$ (since differentiable implies continuous).

For (4), we have
$$
\begin{aligned}
\lim_{x \to r} \frac{\frac{1}{g(x)} - \frac{1}{g(r)}}{1-r} 
& =  \lim_{x \to r} \frac{g(r) - g(x)}{g(x)g(r)(1-r)} \\
& =  - \lim_{x \to r} \frac{g(x) - g(r)}{1-r} \frac{1}{g(x)g(r)} \\
& =  - \lim_{x \to r} \frac{g(x) - g(r)}{1-r} \frac{1}{g(r) \lim_{x \to r}g(x)} \\
& = - g'(r) \frac{1}{g(r) g(r)}  \\
& = - \frac{1}{g^2(r)} g'(r).\\
\end{aligned}
$$
In this chain of equalities, we have used that $g(x)$ is continuous at $r$ to get  $\lim_{x \to r} g(x) = g(r)$.
\end{proof}

\begin{rem} We can deduce the ``Quotient rule'' from parts (5) and (6) above. Indeed, if $f$ and $g$ are differentiable at $r$ and $g'(r)\neq 0$, then $(f/g)'(r) = (f \cdot 1/g )'(r) = f(r) (1/g)'(r) + f'(r) (1/g)(r) = \frac{f(r) (-g'(r))}{g^2(r)} + \frac{f'(r)}{g(r)} = \frac{f'(r) g(r) - f(r) g'(r)}{g^2(r)}$.
\end{rem}

\begin{rem} Using this Theorem and the previous example, we can show (by induction) that if $f(x) = x^n$ for any integer $n \geq 0$, then $f$ is differentiable on all of $\R$ and
we have $f'(x) = nx^{n-1}$ for all $x$. 

Using that differentiation is linear, if $f$ is a polynomial, so that
$f(x) = a_nx^n + \cdots + a_1 x + a_0$ for constants $a_0, \dots, a_n$, we get that $f$ is differentiable on all of $\R$ and 
$$
f'(x) = na_nx^{n-1} + \cdots + a_1.
$$


For $n \in \Z$ such that $n < 0$, and (6) of the Theorem  allows us to
conclude that if $f(x) = x^n$ then $f'(x) = nx^{n-1}$ in this case
too. 
\end{rem}


%\begin{lem}
%Let $f$ and $g$ be two functions, and $a\in \R$. Suppose that $\lim_{x\to a} g(x)=r$ and $f$ is continuous at $r$. Then $\lim_{x\to a} (f\circ g)(x) = f(r)$.
%\end{lem}
%\begin{proof}
%Let $\e>0$. Since $f$ is continuous at $a$, there is some $\gamma >0$ such that if $|y-r|<\gamma$, then $f(y)$ is defined and $|f(y)-f(r)|<\e$. By definition of limit applied to the positive number $\gamma$, there is some $\d>0$ such that if $0<|x-a|<\d$, then $g(x)$ is defined and $|g(x)-r|<\gamma$. Then, for this same $\d$, if $0<|x-a|<\d$, then taking $y=g(x)$, we have that $f(g(x))$ is defined and $|f(g(x))-f(r)|<\e$. This means that $\lim_{x\to a} (f\circ g)(x) = f(r)$.
%\end{proof}



\begin{thm} [Chain Rule]
  Suppose $g$ is differentiable at $s$ and $f$ is
  differentiable  at $g(s)$. Then $f \circ g$ is differentiable at $s$ and
$$
(f \circ g)'(s) = f'(g(s)) g'(s).
$$
\end{thm}


\begin{ex} Say $h(x) = \sqrt{x^4 + 1}$. Then $h = f \circ g$ where $g(x) = x^4 + 1$ and $f(x) = \sqrt{x}$. Since $g$ is a polynomial, it is differentiable on
  all of $\R$ and $g'(x) = 4x^3$. The range of $g$ is $[1, \infty)$
  and as we showed above $f(x)$ is differentiable on all of $(0, \infty)$ and
  that $f'(x) = \frac{1}{2 \sqrt{x}}$ at such points.  Using the Chain Rule we get that $h(x)$ is differentiable on all of $\R$ and that
$$
h'(x) = \frac{4x^3}{2 \sqrt{x^4 + 1}}.
$$
\end{ex}

Before we prove the chain rule, it's worth discussing the most natural approach: we can write 
\[ \lim_{x\to s} \frac{(f\circ g)(x) - (f\circ g)(s)}{x-s} = \lim_{x\to s} \frac{f(g(x)) - f(g(s))}{g(x) - g(s)}  \frac{g(x) - g(s)} {x-s}.\]
Then $\displaystyle \lim_{x\to s} \frac{g(x) - g(s)} {x-s} = g'(s)$, and we would be done if we could also show that 
$\displaystyle \lim_{x\to s} \frac{f(g(x)) - f(g(s))}{g(x) - g(s)} = f'(g(s))$. This certainly looks right at first, especially if we let $r=g(s)$, we think of $g(x)$ as $y$ and observe that the limit as $x$ approaches $s$ of $y=g(x)$ is $r$, since $g$ is continuous, so we would \emph{want} to say that \[ \lim_{x\to s} \frac{f(g(x)) - f(g(s))}{g(x) - g(s)} = \lim_{y\to r} \frac{f(y) - f(r)}{y - r} = f'(r) = f'(g(s)).\]
But there is a problem with this: when $g(x)-g(r)=0$, the function $\displaystyle \frac{f(g(x)) - f(g(s))}{g(x) - g(s)}$ is not defined, so if $g(x)=g(x)$ for values of $x$ that are arbitrarily close to $s$, there is no limit! 


\Nov{29}


\begin{prop}\label{prop-deri} Let $f$ be a function and $r$ be a real number. Assume that $f$ is differentiable at $r$.
\begin{enumerate}
\item If $f'(r)>0$, then there is some $\delta>0$ such that 
\begin{itemize} 
\item$f(x)>f(r)$ for all $x\in (r,r+\delta)$, and
\item$f(x)<f(r)$ for all $x\in (r-\delta,r)$.
\end{itemize}
\item If $f'(r)<0$, then there is some $\delta>0$ such that 
\begin{itemize} 
\item$f(x)<f(r)$ for all $x\in (r,r+\delta)$, and
\item$f(x)>f(r)$ for all $x\in (r-\delta,r)$.
\end{itemize}
\end{enumerate}
\end{prop}




\begin{thm}[Min-Max Theorem]   Let $f$ be a function, $(a,b)$ be an open interval contained in the domain of $f$, and  $r\in (a,b)$. Assume that $f$ is differerentiable at $r$. If
$f$ attains its maximum or minimum value on $(a,b)$ at~$r$ (i.e., either $f(r) \geq f(x)$ for all $x \in (a,b)$ or $f(r) \leq f(x)$ for all $x \in (a,b)$), then $f'(r) = 0$.
\end{thm}







\begin{enumerate}

\item Explain how the Min-Max Theorem follows from Proposition~\ref{prop-deri} (which will prove below).

\begin{framed}
We prove the contrapositive. If $f'(r)\neq 0$, then either $f'(r)>0$ or $f'(r)<0$. In either case, it follows from Proposition~\ref{prop-deri} that there are value of $x$ for which $f(x)$ is larger than $f(r)$ and values of $x$ for which $f(x)$ is smaller than $f(r)$, so $f$ achieves neither a minimum nor a maximum at $r$.
\end{framed}

\item Give a counterexample to the converse of the Min-Max Theorem.

\begin{framed}
If $f(x) = x^3$ on $(-1,1)$, then $f'(x)=0$ but $f(1/2) = 1/8>0 = f(0)$ (so $x=0$ is not a maximum) and $f(-1/2) = -1/8<0=f(0)$  (so $x=0$ is not a minimum).
\end{framed}

\item Let $h$ be a function and $r$ be a real number. Use the definition of limit to show that if $\lim_{x\to r} h(x) >0$, then there is some $\delta>0$ such that $h(x)>0$ for all ${x\in (r-\delta,r) \cup (r,r+\delta)}$.

\begin{framed}
Let $\lim_{x\to r} h(x) = L >0$, and take $\e = L$. Then, by the definition, there is some $\delta>0$ such that if $0<|x-r|<\delta$, then $|h(x) - L|<L$. Rewriting this, if $x\in (r-\delta,r) \cup (r,r+\delta)$, then $ 0< h(x) < 2L$, and in particular, $h(x)>0$. 
\end{framed}


\item Use the definition of derivative and the previous problem to prove part (1) of Proposition~\ref{prop-deri}.

\begin{framed}
Suppose that $f'(r)>0$, and consider $h(x) = \frac{ f(x) - f(r)}{x-r}$. Since $\lim_{x\to r} h(x) > 0$, we know that there is some $\delta>0$ such that if $x\in (r-\delta,r) \cup (r,r+\delta)$, then $h(x)>0$. If $x\in (r,r+\delta)$, then $h(x) > 0$ and $x-r > 0$, so $f(x) - f(r) = h(x) ( x-r) >0$, and hence $f(x) >  f(r)$.
If $x\in (r-\delta,r)$, then $h(x) > 0$ and $x-r < 0$, so $f(x) - f(r) = h(x) ( x-r) <0$, and hence $f(x) <  f(r)$.
\end{framed}

\item Deduce part (2) of Proposition~\ref{prop-deri} from the previous problem (by applying it to a different function).

\begin{framed}
If $f'(r)>0$, then let $g(x) = -f(x)$. Then $g'(r) = -f'(r) >0$, so there is some $\delta>0$ such that $g(x)>g(r)$ for all $x\in (r,r+\delta)$, and
$g(x)<g(r)$ for all $x\in (r-\delta,r)$. Thus, $f(x)<f(r)$ for all $x\in (r,r+\delta)$, and
$f(x)>f(r)$ for all $x\in (r-\delta,r)$.
\end{framed}

\noindent \textbf{Definition:} Let $S$ be a set of real numbers, and $f:S\to \R$ be a function. We say that $f$ is \emph{increasing} on $S$ if for any $a,b\in S$ with $a<b$, we have $f(a) \leq f(b)$.

\

\item Prove or disprove: If $f:\R \to \R$ is differentiable at $r$, and $f'(r)>0$, then there is some $\delta>0$ such that $f$ is increasing on $(r-\delta,r+\delta)$.

\begin{framed}
For a counterexample, consider the function \[ f(x) = \begin{cases} x^2 & \text{if} \ x \in \Q \\ 0 & \text{if} \ x \notin \Q,\end{cases} \]
and let $g(x) = f(x) + x$. Then since $f'(0)=0$, we have $g'(0) = 0 +1 = 1 >0$. However, $f$ is not increasing on any interval of the form $(-\delta,\delta)$. Indeed, there is some rational number $q\in (0,\delta)$, so $f(q) = q^2+ q >q$ since $q>0$. Then there is an irrational number $z\in (q, \min{q^2+q, \delta})$. We have $q<z$ but $f(q) = q^2+q > z = f(z)$, so $f$ is not increasing.
\end{framed}

\item Prove or disprove: If $f$ is continuous on $[a,b]$, and there exist $s,t\in (a,b)$ such that $f'(s) > 0$ and $f'(t)< 0$, then there exist $c,d\in [a,b]$ such that $c\neq d$ and $f(c) = f(d)$.

\begin{framed}
We prove the statement. We consider a few cases. 

First, let's assume that $s<t$ and $f(s)<f(t)$. Then, using the Proposition above we can find some $r$ such that $s<r<t$ and $f(r)>f(t)$. Let $y$ be any number such that ${f(t)< y< f(r)}$. By the IVT, there is some $c\in [r,t]$ such that $f(c) = y$; note that $c\neq r$. Also, since ${f(s) < y < f(r)}$, by the IVT, there is some $d\in [s,r]$ such that $f(d) = y$; note that $c\neq d$ since $c\notin [s,r]$, so we are done in this case.

Second, let's assume that $s<t$ and $f(s)>f(t)$. Then, using the Proposition above we can find some $r$ such that $s<r<t$ and $f(r)>f(s)$. Let $y$ be any number such that ${f(s)< y< f(r)}$. By the IVT, there is some $c\in [s,r]$ such that $f(c) = y$; note that $c\neq r$. Also, since ${f(t) < y < f(r)}$, by the IVT, there is some $d\in [r,t]$ such that $f(d) = y$; note that $c\neq d$ since $c\notin [r,t]$, so we are done in this case.

The other cases ($s>t$ and $f(s)<f(t)$, $s>t$ and $f(s)>f(t)$) are similar.
\end{framed}

\end{enumerate}

\Dec{1}


\begin{proof}[Proof of Chain rule]
Since $f$ is differentiable at $r=g(s)$, we have that
\[f'(r)=\lim_{y\to r} \frac{f(y)-f(r)}{y-r}.\]
We can rewrite this by setting
\[d(y)=\frac{f(y)-f(r)}{y-r} - f'(r) \]
and observing that $f$ is differentiable at $r$ implies that $\lim_{y\to r} d(y) =0$. At the moment, $d(y)$ is not defined at $y=r$, but we can extend the definition by setting
\[d(y) = \begin{cases} \frac{f(y)-f(r)}{y-r} - f'(r) & \text{if } y\neq r \\
0& \text{if } y= r. \end{cases}\]
By the limit condition for continuity, $d(y)$ is continuous at $y=r$. The point is that we can now write
\[ f(y) - f(r) = \big( f'(r) + d(y) \big)  (y-r),\]
which holds for all $y$ in the domain of $f$. Substituting $y=g(x)$ (and $r=g(s)$), we have an equality
\[ f(g(x)) - f(g(s)) = \big( f'(g(s)) + d(g(x)) \big)  (g(x)-g(s)).\]
We can divide both sides to get 
\[ \frac{f(g(x)) - f(g(s))}{x-s} = \big( f'(g(s)) + d(g(x)) \big)  \frac{g(x)-g(s)}{x-s}.\]
We want to take the limit as $x$ approaches $s$ of both sides. Since $g$ is continuous at $x=s$, and $d$ is continuous at $y=r=g(s)$, $d\circ g$ is continuous, which implies that $\lim_{x\to s} d(g(x)) = d(g(s))=0$. Using the algebra rules for limits and the definition of the derivative, we have that the limit of the right hand side above is $f'(g(s)) \cdot g'(s)$. By definition, the limit of the left hand side is $(f\circ g)'(s)$. Thus, we have shown the stated equality.
\end{proof}

%In the homework, we have seen an example of a function that is differentiable at only one point. We also saw an example of a function that is differentiable on $\R$, but the derivative is in fact no longer differentiable. Here is a simpler version of this example.
%
%\begin{ex} Let 
%\[ f(x) = \begin{cases} x^2 &\text{if} \ x\geq 0 \\ 0 &\text{if} \ x<0.\end{cases}\]
%For $r>0$, on some open interval containing $r$, $f(x) = x^2$, so $f'(r)$ equals the derivative of $x^2$ at $r$, which is $2r$. For $r<0$, on some open interval containing $r$, $f(x) = x^2$, so $f'(r)$ equals the derivative of $0$ at $r$, which is $0$. For $r=0$, we need to use the definition.
%\[ \lim_{x\to 0} \frac{f(x) - f(0)}{x-0} = \lim_{x\to 0}\frac{f(x) }{x} = \lim_{x\to 0} \begin{cases} x &\text{if} \ x> 0 \\ 0&\text{if}\  x < 0\end{cases}.\]
%We claim the limit is $0$. Indeed, given $\e>0$, take $\d=\e$. Fix $x$ such that $0<|x|<\d$. If $x>0$, then $|\frac{f(x)}{x}| = x <\d = \e$, and if $x<0$, then $\frac{f(x)}{x}| =0<\e$. This proves the claim. Thus,
%\[ f'(x) = \begin{cases} 2x &\text{if} \ x\geq 0 \\ 0 &\text{if} \ x < 0.\end{cases}\]
%However, this function is not differentiable at $x=0$, as 
%\[ \lim_{x\to 0} \frac{f'(x) - f'(0)}{x-0} = \lim_{x\to 0}\frac{f'(x) }{x} = \lim_{x\to 0} \begin{cases} 2 &\text{if} \ x> 0 \\ 0&\text{if}\ x < 0\end{cases}\]
%does not exist.
%\end{ex}
%
%When the derivative of a function is again differentiable, we call that derivative the second derivative of the original function.
%
%Must the derivative of a function be continuous? If we try to make the derivative ``jump'' at a point, we have a problem. For example, if we take a function $f(x)$ with $f(0)=0$ and $f'(x)=1$, and another function $g(x)$ with $g(0)=0$ and $g'(0)=-1$, then the function obtained by combining these
%\[ h(x) = \begin{cases} f(x) &\text{if} \ x\geq 0 \\ g(x) &\text{if} \ x< 0\end{cases}\]
%is no longer differentiable at $x=0$.

\begin{ex}
Consider the function $f(x)=\begin{cases} x^2 \sin(\frac{1}{x}) & \text{if } x\neq 0 \\ 0 & \text{if } x=0.\end{cases}$
First, let's consider whether $f(x)$ is differentiable at $x=0$. 

\[ \lim_{x\to 0} \frac{f(x) - f(0)}{x-0} = \lim_{x\to 0} \frac{x^2 \sin(\frac{1}{x})}{x} = \lim_{x\to 0} x \sin\left(\frac{1}{x}\right) =0,\]
as you showed using the Squeeze Theorem. Thus $f'(0)=0$.
If we take for granted that $\sin'(x)=\cos(x)$ for all $x$, then it follows from the chain rule that $f$ is differentiable for all $x\neq 0$ and in particular that $f'(x) = 2x \sin(\frac{1}{x}) - \cos(\frac{1}{x})$ for $x\neq 0$. Altogether, we have that $f$ is differentiable on all of $\R$, and
\[ f'(x) = \begin{cases}   2x \sin(\frac{1}{x}) - \cos(\frac{1}{x}) &\text{if} \ x\neq 0 \\0 &\text{if} \ x=0 .\end{cases}\]
This derivative function is not continuous at $x=0$. One way to see this is to consider the two sequences $\{\frac{1}{2\pi n}\}_{n=1}^\infty$ and $\{\frac{1}{2\pi n + \pi}\}_{n=1}^\infty$ which both converge to $0$, but the resulting sequences after $f'$ is applied converge to $1$ and $-1$, respectively, so $\lim_{x\to 0} f'(x)$ does not exist.
\end{ex}

\Dec{3}

\begin{thm}[Rolle's Theorem]
Let $f$ be continuous on the closed interval $[a,b]$ and differentiable at every point of $(a,b)$. If $f(a) = f(b)$, then there exists a $c \in
  (a,b)$ such that $f'(c) = 0$.
\end{thm}



\begin{thm}[Mean Value Theorem] Assume $f$ is continuous on the closed interval $[a,b]$ and differentiable at every point of $(a,b)$. Then there exists a $c \in
  (a,b)$ such that 
$$
f'(c) = \frac{f(b) - f(a)}{b-a}.
$$
\end{thm}




\begin{defn} Let $f$ be a function, and $S\subseteq \R$ be a set of real numbers contained in domain of~$f$. We say that 
\begin{itemize}
\item $f$ is \emph{increasing} on $S$ if for any $a,b\in S$ with $a<b$ we have $f(a) \leq f(b)$;
\item $f$ is \emph{decreasing} on $S$ if for any $a,b\in S$ with $a<b$ we have $f(a) \geq f(b)$;
\item $f$ is \emph{constant} on $S$ if for any $a,b\in S$ with $a<b$ we have $f(a) = f(b)$;
\item  $f$ is \emph{strictly increasing} on $S$ if for any $a,b\in S$ with $a<b$ we have $f(a) < f(b)$;
\item $f$ is \emph{strictly decreasing} on $S$ if for any $a,b\in S$ with $a<b$ we have $f(a) > f(b)$.
\end{itemize}
\end{defn}



\begin{cor}
Suppose $I$ is an open interval (that is, $I = (a,b)$, $(a, \infty)$, $(-\infty, b)$, or $(\infty, \infty)$) and $f$ is differentiable on all of $I$.  
\begin{enumerate}
\item $f'(x) \geq 0$ for all $x \in I$ if and only if $f$ is increasing on all of $I$.
\item $f'(x) \leq 0$ for all $x \in I$ if and only if $f$ is decreasing on all of $I$.
\item $f'(x) = 0$ for all $x \in I$ if and only if $f$ is a constant function on $I$. 
\end{enumerate}
\end{cor}




\begin{enumerate}
\item In this problem, we prove Rolle's Theorem. \begin{enumerate}
\item First, assume that $f$ is constant on $[a,b]$, and prove the Theorem in this case.
\item Explain why $f$ has a minimum value and a maximum value on $[a,b]$.
\item Explain why, in the case that $f$ is not constant, either the minimum or maximum value for $f$ occurs in $(a,b)$, and conclude the proof.
\end{enumerate}

\begin{framed}
If $f$ is constant on $[a,b]$, then the derivative of $f$ is zero at every point. By the Extreme Value Theorem, $f$ attains a minimum and a maximum on $[a,b]$; say $m$ and $M$, respectively. We have $m \leq f(a) = f(b) \leq M$. If $m$ and $M$ are both equal to $f(a)$ and $f(b)$, then $f$ is constant on $[a,b]$, and we're done. Otherwise, either the minimum or maximum occurs at some $c$  other than $a$ and $b$, so in $c\in (a,b)$. Then the Min-Max Theorem says that $f'(c)=0$.
\end{framed}


\item Prove the Mean Value Theorem.
\begin{itemize}
\item Suggestion: Let $\ell(x) = \frac{f(b) - f(a)}{b-a}\, x$, and show that $f(x) - \ell(x)$ satisfies the hypotheses of Rolle's Theorem.
\end{itemize}

\begin{framed}
Let $\ell(x) = \frac{f(b) - f(a)}{b-a}\, x$. Note that \[\begin{aligned} (f(b) -\ell(b))& - (f(a) - \ell(a)) = (f(b) - f(a)) - (\ell(b) - \ell(a)) \\&= (f(b)-f(a)) - (\frac{f(b)-f(a)}{b-a} \ (b-a)) = 0,\end{aligned}\]
and $f(x) - \ell(x)$ is continuous on $[a,b]$ and differentiable on $(a,b)$. Thus, by Rolle's Theorem, there is some $c\in (a,b)$ such that \[0=(f-\ell)'(c)=f'(c) - \frac{f(b)-f(a)}{b-a},\]
and the theorem follows.
\end{framed}

\item In this problem, we prove Corollary~36.4.
\begin{enumerate}
\item For the $(\Rightarrow)$ direction of (1), let $a,b\in I$ with $a<b$. Explain why the Mean Value Theorem applies to $f$ on $[a,b]$, and apply it.
\item For the $(\Leftarrow)$ direction of (1), prove the contrapositive using a result from Monday.
\item Prove the rest of the Corollary.
\end{enumerate}

\begin{framed}
Assume that $f'(x) \geq 0$ for all $x\in I$. For $a<b$ in $I$, we have that $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$, so the MVT applies: there is some $c\in (a,b)$ such that $f'(c) = \frac{f(b)-f(a)}{b-a}$. Since $f'(c) \geq 0$ and $b-a>0$, we must have $f(b)-f(a)\geq 0$, so $f(a) \leq f(b)$.

Now assume that $f'(x) < 0$ for all some $x\in I$. By Proposition~34.1, there is some $\delta>0$ such that $f(y) < f(x)$ for all $y\in (x,x+\d)$, so there is some $y\in I$ with $x<y$ and $f(x) > f(y)$. This implies that $f$ is not increasing on $I$.

For (2), we can argue similarly or apply (1) to $-f(x)$.

For (3), we can argue similarly or observe that $f$ is constant if and only if it is both increasing and decreasing on $I$, and that $f'(x) = 0$ on $I$ if and only if $f'(x)\geq 0$ and $f'(x)\leq 0$ for all $x\in I$.
\end{framed}

\item Prove or disprove: If $J= (-\infty,0) \cup (0,\infty)$ and that $f'(x) = 0$ for all $x\in J$, then $f$ is constant on $J$.

\

\item Prove or disprove: If $f$ is differentiable on $\R$ and $f'(r)>0$, then there is some $\delta>0$ such that $f$ is increasing on $(r-\delta,r+\delta)$.

\


\item Let $f$ be a function and $r$ be a real number. Suppose that $f$ is differentiable on an open interval containing $r$, and $f'$ is also differentiable at $r$. Show that if $f'(r)=0$  and $f''(r)<0$, then there is some $\delta>0$ such that $f$ achieves its maximum on $(r-\delta,r+\delta)$ at $x=r$.

\

\item Come up with a precise definition of \emph{concave up/down} on an interval $I$, then state and prove the analogue of Corollary~36.4 for concavity using second derivatives.
\end{enumerate}

\Dec{6}

\begin{defn} Let $S$ and $T$ be two subsets of $\R$, and $f:S\to T$ be a function. We say that a function $g: T\to S$ is the \emph{inverse function} of $f$ if $(g\circ f)(x)= x$ for all $x\in S$ and $(f\circ g)(y) = y$ for all $y\in T$. When an inverse function exists, we denote it by $f^{-1}$.
\end{defn}

\begin{ex} For $f:[0,\infty) \to [0,\infty)$ given by the rule $f(x)=x^2$, the inverse function $f^{-1}:[0,\infty) \to [0,\infty)$ is given by $f^{-1}(x) = \sqrt{x}$.
\end{ex}

\begin{rem} A function $f: S\to T$  has an inverse function if for every $t\in T$ there is a unique $s\in S$ such that $f(s)=t$ (by setting $f^{-1}(t)$ to be this unique $s$). The converse is also true.
\end{rem}

\begin{lem}\label{lem:oi} Let $I$ be an open interval and $f: I \to \R$ be a strictly increasing function. Then for any $a,b\in I$ with $a<b$, the function $f:(a,b) \to (f(a),f(b))$ given by restricting the domain of $f$ to $(a,b)$ has an inverse function $f^{-1}:(f(a),f(b)) \to (a,b)$.
\end{lem}
\begin{proof} Fix $a,b\in I$ with $a<b$. We will show that for any $y\in (f(a),f(b))$, there is a unique $c\in (a,b)$ such that $f(c) = y$.

For existence, we apply the IVT to $f$ on $[a,b]$: we get that there is a $c\in [a,b]$ such that $f(c)=y$. Since $y\neq f(a)$ and $y\neq f(b)$, we must have $c\neq a$ and $c\neq b$, so $c\in (a,b)$.

For uniqueness, suppose that $c,d\in(a,b)$ with $f(c)=f(d)=y$. Both $c<d$ and $d<c$ are impossible: if $c<d$ then $f(c)<f(d)$, and if $d<c$ then $f(d)<f(c)$. Thus $c=d$.
\end{proof}

%\begin{lem} Let $S$ and $T$ be two subsets of $\R$, and $f:S\to T$ be a function. $f$ has an inverse function if and only if for every $t\in T$ there is a unique $s\in S$ such that $f(s)=t$.
%\end{lem}
%\begin{proof}
%Suppose that $f$ has an inverse function $f^{-1}:T\to S$. Let $t\in T$. Note that for $s=f^{-1}(t)$ we have $f(s) = f(f^{-1}(t))=t$. If $f(s)=t$ and $f(s')=t$ then $s=f^{-1}(f(s))=f^{-1}(t)$ and likewise $s'=f^{-1}(t)$.

%Suppose that $f$ satisfies the condition that or every $t\in T$ there is a unique $s\in S$ such that $f(s)=t$.
%We define a function $g:T\to S$ by the rule $

\begin{prop}\label{prop:oi} Let $I$ and $J$ be open intervals. Let $f:I\to J$ be a function, and $f^{-1}: J \to I$ be its inverse. If $f$ is strictly increasing and continuous, then $f^{-1}$ is strictly increasing and continuous.
\end{prop}
\begin{proof}
First we show that $f^{-1}$ is strictly increasing. Let $c,d\in J$ with $c<d$. Let $a=f^{-1}(c)$ and $b=f^{-1}(d)$. We must have $a<b$; otherwise, $a\geq b$, and since $f$ is strictly increasing, $c=f(a) \geq f(b) =d$, which contradicts that $c<d$. This shows that $f^{-1}$ is strictly increasing.

Now we show that $f^{-1}$ is continuous. This means that $f^{-1}$ is continuous at $t$ for every $t\in J$. Let $t\in J$ be given, and set $s=f^{-1}(t)$. Let $\e>0$ be arbitrary. We can find some $\e_0>0$ such that $\e_0 \leq \e$ and $[s-\e_0 , s+\e_0] \subseteq I$ (e.g., if $I=(u,v)$, take $\e_0= \min\{\e, s-u,v-t\}$). Since $f$ is strictly increasing
\[ f(s-\e_0) < f(s) =t < f(s+\e_0).\]
Take $\d$ to be any positive number such that
\[ f(s-\e_0) < t-\d < t + \d < f(s+\e_0)\]
(e.g., $\d = \frac{1}{2} \min\{ t- f(s-\e_0), f(s+\e_0) -t\}$.)
We claim that this $\d$ works to show that $f^{-1}$ is continuous at $t$. Indeed, if $y$ is any real number such that $|y-t|<\d$, then 
\[ f(s-\e_0) < y < f(s+\e_0),\]
so
\[ s-\e_0 = f^{-1}(f(s-\e_0)) < f^{-1}(y) < f^{-1}(f(s+\e_0)) = s+\e_0,\]
since $f^{-1}$ is strictly increasing.
Thus, \[ |f^{-1}(y) -f^{-1}(t)| = |f^{-1}(y) - s| < \e_0 \leq \e.\]
This proves that $f^{-1}$ is continuous at $y=t$.
\end{proof}

\begin{thm}[Inverse Function Theorem] Let $f:S\to \R$ be a function. Suppose that $f$ is differentiable at $s$, that $f'(s) \neq 0$, and that $f'$ is continuous at $s$. (In particular, $f$ is differentiable on some interval around $s$.) Then there is some open interval $I$ that contains $s$ and an open interval $J$ that contains $f(s)$ such that $f: I\to J$ has an inverse function $f^{-1}:J \to I$. Furthermore, $f^{-1}$ is differentiable at $f(s)$, and $(f^{-1})'(f(s))=\frac{1}{f'(s)}$.
\end{thm}
\begin{proof}
Let $f$ be as in the statement. We either have $f'(s)>0$ or $f'(s)<0$. 

First, we consider the case $f'(s)>0$. Since $f'$ is continuous at $s$, there is some $\d>0$ such that $f'(x)>0$ for all $x\in (s-\d,s+\d)$. Let $I'=(s-\d,s+\d)$. By an argument similar to Corollary~36.4, $f$ is strictly increasing on $I'$. Since $f$ is differentiable on $I$, it is also continuous on $I$. Let $a,b\in I'$ with $a<s<b$. Set $I=(a,b)$.

Then, by Lemma~\ref{lem:oi} $f:(a,b) \to (f(a),f(b))$ is strictly increasing and continuous and has an inverse function $f^{-1}:(f(a),f(b))\to (a,b)$. By Proposition~\ref{prop:oi}, $f^{-1}$ is also strictly increasing and continuous.

Now we show that $f^{-1}$ is differentiable at $f(s)$. Let $\{y_n\}_{n=1}^\infty$ be a sequence with $y_n\neq f(s)$ for all $n$ that converges to $f(s)$. Let $x_n = f^{-1}(y_n)$ for all $n$, so $f(x_n) = f(f^{-1}(y_n))=y_n$. Since $f^{-1}$ is continuous at $f(s)$, 
\[ \lim_{n\to \infty} x_n = \lim_{n\to \infty} f^{-1}(y_n) = f^{-1}  \left( \lim_{n\to \infty} y_n\right) = f^{-1}(f(s)) = s.\]
Then
\[ \lim_{n\to \infty} \frac{f^{-1}(y_n) - f^{-1}(f(s))}{y_n - f(s)} = \lim_{n\to \infty} \frac{x_n - s}{f(x_n) - f(s)},\]
and since
\[ \lim_{x\to s} {f(s)-f(s)}{x-s} = f'(s)\neq 0,\]
we have
\[ \lim_{n\to \infty} \frac{f(x_n) - f(s)}{x_n - s} = f'(s) \qquad \lim_{n\to \infty} \frac{x_n - s}{f(x_n) - f(s)} = \frac{1}{f'(s)}\]
by Theorem~25.1.
Since this holds for all sequences $\{y_n\}_{n=1}^\infty$, we conclude that 
\[ \lim_{y\to s} \frac{f^{-1}(y) - f^{-1}(f(s))}{y - f(s)} = \frac{1}{f'(s)}.\]
This concludes this case.

Now, suppose that $f'(s)<0$. Set $g(x) =-f(x)$. Then since $f'$ is continuous at $s$, $g'$ is as well, and $g'(s)>0$. By the first case, there exist $a<s<b$ such that $g:(a,b)\to (g(a),g(b))$ has an inverse $g^{-1}: (g(a),g(b)) \to (a,b)$. Define the function $h:(f(b),f(a)) \to (a,b)$ by the rule $h(y) = g^{-1}(-y)$. Then, if $x\in (a,b)$, $(h\circ f)(x) = g^{-1}(-(-g(x))) = g^{-1}(g(x))=x$, and if $y\in (f(b),f(a))$, $-y\in (g(a),g(b))$, so $f\circ h)(y) = - g(g^{-1}(-y))=-(-y)=y$. Thus, $h=f^{-1}$ is an inverse for $f$ on $(a,b)$. Finally, by the Chain rule, since the function $-x$ is differentiable everywhere, we get 
\[h'(f(y)) = -1 \cdot (g^{-1})'(-f(y)) = -1 \cdot (g^{-1})'(g(y)) = \frac{-1}{g'(y)} = \frac{1}{f'(y)}.\qedhere\]
\end{proof}



%I want to mention a Corollary of the Min-Max Theorem.
%
%
%
%\begin{cor} Suppose $f$ is continuous on the closed interval $[a,b]$.
%Then $f$ attains its maximum and minimum values
%  on $[a,b]$ and both of these occur at points $x$ such that one of the following conditions hold:
%\begin{enumerate}
%\item $x = a$,
%\item  $x = b$, 
%\item $f$ is differentiable at $x$ and $f'(x) = 0$, or
%\item $f$ is not differentiable at $x$.
%\end{enumerate}
%\end{cor}
%
%\begin{proof} The Extreme Value Theorem shows that $f$ attains its maximum and minimum --- say its maximum occurs at $r$ and its minimum occurs at $s$. 
%  We need to show that $r = a$, $r = b$, $f'(r) = 0$, or $f$ is not differentiable at $r$, and similarly for $s$.
%
%  Assume the first, second, and fourth conditions
%do not hold --- i.e. assume $r \ne a$, $r \ne b$, and that $f$ is differentiable. Then by the Min-Max
%Theorem, $f'(r) = 0$.
%
%Similarly one shows that one of these four properties holds for $s$.
%\end{proof}
%
%\begin{rem} Each of the four cases above is possible. For example:
%\begin{itemize}
%\item $f(x)=x$ achieves its maximum on $[-1,1]$ at $x=1$ and its minimum on $[-1,1]$ at $x=-1$, but $f'(x)=1\neq 0$ at both of these points.
%\item $f(x)=|x|$ achieves its minimum on $[-1,1]$ at $x=0$, where $f$ is not differentiable.
%\end{itemize}
%\end{rem}
%
%
%
%Our last big goal in studying derivatives is to pin down the relationship between derivatives and increasing/decreasing functions. We will need the following definition.

\begin{comment}

\begin{defn} Let $f$ be a function, and $S\subseteq \R$ be a set of real numbers contained in domain of $f$. We say that 
\begin{itemize}
\item $f$ is \emph{increasing} on $S$ if for any $a,b\in S$ with $a<b$ we have $f(a) \leq f(b)$;
\item $f$ is \emph{decreasing} on $S$ if for any $a,b\in S$ with $a<b$ we have $f(a) \geq f(b)$;
\item $f$ is \emph{constant} on $S$ if for any $a,b\in S$ with $a<b$ we have $f(a) = f(b)$;
\item  $f$ is \emph{strictly increasing} on $S$ if for any $a,b\in S$ with $a<b$ we have $f(a) < f(b)$;
\item $f$ is \emph{strictly decreasing} on $S$ if for any $a,b\in S$ with $a<b$ we have $f(a) > f(b)$.
\end{itemize}
\end{defn}




\begin{thm}[Rolle's Theorem] 
Assume $f$ is continuous on the closed interval $[a,b]$ and differentiable at every point of $(a,b)$. If $f(a) = f(b) = 0$, then there exists a $c \in
  (a,b)$ such that $f'(c) = 0$.
\end{thm}

\begin{proof} By the EVT Theorem $f$ attains a maximum value $M$ and a minimum value $m$ on $[a,b]$. If $m = M$ then $f$ is a constant function and in this case
  $f'(c) = 0$ for every $c$ in $(a,b)$. Otherwise, we either have $M > 0$ or $m < 0$. 
  If $M > 0$, then choose a $c$ so that $f(c) = M$. Note that $c \in (a,b)$, and so by the Corollary to the Min-Max Theorem, since we assume $f$ is differentiable at $c$,
  we must have $f'(c) =0$.

  Similarly, if $m > 0$, then choose a $c$ so that $f(c) = m$. The same reasoning shows $f'(c) =0$.
\end{proof}


\begin{cor}[The Mean-Value Theorem] 
Assume $f$ is continuous on the closed interval $[a,b]$ and differentiable at every point of $(a,b)$. Then there exists a $c \in
  (a,b)$ such that 
$$
f'(c) = \frac{f(b) - f(a)}{b-a}.
$$
\end{cor}


\begin{proof} The trick is to ``tilt'' $f(x)$ so that we can apply Rolle's Theorem.
That is, let 
$$
g(x) = f(x) - l(x)
$$
where $l(x)$ is the linear function satisfying  $l(a) = f(a)$ and $l(b) = f(b)$ --- in detail,  
$$
l(x) = \frac{f(b)}{b-a} (x-a) +\frac{f(a)}{b-a} (x-b).
$$
Then $g$ is continuous on $[a,b]$, differentiable on $(a,b)$ and we have $g(a) = 0$ and $g(b) = 0$. By Rolle's Theorem, there is a point $c \in (a,b)$ such that
$g'(c) = 0$. But
$$
g'(x) = f'(x) - l'(x) = f'(x) - \frac{f(b)}{b-a} +\frac{f(a)}{b-a}  = f'(x) - \frac{f(b) - f(a)}{b-a}.
$$
and so the fact that $g'(c) = 0$ gives that
$$
f'(c) = \frac{f(b) - f(a)}{b-a}.
$$
\end{proof}

The MVT has many applications. Here is one:

\begin{cor}
Suppose $I$ is an open interval (that is, $I = (a,b)$, $(a, \infty)$, $(-\infty, b)$ or $(\infty, \infty)$) and $f$ is differentiable on all of $I$.  
\begin{enumerate}
\item $f'(x) \geq 0$ for all $x \in I$ if and only if $f$ is increasing on all of $I$.
\item $f'(x) \leq 0$ for all $x \in I$ if and only if $f$ is decreasing on all of $I$.
\item $f'(x) = 0$ for all $x \in I$ if and only if $f$ is a constant function on $I$. 
\end{enumerate}
\end{cor}

\begin{rem} Remember that, by definition, $f$ is increasing on $I$ if and only if $f(a) \leq f(b)$ for all $a \leq b$, and similarly for decreasing.
  \end{rem}

  \begin{proof} For part (1), assume $f'(x) \geq 0$ for all $x \in I$.  Pick any two points $a,b \in I$ with $a < b$. Since $f$ is differentiable at every point of $[a,b]$,
  it is continuous at every point in $[a,b]$ too. 
So, we have that $f$ is continuous on $[a,b]$ and
  differentiable on $(a,b)$, and hence,  by the MVT, there is a $c \in (a,b)$
  such that
$$
f'(c) = \frac{f(b)- f(a)}{b-a}.
$$
Since $b-a > 0$ and $f'(c) \geq 0$, it follows that $f(b) \geq f(a)$.  This proves $f$ is increasing on $I$. 

For the converse we prove instead the contrapositive. Assume there is
a point $x \in I$ such that $f'(x) < 0$. We will show that $f$ is not increasing. Since $f'(x) < 0$, as we showed in the
proof of Min-Max Theorem, there is a $\d > 0$
such that for all $y$ such that $x < y < x + \d$ and $y \leq b$, we have $f(y) < f(x)$; i.e. $f$ is not increasing. 

The proof of (2) is very similar --- or we can deduce (2) from (1) by considering $-f(x)$.

For (3), we already know that if $f$ is the constant function then $f'(x) = 0$ for all $x$. 
If $f'(x) = 0$ for all $x \in I$, then by (1) and (2), $f$ is both increasing and decreasing on all of $I$, and this only occurs when $f$ is constant.
\end{proof}

\begin{comment}

%$f(x)$: \vskip .2in\includegraphics[width = 1.75in]{Nov6graph1.png}

\vskip .5in

%$f'(x)$: \vskip .2in \includegraphics[width = 1.75in]{Nov6plot2.png}

\end{ex}


On the other hand, Darboux's Theorem allows us to conclude that certain functions cannot have anti-derivatives: 


\begin{ex}
  Suppose
$$
g(x) = 
\begin{cases}
x & \text{ if $x < 1$ and }\\
x + 1& \text{ if $x \geq 1$.} \\ 
\end{cases}
$$
Does $g(x)$ have an anti-derivative? No, according to Darboux's
Theorem: 
If $g(x) = f'(x)$ for all $x$ for some function $f(x)$, then 
taking $a = 0$ and $b = 2$ in Darboux's Theorem, we would have that for all $y$ such that $0 = g(0) < y < g(2) = 3 $ there would be a $c \in
(0, 2)$ such that $g(c) = y$. This is clearly not the case for $y = \frac32$, and so no such $f$ exists. 
\end{ex}
















 
\section{Monday, November 9}

We will next talk about definite integrals. Recall that if $f(x)$ is a function defined on $[a,b]$, then $\int_a^b f(x) \, dx$ is, heuristically speaking, the
area under the graph of $f(x)$ between $x = a$ and $x = b$ (at least if $f$ is positive). In particular, it is a number. Do not confuse this number with the
primary method of finding it: namely, the Fundamental Theorem of Calculus (which we will talk about eventually). 


But what do we really mean by ``area under the graph''? What is the rigorous definition?

In order to motivate the rigorous  definition of integral, 
let us consider a few examples and ask ourselves what the value of the definite integral ``ought'' to be:

\begin{itemize}

\item Suppose $f(x) = x$ and the interval is $[0,3]$. What should
  $\int_0^3 f(x) \, dx$ be? It should be $\frac92$.

\item Suppose
$$
f(x) = 
\begin{cases}
x & \text{ if $0 \leq x < 1$ and }\\
x + 1& \text{ if $1 \leq x \leq 2$.} \\ 
\end{cases}
$$
What should $\int_0^2 f(x) \, dx$ be? It should be $3$. Note that in this example $f(x)$ does not have an anti-derivative, as we saw above. So, there is no way to calculate the
value of $\int_0^2 f(x) \, dx$ using the Fundamental Theorem of Calculus (at least not by itself). 

\item Suppose $f(x) = 0$ for all $x \ne 0$ and $f(0) = 42$. What should $\int_{-7}^{13} f(x) \, dx$ be? It should be $0$. Here too $f(x)$ has no anti-derivative.  

\item Suppose $f(x) = 42$ for all $x \ne 0$ and $f(0) = 0$. What should $\int_{-7}^{13} f(x) \, dx$ be? It should be $19
  \cdot 42$.


\item Suppose
$$
f(x) = 
\begin{cases}
1 & \text{ if $x \in \Q$ and }\\
0 & \text{ if $x \notin \Q$.} \\ 
\end{cases}
$$
What should $\int_0^1 f(x) \, dx$ be? It is in fact undefined.

\end{itemize}





You might recall that one can approximate the value of $\int_a^b f(x) \, dx$ using Riemann sums. It turns out that Riemann sums are actually {\em how the
  definite integral is defined rigorously}. 

In calculus, you might only have seen left and right Riemann sums, and you probably only saw such things when the various rectangles involved all had the same
width. We will need more general versions here. 

Here is some important terminology:

\begin{defn} Given a closed interval $[a,b]$, a {\em partition} of $[a,b]$ is any list of points $\cP = (x_0, x_1, \dots, x_n)$ for some $n \in \N$ such that
$$
a = x_0 < x_1 < \cdots < x_n = b
$$

The {\em norm} of a such a partition $\cP$, written $\| \cP \|$,  is defined to be the
  largest number occurring in the list 
$$
x_1 - x_0, x_2 - x_1,   \dots, x_n - x_{n-1}. 
$$

Given a partition $\cP = (x_0, x_1, \dots, x_n)$ of $[a,b]$, 
a list of {\em sampling points} for $\cP$ is a list $\cS = (s_1, \dots, s_n)$ such that $s_i \in [x_{i-1}, x_i]$ for all $i = 1,
\dots, n$; i.e. $x_{i-1} \leq s_i \leq x_i$ for all $i$. 

Now assume $f$ is a function defined on $[a,b]$. Given a partition $\cP = (x_0, x_1, \dots, x_n)$ of $[a,b]$ and sampling points $\cS = (s_1, \dots, s_n)$, the
{\em Riemann sum} associated to $f$, $\cP$ and $\cS$ is the number 
$$
\begin{aligned}
RS(f, \cP, \cS) & = \sum_{i=1}^n f(s_i)(x_i - x_{i-1}) \\
& = f(s_1)(x_1 - x_0) +  f(s_2)(x_2 - x_1) +  f(s_3)(x_3 - x_2) + \cdots +  f(s_n)(x_n - x_{n-1}).\\
\end{aligned}
$$
\end{defn}




\begin{ex} Let $f(x) = x^2$ and consider the closed interval $[3, 7]$. 

Let $\cP$ be the partition $(3, 3.2, 5.1, 6, 6.8,  7)$ (so $n = 5$ here). 
Note that $\|\cP\| = 1.9$.


Let $\cS$ be the sampling points  $(3.15, 4, 5.5, 6, 7)$. Then
$$
RS(f, \cP, \cS) = 3.15^2(.2) + 4^2(1.9) + 5.5^2(.9) + 6^2(.8) + 7^2(.2) = 98.2095 \text{ (exactly)}.
$$

This number approximates the area under $y = x^2$, above the $x$-axis, between $x = 3$ and $x = 7$. 
Of course, from calculus we know that  the actual value of this area is $\int_3^7 x^2 \, dx = \frac{7^3}{3} -
\frac{3^3}{3} = 105.\overline{3}$.
\end{ex}

% \begin{ex}
% Let $f(x) = x^2$ and use the interval $[3,7]$ again. This time take $\cP = (3,7)$ (so, $n = 1$, the smallest possible
% value)
% and $\cS = (3)$. Then $RS(f, \cP, \cS) = 3^2 \cdot 4 = 36$.

% Note that $\|\cP \| = 4$.
% \end{ex}




% \begin{ex} In general, given a function $f(x)$  defined on an interval $[a,b]$, if we pick any $n \in \N$, we can form the {\em uniform partition} $\cP = (x_0,
%   \dots, x_n)$ of $[a,b]$ by setting 
% $x_i = a + i \cdot \frac{b-a}{n}$ for $i = 0, \dots n$. So, 
% $$
% \cP = (a, a + \frac{b-a}{n}, a + 2 \frac{b-a}{n}, a + 3 \frac{b-a}{n}, \dots, b).
% $$ 
% Note that $x_i - x_{i-1} = \frac{b-a}{n}$ for all $i$. (In particular, $\| \cP \| = \frac{b-a}{n}$.)

% For this uniform partition  $\cP$, if we set $\cS_L = (x_0, \dots, x_{n-1})$, then $RS(f, \cP, \cS_L)$ is the classical {\em left-hand Riemann sum with $n$ subdivisions}.
% If we set $\cS_R = (x_1, \dots, x_n)$ then $RS(f, \cP, \cS_R)$ is the classical {\em right-hand Riemann sum with $n$ subdivisions}.
% Finally if $\cS_M = (s_1, \dots, s_n)$ is given by $s_i = \frac{x_{i-1} + x_i}{2}$, then $RS(f, \cP, \cS_M)$ is the ``{\em mid-point rule}''
% \end{ex}



We can now define $\int_a^b f(x) \, dx$. Loosely speaking, it is the number $I$ so that if we
consider partitions $\cP$ of very small norm, then $RS(f, \cP, \cS)$ is very close to $I$ for all $\cS$. Such a number
need not exist, however, and so $\int_a^b f(x) \, dx$ is sometimes undefined.


\begin{defn} Assume $f(x)$ is defined on $[a,b]$. We say $f$ is {\em integrable} on $[a,b]$ if there exists a number $I$ with the following property:
for all $\e > 0$, there exists a $\d > 0$ such that if $\cP$ is any partition of $[a,b]$ such that $\| \cP \| < \d$ and $\cS$ is any list of sampling points for
$\cP$,
then we
have $|I - RS(f,\cP, \cS)| < \e$. 

In this case, we write  $\int_a^b f(x) \, dx$ for this number $I$. 
(As a technical point: one can prove that  if such an $I$ exists, then it is unique, and so 
$\int_a^b f(x) \, dx$ is well defined when it exists.)
\end{defn}






\section{Wednesday, November 11}

This is a remarkably abstract definition, to be sure. Let's establish a simple example:

\begin{ex} Let $f(x) = x$ defined on $[0,3]$. I claim $f$ is integrable on $[0,3]$ and that $\int_0^3 f(x) \, dx = \frac92$. 
Pick $\e >0$. We need to show that there is a $\d > 0$ such that 
if $\cP$ is any partition of $[0,3]$ such that $\| \cP \| < \d$ and $\cS$ is any list of
sampling points for $\cP$, then $|RS(f, \cP, \cS) - \frac92| < \e$.

To prove this, let us make a few  observations:

Let $\cP = (x_0, x_1, \dots, x_n)$ be any partition of $[0,3]$, so
that $n \in \N$ and $0 = x_0 < x_1 < \cdots < x_n = 3$. 

Define $\cS_L = (x_0, x_1, \dots, x_{n-1})$ and $\cS_R = (x_1, x_2, \dots, x_{n})$. These are both lists of  sampling points for $\cP$. Moreover, since our function is
increasing, 
$$
RS(f, \cP, \cS_L) \leq RS(f, \cP, \cS) \leq RS(f, \cP, \cS_R). 
$$
where $\cS = (s_1, \dots, s_n)$ is any list of sampling poitns for this partition $\cP$. 


Now, a picture shows that $RS(f, \cP, \cS_L) < \frac{9}{2}$ and that 
$\frac92 - RS(f, \cP, \cS_L)$ is the sum of the areas of $n$ little equilateral right triangles, of width and height $x_i - x_{i-1}$, for $i = 1, \dots, n$.
So
$$
\frac92 - RS(f, \cP, \cS_L) = \sum_{i=1}^n \frac12 (x_i - x_{i-1})^2.
$$
Since $x_i  - x_{i-1} \leq \| \cP \|$ for all $i$ (by definition), we get
$$
\frac92 - RS(f, \cP, \cS_L) = \sum_{i=1}^n \frac12 (x_i - x_{i-1})^2 \leq \sum_{i=1}^n \frac12 \| \cP \|  (x_i - x_{i-1}) 
= \frac12 \| \cP \| \sum_{i=1}^n  (x_i - x_{i-1})= \frac{3 \| \cP \|}{2}.
$$
The last equation holds since 
$$
\sum_{i=1}^n  (x_i - x_{i-1}) = (x_1 - x_0) + (x_2 -x_1) + (x_3 - x_2) + \cdots + (x_n - x_{n-1}) = x_n - x_0 = 3 - 0 = 3
$$



Similar reasoning gives that $RS(f, \cP, \cS_R) > \frac{9}{2}$ and 
$$
RS(f, \cP, \cS_R) - \frac92  \leq \frac{3 \| \cP \|}{2}.
$$

We can now establish our claim carefully. Pick $\e > 0$ and let $\d = \frac{\e}{3}$.
Let $\cP$ is any partition of $[0,3]$ such that $\| \cP \| < \d$ and $\cS$ is any list of
sampling points for $\cP$. We have
$$
RS(f, \cP, \cS_L)\leq RS(f, \cP, \cS) \leq RS(f, \cP, \cS_R),
$$
$$
\frac92 - \frac{3 \| \cP \|}{2} < RS(f, \cP, \cS_L) < \frac92
$$
and
$$
\frac92 < RS(f, \cP, \cS_R) < \frac92 + \frac{3 \| \cP \|}{2}
$$
from which it follows that
$$
\frac92 - \frac{3 \| \cP \|}{2} \leq RS(f, \cP, \cS) \leq \frac92 + \frac{3 \| \cP \|}{2}
$$
and since $\| \cP \| < \frac{\e}{3}$ this gives
$$
\frac92 - \frac{\e}{2} \leq RS(f, \cP, \cS) \leq \frac92 + \frac{\e}{2}
$$
and thus $|RS(f, \cP, \cS) - \frac92| < \e$. This proves $f(x)$ is integrable on
$[0,3]$ and that $\int_0^3 x \, dx = \frac{9}{2}$. 
\end{ex}


Some of the ideas used in the previous example are used to prove the following result:

\begin{thm} \label{thm410} If $f(x)$ is monotone on $[a,b]$, then $f$ is integrable on $[a,b]$.
\end{thm}

If we have time before the end of the semester, I will prove this result. 



Here is an easier example:

\begin{ex} Let $f(x)$ be the constant function $f(x) = c$.
For any $a < b$,  I claim $f$ is integrable on the closed interval $[a,b]$ and that $\int_a^b f(x) \, dx = (b-a)c$.

This turns out to be really easy: Let $\cP = (x_0, \dots, x_n)$ be any partition of $[a,b]$ and $\cS = (s_1, \dots, s_n)$ any list of sampling points for
$\cP$. Then
$$
RS(f, \cP, \cS) = \sum_i f(s_i) (x_i - x_{i-1}) = c \sum_i  (x_i - x_{i-1}) = c(b-a).
$$
So, every single Riemann sum gives the same value, namely $c(b-a)$.

Pick $\e> 0$. And let $\d = 10^{100}$, or any positive number you like. If
$\cP = (x_0, \dots, x_n)$ is a  partition of $[a,b]$ such that $\| \cP \| < \d$, and $\cS = (s_1, \dots, s_n)$ any list of sampling points for $\cP$, then $|RS(f,
\cP, \cS) - c(b-a)| = 0 < \e$. This proves $f$ is integrable on the closed interval $[a,b]$ and that $\int_a^b f(x) \, dx = (b-a)c$.
\end{ex}



\begin{ex} 
Define a function  by
$$
f(x) = 
\begin{cases}
1 & \text{ if $x < 1$ and }\\
2& \text{ if $x \geq 1$.} \\ 
\end{cases}
$$
According to the Theorem above, this function is integrable on $[0,2]$. We
would guess that 
$$
\int_0^2 f(x) \, dx = 3.
$$
Let us prove this carefully:

Pick $\e > 0$. Set $\d = \e/4$. Let $\cP = (x_0, \dots, x_n)$ be any partition of $[0,2]$ such that $\|\cP\| < \d$ and
let $\cS = (s_1, \dots, s_n)$ be any list of sampling points for $\cP$.
We need to prove that $|RS(f, \cP, \cS) - 3| < \epsilon$.

Recall $RS(f, \cP, \cS) - 3| = \sum_{i=1}^n f(s_i)(x_i - x_{i-1})$. Let us consider two cases:

{\bf Case 1}: Assume $x_i \ne 1$ for all $i$. Then there is a $j$ such that $x_{j-1} < 1 < x_j$, and we get
$$
\begin{aligned}
  RS(f, \cP, \cS) 
  & = \sum_{i=1}^{j-1}  1 (x_i - x_{i-1}) +  f(s_j)(x_j - x_{j-1})  +\sum_{i=j+1}^{n}  2 (x_i - x_{i-1}) \\
  & = (x_{j-1} - x_0) +  f(s_j)(x_j - x_{j-1})  +2(x_n - x_{j}) \\
    & = (x_{j-1} - 0) +  f(s_j)(x_j - x_{j-1})  +2(2 - x_{j}) 
\end{aligned}
$$
Since $x_{j-1} < 1 < x_j$ and $x_j - x_{j-1} < \d$ we have $1 - x_{j-1} < \d$ and $x_j - 1 < \d$. 
It follows tthat
$$
3 - 3\d  <   RS(f, \cP, \cS)  < 3 + 2 \d.
$$
Since $\d = \e/4$, it follows that $|RS(d, \cP, \cS) - 3| < \e$.


{\bf Case 2}: Assume $x_j = 1$ for some $j \in \{1, \dots, n-1\}$.
Then
$$
\begin{aligned}
  RS(f, \cP, \cS) 
  & = \sum_{i=1}^{j-1}  1 (x_i - x_{i-1}) +  f(s_j)(x_j - x_{j-1})  +
  f(s_{j+1})(x_{j+1} - x_{j}) +
  \sum_{i=j+2}^{n}  2 (x_i - x_{i-1}) \\
  & = (x_{j-1} - 0) +  f(s_j)(1 - x_{j-1}) +
  f(s_{j+1})(x_{j+1} - 1) 
  +2(2 - x_{j+1})
\end{aligned}
$$
In a similar way to the first case we get
$$
(1 - \d) + 2(1 - \d) < RS(f, \cP, \cS) < 1 + 2 \d + 2 \d + 2 
$$
and hence
$$
3 - 2 \d < RS(f, \cP, \cS) < 3 + 4 \d.
$$
Since $\d = \e/4$, it follows that $|RS(d, \cP, \cS) - 3| < \e$.
\end{ex}


\section{Friday, November 13}


We have yet to give an example of a function that isn't integrable. Here is one:



\begin{ex} Let $f$ be the function 
$$
f(x) = 
\begin{cases}
1 & \text{ if $x \in \Q$ and }\\
0 & \text{ if $x \notin \Q$.} \\ 
\end{cases}
$$
I claim $f$ not integrable  on $[0,1]$.

\begin{proof} By way of contradiction, suppose $f$ is integrable on $[0,1]$ and that the value of $\int_0^1 f(x) \, dx$ is $I$. Then for all $\epsilon > 0$, there is a $\d > 0$
  such that for every partition with $\|\cP \| < \d$ and all lists of sampling points $\cS$ for $\cP$, we have $|RS(f, \cP, \cS) - I| < \epsilon$. In particular,
  for $\e = \frac12$, there is a $\d > 0$ such that this condition holds.

  Let $\cP = (x_0, \dots, x_n)$ be any partition of $[0,1]$ such that $\| \cP \| < \d$. For example, we could take $x_i = \frac{i}{n}$ for $n$ a large enough integer so that $\frac{1}{n} <
  \d$.
  By the density of rationals, for each $i$ we can find a rational number $s_i \in [x_{i-1}, x_i]$. Then $\cS = (s_1, \dots, s_n)$ is a list of sampling points for $\cP$ and
  since $f(s_i) = 1$ for all $i$ we have
  $$
  RS(f, \cP, \cS) =   \sum_i f(s_i) (x_i - x_{i-1}) = \sum_i 1 \cdot (x_i - x_{i-1}) = x_n - x_0 = 1 - 0 = 1.
  $$
  This proves that $|I - 1| < \e = \frac12$ and hence $\frac12 < I < \frac32$.

  On the other hand, by the density of itrationals, for each $i$ we can find a rational number $s'_i \in [x_{i-1}, x_i]$. Then $\cS' = (s'_1, \dots, s'_n)$ is a list of sampling points for $\cP$ and
  since $f(s_i) = 0$ for all $i$ we have
  $$
  RS(f, \cP, \cS') =   \sum_i f(s'_i) (x_i - x_{i-1}) = \sum_i 0 \cdot (x_i - x_{i-1}) = 0.
  $$
  This proves that $|I - 0| < \e = \frac12$ and hence $-\frac12 < I < \frac12$.

  So we have $I < \frac12$ and $I > \frac12$, which is not possible. Thus $f$ must not be integrable.
  \end{proof}
\end{ex}








% ---------SKIP -------------


% \begin{proof}[Proof of Theorem \ref{thm410}] 
% We prove this when $f$ is increasing on $[a,b]$--- the proof when $f$ is decreasing on $[a,b]$ is very similar.

% As a matter of notation, if $\cP$ is any partition of $[a,b]$ we write
% $$
% LHRS(f, \cP) = RS(f, \cP, \cS_L)
% $$
% where $\cS_L = (x_0, \dots, x_{n-1})$ and
% $$
% RHRS(f, \cP) = RS(f, \cP, \cS_R)
% $$
% where $\cS_R = (x_1, \dots, x_{n})$. ($LHRS$ stands for left-hand Riemann sum and 
% $RHRS$ stands for left-hand Riemann sum.) 

% {\bf Claim 1}: 
% Given any partition $\cP$ of $[a,b]$, we have
% $$
% f(a)(b-a) \leq LHRS(f, \cP) \leq RS(f, \cP, \cS) \leq RHRS(f, \cP) \leq f(b)(b-a)
% $$
% for any list of sampling points $\cS$ of $\cP$.

% Claim 1 holds since $f$ is increasing:
% $$
% LHRS(f, \cP) 
% = \sum_i f(x_{i-1}) (x_i - x_{i-1}) \leq \sum_i f(s_i) (x_i - x_{i-1})  = RS(f, \cP, \cS)
% $$
% since $x_{i-1} \leq s_i$ and so $f(x_{i-1}) \leq f(s_i)$ for each $i$. Similarly, 
% $RS(f, \cP, \cS) \leq RHRS(f, \cP)$.  
% Finally, $f(a)(b-a) \leq LHRS(f, \cP)$ and $RHRS(f, \cP) \leq f(b)(b-a)$ hold since $f(a) \leq f(x_i) \leq f(b)$ for all $i$. 


% {\bf Claim 2}: 
% $RHRS(f,\cP) - LHRS(f, \cP) \leq (f(b)-f(a)) \| \cP\|$.

% To see that Claim 2 holds, note that
% $$
% RHRS(f,\cP) - LHRS(f, \cP) = \sum_i (f(x_i) - f(x_{i-1}))(x_i - x_{i-1}) \leq \sum_i (f(x_i) - f(x_{i-1})) \|\cP\|
% $$
% and
% $$
% \sum_i (f(x_i) - f(x_{i-1})) \|\cP\| = \|\cP\|   \sum_i (f(x_i) - f(x_{i-1})) = \|\cP\| (f(b) - f(a))
% $$
% since the sum ``telescopes''.


% We now define two sets of real numbers:
% $$
% L := \{ LHRS(f, \cP) \, | \, \text{$\cP$ is any partition of $[0,1]$} \}
% $$
% and
% $$
% U := \{ RHRS(f, \cP) \, | \, \text{$\cP$ is any partition of $[0,1]$.} \}
% $$
% Intuitively, since $f$ is increasing, every member of $L$ is an under-estimate of the true value of the integral and
% every member of $U$ is an over-estibale of the true value of the integral. This is merely intuition at this point, but
% we can prove:

% {\bf Claim 3}: Every element of $L$ is less than or equal to every element of $U$. That is, for any two partitions $\cP$
% and $\cP'$ of $[a,b]$ we have $LHRS(f, \cP) \leq RHRS(f, \cP')$. 

% To prove this, we use that given two partitions $\cP$ and $\cP'$, 
% there is another partition $\cP''$ that involves all of the points
% occuring in either $\cP$ or  $\cP'$. Loosely speaking, every rectangle occuring in $LHRS(f, \cP)$ will be split into one or more rectangles in $LHRS(f,
% \cP'')$, and the heights of these sub-rectangles are all at least as big as those in the original, since $f$ is increasing. It follows that
% $$
% LHRS(f, \cP) \leq LHRS(f, \cP'').
% $$
% In a similar way, we have
% $$
% RHRS(f, \cP') \geq RHRS(f, \cP'').
% $$
% By Claim 1 we have  $LHRS(f, \cP'')  \leq RHRS(f, \cP'')$ and putting these together gives
% $$
% LHRS(f, \cP) \leq RHRS(f, \cP').
% $$
% This proves Claim 3.


% Now, it follows from Claim 3 that $\sup(L)$ exists,  $\inf(U)$ exists, and $\sup(L)
% \leq \inf(U)$. 

% {\bf Claim 4}: $\sup(L) = \inf(U)$

% To prove Claim 4, suppose it is not true. Then $\sup(L) < \inf(U)$. Appy Claim 2 using any partition $\cP$ such that $\|\cP\| \leq \frac{\inf(U) -
%   \sup(L)}{2(f(b)-f(a))}$. This gives that
% $$
% RHRS(f, \cP) - LHRS(f, \cP)\leq \frac{\inf(U) - \sup(L)}{2}.
% $$
% But $RHRS(f, \cP) \geq \inf(U)$ and $LHRS(f, \cP) \leq \sup(L)$ and so
% $$
% RHRS(f, \cP) - LHRS(f, \cP) \geq \inf(U) - \sup(L).
% $$
% We have reasched a contradiction. This proves Claim 4.

% We now let $I = \sup(L) = \inf(U)$ and prove $f$ that is integrable on $[0,1]$ and that $I = \int_a^b f(x) \, dx$. 
% Pick $\e >0$. Let $\d = \frac{\e}{2(f(b)-f(a))}$. Let $\cP$ be any partition with $\|\cP\| < \d$ and let $\cS$ be any list of
% sampling points for $\cP$. By  Claim 2, we have
% $$
% |RHRS(f,\cP) - LHRS(f,\cP)| < \frac{\e}2
% $$
% and by defintion of $I$ (using Claim 4) we have
% $$
% RHRS(f, \cP) \leq I \leq LHRS(f, \cP).
% $$
% Using Claim 1
% $$
% RHRS(f, \cP) \leq RS(f, \cP, \cS) \leq LHRS(f, \cP).
% $$
% It follows that 
% $$
% |I - RS(f, \cP, \cS)| < 2 \frac{\e}{2} = \e.
% $$
% This proves $f$ is integrable and that
% $\int_a^b f(x) \, dx = I$.
% \end{proof}




% Note that the previous theorem gives us absolutely no idea what the value of $\int_a^b f(x) \, dx$, merely that it is a well-defined number. As you recall from
% Calculus, actually finding the value of such a number typically uses the Fundamental Theorem of Caluclus, which involves
% finding an anti-derivative. This is often very difficult. 







% \begin{proof} Set $I = \int_a^b f(x) \, dx$. Pick $\e > 0$. 

% %%Set $\e' = \min\left\{\frac{\e}{2|f(c) - g(c)|}, \frac{\e}{2} \right\}$.
% By definition there is a $\d' > 0$ such that 
% if $\cP$ is any partition of $[a,b]$ with $\| \cP \| <
%   \d'$ and $\cS$ is any list of sampling points for $\cP$, then $|RS(f, \cP, \cS) - I| < \frac{\e}{2}$.

% Set $\d = \min\{\d', \frac{\e}{4(|f(c) - g(c)|}\}$.
% Suppose $\cP = (x_0, \dots, x_n)$ is any partition of $[a,b]$ with $\| \cP \| <
%   \d$ and $\cS = (s_1, \dots, s_n)$ is any list of sampling poitns for $\cP$.  Note that $s_i = c$ for at most two values of $i$. Thus, in the expression
% $$
% RS(g, \cP, \cS) = \sum_i g(s_i) (x_i - x_{i-1})
% $$
% all but at most two terms coincide with $f(s_i) (x_i - x_{i-1})$. For each $i$ we have $x_i - x_{i-1} < \d$. It follows that
% $$
% |RS(f, \cP, \cS) - RS(g, \cP, \cS)| \leq |f(c) - g(c)| 2 \d  \leq |f(c) - g(c)|2 \frac{\e}{4(|f(c) - g(c)|} = \frac{\e}{2}.
% $$
% We get
% $$
% |RS(g, \cP, \cS) - I| \leq |RS(f, \cP, \cS) - I| + |RS(f, \cP, \cS) - RS(g, \cP, \cS)| < \frac{\e}{2} +\frac{\e}{2} = \e.
% $$
% This proves $g(x)$ is integrable on $[a,b]$ and that $\int_a^b g(x) \, dx = I$.
% \end{proof}


% \begin{prop} Suppose $f$ is integrable on $[a,b]$. Given and $c > b$, define
% $$
% g(x) = 
% \begin{cases}
% f(x)  & \text{if $a \leq x \leq b$ and } \\
% 0  & \text{if $b <x \leq c$.
% \end{cases}
% $$
% Then $g$ is integrable on $[a,c]$ and $\int_a^c g(x) \, dx = \int_a^b f(x) \, dx$.
% \end{prop}

% \begin{proof} Let $I = \int_a^b f(x) \, dx$. Pick $\e > 0$. By definition there is an $\d > 0$ such that if $\cP$ is any
%   partition of $[a,b]$ with $\| \cP \| < \d$ then $|RS(f,\cP, \cS) - I| < \e$ for all $\cS$. I claim this $\d$ "works"
%   for $g$ too:

% Let $\cP$ be any partition of $[a,c]$ with $\|\cP\| < \d$. 


% Here  a technical, but useful result.

% \begin{prop} \label{prop416}
% For numbers $a < b < c$ and a function $f$, if $f$ is integrable on $[a,b]$ and $f$ is integrable on $[b, c]$, then $f$ is integrable on $[a,c]$
%   and we have
% $$
% \int_a^c f(x) \, dx = \int_a^b f(x) \, dx + \int_b^c f(x) \, dx.
% $$
% \end{prop}

% We will also skip the proof of this. 




We come to the first Fundamental Theorem of Calculus. 

\begin{thm}[Fundamental Theorem of Calculus, Part I]
Let $f$ be a function that is defined on the closed interval $[a,b]$. Assume that
\begin{enumerate}
\item $f$ is integrable on $[a,b]$ and
\item there exists a function $F(x)$ that is continuous on $[a,b]$ and 
  differentiable  at every point of $(a,b)$, and that satisfies $f(x) = F'(x)$ for all $x \in (a,b)$. 
\end{enumerate}
Then
$$
\int_a^b f(x) \, dx = F(b) - F(a).
$$
\end{thm}




\begin{proof} 
The key point is the following:

{\bf Claim}: For any partition $\cP = (x_0, \dots, x_n)$ of $[a,b]$, there exists a list of sampling points $\cS$ for $\cP$ such that $RS(f, \cP, \cS) = F(b)
- F(a)$. 

To prove the claim, we apply the Mean Value Theorem to the function $F(x)$ for each of the sub-intervals $[x_{i-1}, x_i]$ for $i = 1, \dots, n$. By assumption,
for each $i$,  $F$ is continuous on $[x_{i-1}, x_i]$  and differentiable on each point of $(x_{i-1}, x_i)$. So, we may apply the Mean Value Theorem, which gives
that there is a point $s_i
\in (x_{i-1} x_i)$ such that $F'(s_i) = \frac{F(x_i) - F(x_{i-1})}{x_i  - x_{i-1}}$. The collection of points $s_1, \dots, s_n$ 
forms a list of sampling points $\cS = (s_1,
\dots, s_n)$ for $\cP$. Since we are assuming $f(x) = F'(x)$ for all $x$, we have
$$
\begin{aligned}
RS(f, \cP, \cS) & = \sum_i f(s_i) (x_i - x_{i-1})  \\
& = \sum_i F'(s_i) (x_i - x_{i-1}) \\
& = \sum_i (F(x_i) - F(x_{i-1}) \\
& = F(b) - F(a). \\
\end{aligned}
$$
This proves the claim.





Now, since we assume $f$ is integrable, there is some number $I$ so that 
for all $\e > 0$ there is a $\d > 0$ such that if $\cP$ is any partition with $\| \cP \| < \d$ and $\cS$ is any list of sampling points for $\cP$, we have
$|RS(f, \cP, \cS) - I| < \e$. 
Our goal is to prove $I = F(b) - F(a)$. Suppose they are not equal. Set $\e = |F(b) - F(a) - I| > 0$, and let $\d > 0$ be as above. 
Choose any  partition $\cP$ such that $\| \cP \| < \d$ --- for example, we could
choose $n$ large enough so that $n > \frac{\e}{b-a}$ and set $x_i = a + \frac{i(b-a)}{n}$ for $i = 0, \dots n$.
Then
$$
|RS(f, \cP, \cS) - I| < \e
$$
for any list of sampling points $\cS$ for $\cP$. 

But, by the claim, there is a choice of sampling points $\cS$ for $\cP$ so that 
$$
RS(f, \cP, \cS) = F(b) - F(a).
$$ 
It follows that
$$
|F(b) - F(a) - I| < \e = |F(b) - F(a) - I|,
$$
which is not possible. We conclude that $F(b) - F(a) = I$.
\end{proof}





I won't give any examples of using FTC, Part I to find the value of a definite integral, since you spent a good portion of
Calculus II on that topic. Instead, let me give an example where it does {\em not} apply:


\begin{ex} 
Recall the function defined as
$$
f(x) = 
\begin{cases}
1 & \text{ if $x < 1$ and }\\
2& \text{ if $x \geq 1$.} \\ 
\end{cases}
$$
We showed that $f$ is integrable on $[0,2]$ and that $\int_0^2 f(x) = 3$ above. There is no way to arrive at this using the FTC, since this function
does {\em not} have an anti-derivative. Why doesn't it?
\end{ex}


\section{Monday, November 16}


Recall that every monotone function on a closed interval is integrable. Here is another large class of integrable functions. 

\begin{thm} If $f$ is continuous on $[a,b]$ then $f$ is integrable on $[a,b]$.
\end{thm}

In the interest of time, we will skip the proof of this Theorem. It's proof requires the notion of ``uniform continuity'', which we have
skipped.




\begin{cor} Suppose $f$ is continuous on $[a,b]$ and there is a function $F(x)$ defined on $[a,b]$ that is continuous on $[a,b]$ and differentiable on $(a,b)$ and such that $F'(x)
  = f(x)$ for all $x \in (a,b)$. Then $f$ is integrable on $[a,b]$ and $\int_a^b f(x) \, dx = F(b) - F(a)$.
\end{cor}






\begin{rem} Note that we assume both that $f$ is integrable on $[a,b]$ and that it has an antiderivative on $(a,b)$ in the FTC, Part I. Many examples, such as the
  previous one, show that even if
  $f$ is integrable on $[a,b]$, then it need not have an anti-derivative. 

Less obvious is that it is possible for $f$ to have an anti-derivative on $[a,b]$ and yet not be integrable! In
other words, it is possible for an evil calculus instructor to pose the question ``What is $\int_a^b f(x) \, dx$?'' for
a certain function $f(x)$ such that (a) $f(x)$ has an anti-derivative $F(x)$ and yet (2) $\int_a^b f(x) \, dx$ is {\em not}
$F(b) - F(a)$, because $\int_a^b f(x) \, dx$ isn't actually defined!

But, such examples are quite rare and complicated. The first discovered 
such example is ``Volterra's
Function'', which you can read about on Wikipedia.
\end{rem}




In order to apply the FTC, Part I, at the very least one needs to know that an anti-derivative of $f(x)$ exists. If one
knows that it exists, then finding it
explicitly is of course very hard, as you know from Calculus II. But let us focus just on the abstract existence
question.

As we've seen, Darboux's Theorem says that if a functions fails to have the ``intermediate value property'', then it
cannot have an anti-derivative. Since continuous functions do indeed have the ``intermediate value property'', there is no
obvious reason why they cannot have antideritaves. In fact, they do, and this is the topic of FTC, II:




\begin{thm}[Fundamental Theorem of Calculus, Part II] Assume $f(x)$ is continuous on $[a,b]$. Then the function defined by 
$$
F(x) = \text{the value of the definite integral of $f$ on $[a,x]$} = \int_a^x f(t) \, dt
$$
for $x \in [a,b]$ is continuous on $[a,b]$ and differentiable on $(a,b)$. Moreover, $F'(x) = f(x)$ for all $x \in (a,b)$. 
\end{thm}

\begin{rem} Since we assume $f$ is continuous, it is integral on  $[a,x]$ for all $x$, and thus the definition of $F(x)$ makes sense.
\end{rem}

To prove this Theorem, we first prove: 


\begin{lem}[``The Mean Value Theorem for Integrals''] \label{lem421}
Suppose $f(x)$ is continuous on some closed interval $[s,t]$.
Then there is a $z \in [s,t]$ such that
$$
f(z) = \frac{\int_s^t f(y) \, dy}{t-s}.
$$
\end{lem}

\begin{rem} Recall that since $f$ is continuous on $[s,t]$, it is integrable on $[s,t]$.
\end{rem}

\begin{rem} 
The proposition is called ``the mean value theorem for integrals''
because $\frac{\int_s^t f(y) \, dy}{t-s}$ is the average value of $f$ on $[s,t]$. 
So, it is asserting that $f$ attains its average value.
\end{rem}


\begin{proof}
  Since $f$ is continuous on $[s,t]$ it attains its maximum value $M$ and
its minimum value $m$ at some points.
So, $m \leq f(y) \leq M$ for all $y \in [s,t]$. It follows from a homework problem that
$$
\int_s^t m \, dy \leq \int_s^t f(y) \, dy \leq \int_s^t M \, dy.
$$
But we also know that
$$
\int_s^t m \, dy  = m(t-s)  \and \int_s^t M \, dy  = M(t-s)
$$
and thus, upon dividing by $t-s$, we get
$$
m \leq \frac{\int_s^t f(y) \, dy} {t-s} \leq M
$$
Since $f$ is equal to $m$ at some point in $[s,t]$ and is equal to $M$ at another point, and $f$ is continuous, by 
the Intermetiate Value Theorem, there is an $z \in [s,t]$ such that
$$
f(z) =  \frac{\int_s^t f(y) \, dy} {t-s}. 
$$
\end{proof}


We will also need the following Lemma, whose proof I skip:

\begin{lem} \label{lem1116}
  If $f$ is continuous on $[a,b]$, then for all $a \leq r \leq b$ we have
  $$
  \int_a^b f(t) \, dt =   \int_a^r f(t) \, dt+   \int_r^b f(t).
  $$
\end{lem}

Let us finally prove the FTC, Part II

\begin{proof}[Proof of FTC, Part II]
Assume $f$ is continuous on $[a,b]$ and recall that we define $F(x) = \int_a^x f(t) \, dt$. 
We need to prove the following three things:

\begin{enumerate}
\item[(a)] For any number $r \in (a,b)$, 
$$
\lim_{x \to r} \frac{F(x) - F(r)}{x-r} = f(r).
$$
This will show $F(x)$ is differentiable (and hence continuous) at $r$ and that $F'(r) = f(r)$.

\item[(b)] 
$$
\lim_{x \to a^+} F(x) = 0.
$$
(Recall that $F(a) = 0$.)
 
\item[(c)] 
$$
\lim_{x \to b^-} F(x) = F(b)
$$
(Note that $F(b) = \int_a^b f(x) \, dx$. 
\end{enumerate}


To prove (a), fix $r \in (a,b)$. We need to show
$$
\lim_{x \to r} \frac{F(x) - F(r)}{x-r} = f(r).
$$
Pick $\e > 0$. Since $f$ is continuous at $r$, there is a $\d > 0$
such that if $|x-r| < \d$, then $|f(x) - f(r)| < \e$.  I claim that this $\d$ ``works'' also to establish this limit. That is I claim that
\begin{quote}
  if $0 < |x-r| < \d$, then $\left|\frac{F(x) - F(r)}{x-r} - f(r)\right| < \e$.
\end{quote}

If $r < x < r + \d$, apply the Mean Value Theorem for Integrals (Lemma \ref{lem421}) using the interval $[r,x]$ to get that there is a $z \in [r,x]$ such that
$$
f(z) = \frac{\int_r^x f(t) \, dt}{r-x}.
$$
By Lemma \ref{lem1116}
$$
\int_a^x f(t) \,dt = \int_a^r f(t) \, dt + \int_r^x f(t) \, dt
$$
and thus
$$
\int_r^x f(t) \, dt = F(x) - F(r).
$$
It follows that
$$
f(z) = \frac{F(x) - F(r)}{x-r}.
$$

If $r - \d < x < r$, we may apply Lemma \ref{lem421} again, this time using $[x,r]$, to get in  a similar way that there is a $z \in [x,r]$ such that
$$
f(z) = \frac{\int_x^r f(y) \, dy}{r-x} =  \frac{F(r) - F(x)} {r-x} =  \frac{F(x) - F(r)} {x-r}.
$$

In either case, since $z$ is between $x$ and $r$ and $|x - r| < \d$, we have $|z-r| < \d$ too. Thus
$$
\left| \frac{F(x) - F(r)} {x-r}  - f(r) \right| =\left| f(z)  - f(r) \right| < \e.
$$
This proves $\lim_{x \to r} \frac{F(x) - F(r)}{x-r} = f(r)$.

I omit the proots of facts  (b)
and (c).
% To show (b),
% $$
% \lim_{x \to a^+} F(x) = 0,
% $$
% we start by showing
% $$
% \lim_{x \to a^+} \frac{F(x) - F(a)}{x-a} = f(a).
% $$
% Pick $\e > 0$. Since $f$ is continuous at $a$, there is a $\d > 0$ such that if $a \leq x < a + \d$ then $|f(x) - f(a)| < \e$. Pick any such $x$ and apply the
% Proposition to $[s,t] = [a,x]$ to get that there is a $z \in [a,x]$ such that $f(z) = \frac{F(x) - F(a)}{x-a}$. Then $a \leq z < a + \d$ too and so $|f(z) - f(a)| < \e$. 
% Thus
% $$
% \left|\frac{F(x) - F(a)}{x-a} - f(a)\right| < \e.
% $$
% This proves
% $$
% \lim_{x \to a^+} \frac{F(x) - F(a)}{x-a} = f(a).
% $$
% From here we get
% $$
% \lim_{x \to a^+} F(x)  =
% \lim_{x \to a^+} \frac{F(x) - F(a)}{x-a}(x-a) =
% \lim_{x \to a^+} \frac{F(x) - F(a)}{x-a} \lim_{x \to a^+} (x-a) = f(a) \cdot 0 = F(a).
% $$


% The proof of (c), 
% $$
% \lim_{x \to b-} F(x) = F(b),
% $$
% is very similar, and omitted. 
\end{proof}

\end{document}


\begin{thm}[Fundamental Theorem of Calculus, Part II] Assume $f(x)$ is continuous on $[a,b]$. Then the function defined by 
$$
F(x) = \text{the value of the definite integral of $f$ on $[a,x]$} = \int_a^x f(y) \, dy
$$
for $x \in [a,b]$ is continuous on $[a,b]$ and differntiable on $(a,b)$. Moreover, $F'(x) = f(x)$ for all $x \in (a,b)$. 
\end{thm}

\begin{cor} If a function is continuous on $[a,b]$ then it has an anti-derivative on $(a,b)$.
\end{cor}


\begin{ex} Let $f(x) = |x|$ defined on $[-1,1]$. This is not differentiable but by the FTC, II it has an
  anti-derivative. What is one? Set 
$$
F(x) = \int_{-1}^x |y| \, dy.
$$
For $x \leq 0$, we have
$$
F(x) = \int_{-1}^x -y \, dy = \frac{-x^2}{2}  - \frac12,
$$
by FTC, I. In particular, $F(0) = -\frac12$. For $x > 0$ we have
$$
F(x) = \int_{-1}^0 -y \, dy + \int_0^x y dy = F(0) + \int_0^x y dy = -\frac12 + \frac{x^2}{2}. 
$$
We get that 
$$
F(x) = 
\begin{cases}
\frac{-x^2}{2}  - \frac12 & \text{if $x \leq 0$ and} \\
-\frac12 + \frac{x^2}{2}  & \text{if $x > 0$} \\
\end{cases}
$$
is an anti-derivative of $|x|$ on $(-1,1)$. If we add $\frac12$ we get another anti-derivative, and it is simpler to
write:
$$
G(x) = 
\begin{cases}
\frac{-x^2}{2}  & \text{if $x \leq 0$ and} \\
+ \frac{x^2}{2}  & \text{if $x > 0$} \\
\end{cases}
$$
is also an anti-derivative, and we can write it as 
$$
G(x) = 
\begin{cases}
\frac{x^3}{|x|} & \text{if $x \ne 0$ and} \\
0 & \text{if $x = 0$.} \\
\end{cases}
$$
In fact, this is an anti-derivative of $|x|$ on all of $\R$.
\end{ex}

\section{April 17, 2017}


A couple times above we used the following proposition:

\begin{prop} Let $f$ and $g$ be any two functions defined on the closed interval $[a,b]$ and that they are equal at all points except one --- that is, 
assume there is a number $c \in   [a,b]$ such that $g(x) = f(x)$ for all $x \ne c$.
If $f$ is integrable on $[a,b]$, then so is $g$ and $\int_a^b g(x) \, dx = \int_a^b f(x) \, dx$.
\end{prop}

\begin{rem} You are not allowed to use this to do the homework!
\end{rem}

We will skip the proof, but it is proven in a way very similar
to one of the homework problems. (The homework problem is the special
case in which $f(x)$ is the contant $0$ function.)

We will now prove FTC, II.

\begin{proof}[Proof of FTC, II]
Assume $f$ is continuous on $[a,b]$ and define $F(x) = \int_a^x f(y) \, dy$. Pick any number $r \in (a,b)$. We need to prove
$$
\lim_{x \to r} \frac{F(x) - F(r)}{x-r} = f(r).
$$


{\bf Claim}: For any $x \in [a,b]$ with $x \ne r$,  
there is $z$ such that $f(z) =\frac{F(r) - F(x)} {r-x}$ and 
 $z$ is between $x$ and $r$ (i.e., either $x \leq z \leq r$ or $r \leq z \leq x$).


{\bf Proof of claim:} 
Suppose $x > r$. Then $F(x) - F(r) = \int_r^x f(y) \, dy$ by Proposition \ref{prop416}. Since $f$ is continuous on $[r,x]$ it attains its maximum value $M$ and
its minimum value $m$. 
So, $m \leq f(y) \leq M$ for all $y \in [r,x]$. It follows from a homework problem that
$$
m(x-r) = \int_r^x m \, dy \leq F(x) - F(r) \leq \int_r^x M \, dy = M(x-r)
$$
and thus
$$
m \leq \frac{F(x) - F(r)} {x-r} \leq M
$$
We now apply the Intermetiate Value Theorem. This gives that there is an $z \in [r,x]$ such that
$$
f(z) = \frac{F(x) - F(r)} {x-y}
$$

The proof of the claim for the case $x < r$ is essentially identical to the first case. 



We are now prepared to prove 
$$
\lim_{x \to r} \frac{F(x) - F(r)}{x-r} = f(r).
$$
Pick $\e > 0$. Since $f$ is continuous, there is a $\d > 0$ such that if $r - \d < x < r + \d$, then $|f(x) - f(r)| < \e$. 
For any such $x$, let $z$ be as in the Claim, so that $z$ is between $x$ and $r$ and
$f(z) = \frac{F(x) - F(r)} {x-y}$.  Since $z$ is between $x$ and $r$, we have
$r - \d < z < r + \d$ too and thus
$$
\left| \frac{F(x) - F(r)} {x-y}  - f(r) \right| =\left| f(z)  - f(r) \right| < \e.
$$
This proves $\lim_{x \to r} \frac{F(x) - F(r)}{x-r} = f(r)$.


This proves that $F(x)$ is differentiable on $(a,b)$ and that $F'(x) = f(x)$ for all $x \in (a,b)$. Since differentiabiilty implies continuijty, it follows that
$F(x)$ is continuous on $(a,b)$. But we still need to show it is continuous at $a$ and $b$. This is actually done in the same way: if $r = a$, the above
argument actually shows that
$$
\lim_{x \to a^+} \frac{F(x) - F(a)}{x-a} = f(a)
$$
and thus
$$
\lim_{x \to a^+} (F(x) - F(a)) =\lim_{x \to a^+} \frac{F(x) - F(a)}{x-a} (x-a) = f(a) \cdot 0
$$
from which it follows that $\lim_{x \to a^+} F(x) = F(a)$. (In fact,  $F(a) = 0$.)
Similarly, $\lim_{x \to b^-} F(x) = F(b)$. 
\end{proof}


\begin{rem} FTC I is very nearly (but not quite) a consequence of FTC II. For suppose $f(x)$ is continuous on $[a,b]$ and that there is a function $G(x)$ tha is
  continuous on $[a,b]$ and differentiable on $(a,b)$ and such that $f(x) = G'(x)$ for all $x \in
  (a,b)$. Define $F(x) = \int_a^x f(y) \, dy$. By FTC II we have $F'(x) = f(x)$ for all $x \in (a,b)$. It follows that $(G(x) - F(x))' = 0$ for all $x \in
  (a,b)$ and so there is a contant $c$ such that $G(x) = F(x) + c$ for all $x \in (a,b)$. Since both $G$ nd $F$ are continuous on all of $[a,b]$, we get that
$G(a) = F(a) + c$ and   $G(b) = F(b) + c$ too, by taking $\lim_{x \to a^+}$ and $\lim_{x \to b^-}$. Since $F(a) = 0$, this shows that $c = G(a)$ and so $F(x) =
G(x) - G(a)$ for all $x$. This gives
$$
G(b) - G(a) = F(b) = \int_a^b f(x) \, dx,
$$
which is the conclusion of FTC I.

Why did I say ``not quite'' before? Because in FTC I, we do {\em not} assume $f$ is continous, but that assumption is needed for FTC II. Let's give an example:
\end{rem}

\begin{ex} Recall from before that the function
$$
 f(x) = 
 \begin{cases}
2x \sin(1/x) - \cos(1/x),  & \text{if $x \ne 0$} \\
 0, & \text{if $x = 0$.} \\
 \end{cases}
 $$
is not continuous at $x = 0$, and so FTC II would not apply to this function (on any interval that contains $0$). 

But it nevertheless has an anti-derivative: Let 
$$
F(x) = 
 \begin{cases}
 x^2 \sin(1/x), & \text{if $x \ne 0$} \\
 0, & \text{if $x = 0$.} \\
 \end{cases}
 $$
Then $F$ is differentiable on all of $\R$ and $F'(x) = f(x)$ for all $x \in \R$.  So, {\em provided $f(x)$ is integrable on $[-1,1]$}, we may apply FTC I to get
$$
\int_{-1}^1 f(x) \, dx = F(1) - F(-1) = \sin(1) - \sin(-1).
$$

It turns out that 
$f(x)$ is indeed integrable on $[-1,1]$, and every closed interval.  But I won't prove that. 
\end{ex}





\section{April 21, 2017}

I would like to do a better job proving FTC, II. Recall the statment:



\section{April 24, 2017}


{\bf The Cantor Set}: Define a seqeunce of subsets $C_1, C_2, \dots$ of the closed unit interval $[0,1]$ as follows.

$C_1$ is obtains by removing the middle open third of $[0,1]$ --- that is, $C_1 = [0,1] \setminus (\frac13, \frac23)$. So, $C_1$ is a  disjoint union of two
closed intervals $C_1 = [0, \frac13] \cup [\frac23, 1]$. We form $C_2$ by removing the middle open thirds of each of the two closed subintervals that comprise
$C_1$. So, $C_2$ is  a union of four disjoint closed intervals,
$$
C_2 = [0, \frac19] \cup [\frac29, \frac39] \cup [\frac69, \frac79] \cup [\frac89, 1].
$$
We form $C_3$ by removing the middle open thirds of each of these:
$$
\begin{aligned}
C_3 & = [0, \frac{1}{27}] \cup [\frac{2}{27}, \frac{3}{27}] \cup [\frac{6}{27}, \frac{7}{27}] \cup [\frac{8}{27}, \frac{9}{27}] \\
& \cup  [\frac{18}{27}, \frac{19}{27}] \cup [\frac{20}{27}, \frac{21}{27}] \cup [\frac{24}{27}, \frac{25}{27}] \cup [\frac{26}{27}, \frac{27}{27}] \\
\end{aligned}
$$

More formally, we let $C_0 = [0,1]$ and define $C_n$ for $n \geq 1$ recursively by
$$
C_n = \frac{C_{n-1}}{3} \cup \left(\frac23 + \frac{C_{n-1}}{3}\right)
$$
Here, by $\frac{C_{n-1}}{3}$ we mean the set  $\{ \frac{x}{3} \mid x \in C_{n-1} \}$
and $\frac23 + \frac{C_{n-1}}{3} = \{ \frac23 + \frac{x}{3} \mid x \in C_{n-1} \}$. 
It is not hard to see  that $C_{n} \subseteq C_{n-1}$ for all $n$. 

Define  {\em the Cantor set} to be 
$$
C = C_1 \cap C_2 \cap C_3 \cap \cdots
$$
This is an example of a ``nested'' intersection.  A number $x$ is in $C$ if and only if $x \in C_n$ for all $n$. A
number $x$ is not in $C$ if and only if $x \notin C_n$ for {\em some} $n$.



If a point $x$ occurs as an endpoint of one of closed interval comprising $C_n$, then it will not be removed when we pass to $C_{n+1}$ and it will remain an
endpoint of one of the closed intervals making up $C_{n+1}$. This shows such an $x$ belongs to $C_m$ for all $m$ and
hence $x \in C$. For example, $\frac13 \in C$. In particular, $C \ne \emptyset$.
You might think that {\em only} such endpoints survive all the way to $C$, but that is not correct, as we shall see. 



Another way of describing $C$ is that is consists of those real numbers $x$ such that $0 \leq x \leq 1$ that may be expressed in base $3$ as 
$0.e_1e_2e_3\cdots$ such that $e_i \ne 1$ for all $i$. Here $e_i \in \{0,1,2\}$ for all $i$ and
$$
0.e_1e_2e_3\cdots = \sum_{n=1}^\infty \frac{e_n}{3^n} = 
\lim_{n \to \infty} \left(\frac{e_1}{3} + \frac{e_2}{9} + \cdots + \frac{e_n}{3^n} \right)
$$
So, for example, $.02020202\cdots \in C$. 

We know  $\frac13 \in C$ and $\frac13 = .1$, which appears at first to contradict what I just said. But notice that
$\frac13 = .0222\cdots$ too, since
$$
.0222\cdots = \frac{2}{9} + \frac{2}{27} + \frac{2}{81} + \cdots = \frac{\frac29}{1-\frac13} = \frac13
$$
using the formula for a geometric series. 

In general, a point is an endpoint if and only if it can be represented in base three as either
$$
.e_1e_2 \dots e_N 000\cdots
$$
or
$$
.e_1e_2 \dots e_N 222\cdots
$$
for some $N$ and some $e_i \in \{0,2\}$.







Question: How large is the Cantor set?


On the one hand, it is {\em uncountable}. Recall this means that its elements cannot be listed as a sequence (indexed by $\N$). To prove this, we use the
description of elements of $C$ using base three expansions as above and follow Cantor's diagonal argument: By way of
contradiction, suppose  that $x_1, x_2,
\dots, $ was a complete listting of every element of $C$. Write these numbers in base $3$:
$$
\begin{aligned}
x_1 & = .e_{11} e_{12} e_{13} \cdots \\
x_2 & = .e_{21} e_{22} e_{23} \cdots \\
x_3 & = .e_{31} e_{32} e_{33} \cdots \\
\vdots & = \vdots \\
\end{aligned}
$$
where $e_{i,j} \in \{0,2\}$ for all $i,j$. Now form the number
$$
y := .f_1f_2f_3\dots
$$
where $f_i = 2$ if $e_{i,i} = 0$ and $f_i = 0$ if $e_{i,i} = 2$. Then $y \in C$ but $y \ne x_i$ for all $i$, since $f_i
\ne e_{i,i}$ for all $i$.
We have reached a contradiction, and so it must not be possible to list every element of $C$ in a sequence.

\begin{rem} In fact, this argument shows that nearly all the points of $C$ are {\em not} endpoints.
\end{rem}


Since $C$ is uncountable, it is ``big''. On the other hand, it's ``small'': Let us consider the complement of $C$ in
$I$. It is an infinite disjoint union
$$
U_1 \cup U_2 \cup \cdots
$$
where 
$U_1 = (\frac13, \frac23)$,
$U_2 = (\frac19, \frac29) \cup (\frac79, \frac89)$,
$U_3 = (\frac1{27}, \frac2{27}) \cup (\frac7{27}, \frac8{27}) \cup (\frac{19}{27}, \frac{20}{27}) \cup (\frac{25}{27}, \frac{26}{27})$, and so on

Note that $U_1$ has length  $\frac13$, $U_2$ has length $\frac29$, $U_3$ had length $\frac{4}{27}$, and in general $U_n$ has length $\frac{2^{n-1}}{3^n}$ for
each $n \geq 1$. So, the total length of $I \setminus C$ is
$$
\sum_{n=1}^\infty \frac{2^{n-1}}{3^n} = \frac13 + \frac29 + \frac{4}{27} + \cdots 
= \frac{\frac13}{1-\frac23} = 1.
$$
Since $[0,1] - C$ has length $1$, 
$C$ itself must have length $0$. The term ``length'' here has not been defined carefully, but it can be, and it really
goes by the name of ``measure''. And the argument given here proves that $C$ has {\em measure zero}, which is a indication that it is ``small''.


\end{comment}

\printindex
\end{document}








\end{document}













 